{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4156c326-c25c-4dcd-84e6-1847370bbf53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notebook 출력설정\n",
    "# 주요 라이브러리 임포트\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pylab as plt\n",
    "plt.rcParams[\"figure.figsize\"] = (15,5)\n",
    "plt.rcParams['lines.linewidth'] = 1\n",
    "plt.rcParams['axes.grid'] = True\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c82b88-8d0c-4802-a11d-f7fd13d8a8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR_DATASET = \"C:/Users/0stix/Datasets/\"\n",
    "NAME_PROJECT = \"2203-kaggle-tps2203\"\n",
    "TARGET = \"congetion\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1b4e8e-8d80-42e3-8621-bd91cbfc2c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 고속 데이터셋 평가\n",
    "def eval_df(X, y, model=0):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae441df-3b48-462c-a396-e91376ca9ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2419c865-dac4-41ce-8d09-1ed61ee7b9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원본 데이터셋 로드\n",
    "dir_dataset = \"C:/Users/0stix/Datasets/\"\n",
    "name_project = '2203-kaggle-tps2203'\n",
    "df_train = pd.read_csv(dir_dataset+name_project+'/train.csv')\n",
    "df_test = pd.read_csv(dir_dataset+name_project+'/test.csv')\n",
    "df_sub = pd.read_csv(dir_dataset+name_project+'/sample_submission.csv')\n",
    "\n",
    "len_train = len(df_train)\n",
    "df_all = pd.concat([df_train, df_test], axis=0)\n",
    "target = 'congetion'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c29cb404-0f03-4ba5-873d-ce5b3dbeb898",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '..\\\\input\\\\tabular-playground-series-mar-2022\\\\train.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-a82cab40ad1d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m \u001b[0mdf_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-29-a82cab40ad1d>\u001b[0m in \u001b[0;36mload_data\u001b[1;34m()\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mload_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[0mdata_dir\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"../input/tabular-playground-series-mar-2022\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m     \u001b[0mdf_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_dir\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;34m\"train.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m     \u001b[0mdf_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_dir\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;34m\"test.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[1;31m# Merge the splits so we can process them together\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    608\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    609\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 610\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    611\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    612\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    460\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    461\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 462\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    463\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    464\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    817\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1048\u001b[0m             )\n\u001b[0;32m   1049\u001b[0m         \u001b[1;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1050\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1051\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1052\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1865\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1866\u001b[0m         \u001b[1;31m# open handles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1867\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1868\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1869\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"storage_options\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"encoding\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"memory_map\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"compression\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_open_handles\u001b[1;34m(self, src, kwds)\u001b[0m\n\u001b[0;32m   1360\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHanldes\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1361\u001b[0m         \"\"\"\n\u001b[1;32m-> 1362\u001b[1;33m         self.handles = get_handle(\n\u001b[0m\u001b[0;32m   1363\u001b[0m             \u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1364\u001b[0m             \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    645\u001b[0m                 \u001b[0merrors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"replace\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    646\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 647\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    648\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    649\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '..\\\\input\\\\tabular-playground-series-mar-2022\\\\train.csv'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from IPython.display import display\n",
    "from pandas.api.types import CategoricalDtype\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder, OrdinalEncoder\n",
    "\n",
    "# Algorithms\n",
    "from xgboost import XGBRegressor\n",
    "# from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "# Optuna - Bayesian Optimization \n",
    "# import optuna\n",
    "# from optuna.samplers import TPESampler\n",
    "\n",
    "# Plotly\n",
    "# import plotly.express as px\n",
    "# from plotly.subplots import make_subplots\n",
    "# import plotly.figure_factory as ff\n",
    "# import plotly.offline as offline\n",
    "# import plotly.graph_objs as go\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def load_data():\n",
    "    data_dir = Path(\"../input/tabular-playground-series-mar-2022\")\n",
    "    df_train = pd.read_csv(data_dir / \"train.csv\")\n",
    "    df_test = pd.read_csv(data_dir / \"test.csv\")\n",
    "    # Merge the splits so we can process them together\n",
    "    df = pd.concat([df_train, df_test])\n",
    "    return df\n",
    "\n",
    "df_data = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8de22679-61f7-41d8-a2d7-e6f3de0f7547",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 27.60 Mb (39.3% reduction)\n"
     ]
    }
   ],
   "source": [
    "df_data = df_all.copy()\n",
    "\n",
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int8','int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "\n",
    "    if verbose:\n",
    "        print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    " \n",
    "    return df\n",
    "\n",
    "df_data = reduce_mem_usage(df_data)\n",
    "\n",
    "df_data.time = pd.to_datetime(df_data.time)\n",
    "df_data['year'] = df_data.time.dt.year\n",
    "df_data['month'] = df_data.time.dt.month\n",
    "df_data['week'] = df_data.time.dt.isocalendar().week\n",
    "df_data['hour'] = df_data.time.dt.hour\n",
    "df_data['minute'] = df_data.time.dt.minute\n",
    "df_data['day_of_week'] = df_data.time.dt.day_name()\n",
    "df_data['day_of_year'] = df_data.time.dt.dayofyear\n",
    "df_data['is_weekend'] = (df_data.time.dt.dayofweek >= 5).astype(\"int\")\n",
    "df_data = df_data.set_index('time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b2b370ec-5232-46c2-89f7-02e7c05bbdc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline score: 6.80326 MAE\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# def score_dataset(X, y, model=XGBRegressor(tree_method='gpu_hist', predictor='gpu_predictor'), model_2 = CatBoostRegressor(task_type = 'GPU', silent=True)):\n",
    "def score_dataset(X, y, model=XGBRegressor(), model_2 = CatBoostRegressor(silent=True)):\n",
    "#def score_dataset(X, y, model=XGBRegressor(), model_2 = CatBoostRegressor(silent=True)):\n",
    "    # Label encoding is good for XGBoost and RandomForest, but one-hot\n",
    "    # would be better for models like Lasso or Ridge. The `cat.codes`\n",
    "    # attribute holds the category levels.\n",
    "    for colname in X.select_dtypes([\"object\"]).columns:\n",
    "        X[colname] = LabelEncoder().fit_transform(X[colname])\n",
    "    X['week'] = X['week'].astype(int)\n",
    "    X = X.drop('row_id',axis=1)\n",
    "    # Metric for TPS Mar22 competition is MAE (Mean Absolute Error)\n",
    "    score_xgb = cross_val_score(\n",
    "        model, X, y, cv=5, scoring=\"neg_mean_absolute_error\", n_jobs=1\n",
    "    )\n",
    "    \n",
    "    score_cat = cross_val_score(\n",
    "        model_2, X, y, cv=5, scoring=\"neg_mean_absolute_error\", n_jobs=1\n",
    "    )\n",
    "    \n",
    "    score = -0.5 * (score_xgb.mean() + score_cat.mean())\n",
    "    return score\n",
    "\n",
    "#df_data = df_data.reset_index().set_index('row_id')\n",
    "#df_data = df_data.drop(outliers_index,axis=0)\n",
    "#df_data = df_data.reset_index().set_index('time')\n",
    "\n",
    "x = df_data[df_data['congestion'].isnull() == False].copy()\n",
    "y = pd.DataFrame(x.pop('congestion'))\n",
    "\n",
    "baseline_score = score_dataset(x, y)\n",
    "print(f\"Baseline score: {baseline_score:.5f} MAE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c86a717d-b4db-44a5-85a8-b1a0fa729a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sin, cos, pi, exp\n",
    "sin_vals = {\n",
    "    'NB': 0.0,\n",
    "    'NE': sin(1 * pi/4),\n",
    "    'EB': 1.0,\n",
    "    'SE': sin(3 * pi/4),\n",
    "    'SB': 0.0,\n",
    "    'SW': sin(5 * pi/4),    \n",
    "    'WB': -1.0,    \n",
    "    'NW': sin(7 * pi/4),  \n",
    "}\n",
    "\n",
    "cos_vals = {\n",
    "    'NB': 1.0,\n",
    "    'NE': cos(1 * pi/4),\n",
    "    'EB': 0.0,\n",
    "    'SE': cos(3 * pi/4),\n",
    "    'SB': -1.0,\n",
    "    'SW': cos(5 * pi/4),    \n",
    "    'WB': 0.0,    \n",
    "    'NW': cos(7 * pi/4),  \n",
    "}\n",
    "\n",
    "df_data['sin'] = df_data['direction'].map(sin_vals)\n",
    "df_data['cos'] = df_data['direction'].map(cos_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5e17288e-5b99-45e2-8150-b434c01f1287",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline score: 6.80326 MAE\n"
     ]
    }
   ],
   "source": [
    "x = df_data[df_data['congestion'].isnull() == False].copy()\n",
    "y = pd.DataFrame(x.pop('congestion'))\n",
    "baseline_score = score_dataset(x, y)\n",
    "print(f\"Baseline score: {baseline_score:.5f} MAE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "299c68e7-dcfb-4eb6-a1db-cd6bc624a408",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data['roadway'] = df_data.x.astype(str) + df_data.y.astype(str) + df_data.direction.astype(str)\n",
    "# px.box(df_data[df_data.congestion.isnull() == False], x=\"roadway\", y=\"congestion\", color = 'roadway')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1d6262b5-6829-41db-9bad-f881deee542a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline score: 6.63368 MAE\n"
     ]
    }
   ],
   "source": [
    "df_data['x_cos_hour'] = df_data.x * df_data.cos * df_data.hour\n",
    "df_data['y_sen_hour'] = df_data.y * df_data.sin * df_data.hour\n",
    "\n",
    "df_data = df_data.drop(['year','x','y','direction'], axis=1)\n",
    "\n",
    "x = df_data[df_data['congestion'].isnull() == False].copy()\n",
    "y = pd.DataFrame(x.pop('congestion'))\n",
    "baseline_score = score_dataset(x, y)\n",
    "print(f\"Baseline score: {baseline_score:.5f} MAE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "300eea95-2edf-4f06-8941-38027164922f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline score: 6.25381 MAE\n"
     ]
    }
   ],
   "source": [
    "df_data = df_data.reset_index()\n",
    "keys = ['roadway', 'day_of_week','hour', 'minute']\n",
    "\n",
    "df = df_data.groupby(by=keys).mean().reset_index().set_index(keys)\n",
    "df['mean congestion'] = df['congestion']\n",
    "df_data = df_data.merge(df['mean congestion'], how='left', left_on=keys, right_on=keys)\n",
    "\n",
    "df = df_data.groupby(by=keys).median().reset_index().set_index(keys)\n",
    "df['median congestion'] = df['congestion']\n",
    "df_data = df_data.merge(df['median congestion'], how='left', left_on=keys, right_on=keys)\n",
    "\n",
    "df = df_data.groupby(by=keys).min().reset_index().set_index(keys)\n",
    "df['min congestion'] = df['congestion']\n",
    "df_data = df_data.merge(df['min congestion'], how='left', left_on=keys, right_on=keys)\n",
    "\n",
    "df = df_data.groupby(by=keys).max().reset_index().set_index(keys)\n",
    "df['max congestion'] = df['congestion']\n",
    "df_data = df_data.merge(df['max congestion'], how='left', left_on=keys, right_on=keys)\n",
    "\n",
    "df_data = df_data.set_index('time')\n",
    "\n",
    "x = df_data[df_data['congestion'].isnull() == False].copy()\n",
    "y = pd.DataFrame(x.pop('congestion'))\n",
    "baseline_score = score_dataset(x, y)\n",
    "print(f\"Baseline score: {baseline_score:.5f} MAE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a20c81f7-def9-4b0a-aad5-28f70f4dd369",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "def make_mi_scores(X, y):\n",
    "    X = X.copy()\n",
    "    for colname in X.select_dtypes([\"object\"]):\n",
    "        X[colname], _ = X[colname].factorize()\n",
    "    # All discrete features should now have integer dtypes\n",
    "    #discrete_features = [pd.api.types.is_integer_dtype(t) for t in X.dtypes]\n",
    "    mi_scores = mutual_info_regression(X, y, random_state=0)\n",
    "    mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns)\n",
    "    mi_scores = mi_scores.sort_values(ascending=False)\n",
    "    return mi_scores\n",
    "\n",
    "y = df_data[df_data['congestion'].isnull() == False]['congestion']\n",
    "x = df_data[df_data['congestion'].isnull() == False].drop('congestion', axis=1)\n",
    "mi_scores = make_mi_scores(x, y)\n",
    "mi_scores = pd.DataFrame(mi_scores).reset_index().rename(columns={'index':'Feature'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "28db7687-0d93-4d5f-b33c-2386e30c1095",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'px' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-41-1b43459a0f86>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m fig = px.bar(mi_scores, x='MI Scores', y='Feature', color=\"MI Scores\",\n\u001b[0m\u001b[0;32m      2\u001b[0m              color_continuous_scale='darkmint')\n\u001b[0;32m      3\u001b[0m fig.update_layout(height = 750, title_text=\"Mutual Information Scores\",\n\u001b[0;32m      4\u001b[0m                   title_font=dict(size=29, family=\"Lato, sans-serif\"), xaxis={'categoryorder':'category ascending'}, margin=dict(t=80))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'px' is not defined"
     ]
    }
   ],
   "source": [
    "# fig = px.bar(mi_scores, x='MI Scores', y='Feature', color=\"MI Scores\",\n",
    "#              color_continuous_scale='darkmint')\n",
    "# fig.update_layout(height = 750, title_text=\"Mutual Information Scores\",\n",
    "#                   title_font=dict(size=29, family=\"Lato, sans-serif\"), xaxis={'categoryorder':'category ascending'}, margin=dict(t=80))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "93ebd875-797e-4515-b5df-7d3b8f790e08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 49.52 Mb (46.0% reduction)\n"
     ]
    }
   ],
   "source": [
    "qualitative = [col for col in df_data if df_data[col].dtype == 'object']\n",
    "for feature in qualitative:\n",
    "    df_data[feature] = LabelEncoder().fit_transform(df_data[feature])\n",
    "df_data = reduce_mem_usage(df_data)\n",
    "\n",
    "df_data = df_data.drop(['month','minute','week','day_of_week','is_weekend','day_of_year','cos','sin'],axis=1)\n",
    "\n",
    "df_train = df_data[df_data.congestion.isnull() == False]\n",
    "df_test = df_data[df_data.congestion.isnull() == True]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X = df_train.drop(['congestion','row_id'],axis = 1)\n",
    "y = df_train['congestion']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "eac467ec-e8c9-4ffe-9907-9efa08485c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error as mae\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        \"random_state\":trial.suggest_categorical(\"random_state\", [2022]),\n",
    "        'learning_rate' : trial.suggest_loguniform('learning_rate', 0.0001, 0.3),\n",
    "        'bagging_temperature' :trial.suggest_loguniform('bagging_temperature', 0.01, 100.00),\n",
    "        \"n_estimators\": 1000,\n",
    "        \"max_depth\":trial.suggest_int(\"max_depth\", 4, 16),\n",
    "        'random_strength' :trial.suggest_int('random_strength', 0, 100),\n",
    "        \"l2_leaf_reg\":trial.suggest_float(\"l2_leaf_reg\",1e-8,3e-5),\n",
    "        \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 5, 100),\n",
    "        \"max_bin\": trial.suggest_int(\"max_bin\", 200, 500),\n",
    "        'od_type': trial.suggest_categorical('od_type', ['IncToDec', 'Iter']),\n",
    "        'task_type': trial.suggest_categorical('task_type', ['GPU']),\n",
    "        'loss_function': trial.suggest_categorical('loss_function', ['MAE']),\n",
    "        'eval_metric': trial.suggest_categorical('eval_metric', ['MAE'])\n",
    "    }\n",
    "\n",
    "    model = CatBoostRegressor(**params)\n",
    "    X_train_tmp, X_valid_tmp, y_train_tmp, y_valid_tmp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    model.fit(\n",
    "        X_train_tmp, y_train_tmp,\n",
    "        eval_set=[(X_valid_tmp, y_valid_tmp)],\n",
    "        early_stopping_rounds=35, verbose=0\n",
    "    )\n",
    "        \n",
    "    y_train_pred = model.predict(X_train_tmp)\n",
    "    y_valid_pred = model.predict(X_valid_tmp)\n",
    "    train_mae = mae(y_train_tmp, y_train_pred)\n",
    "    valid_mae = mae(y_valid_tmp, y_valid_pred)\n",
    "    \n",
    "    print(f'MAE of Train: {train_mae}')\n",
    "    print(f'MAE of Validation: {valid_mae}')\n",
    "    \n",
    "    return valid_mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "8b10e147-1a60-4938-a337-a49962cf47d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "allow_optimize = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f6126b54-bcb4-4af4-bc67-bc08942923f6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TPESampler' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-72-3f7e9eb0c7ff>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mallow_optimize\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0msampler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTPESampler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m42\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     study = optuna.create_study(\n",
      "\u001b[1;31mNameError\u001b[0m: name 'TPESampler' is not defined"
     ]
    }
   ],
   "source": [
    "TRIALS = 100\n",
    "TIMEOUT = 3600\n",
    "\n",
    "if allow_optimize:\n",
    "    sampler = TPESampler(seed=42)\n",
    "\n",
    "    study = optuna.create_study(\n",
    "        study_name = 'cat_parameter_opt',\n",
    "        direction = 'minimize',\n",
    "        sampler = sampler,\n",
    "    )\n",
    "    study.optimize(objective, n_trials=TRIALS)\n",
    "    print(\"Best Score:\",study.best_value)\n",
    "    print(\"Best trial\",study.best_trial.params)\n",
    "    \n",
    "    best_params = study.best_params\n",
    "    \n",
    "    X_train_tmp, X_valid_tmp, y_train_tmp, y_valid_tmp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    model_tmp = CatBoostRegressor(**best_params, n_estimators=30000, verbose=1000).fit(X_train_tmp, y_train_tmp, eval_set=[(X_valid_tmp, y_valid_tmp)], early_stopping_rounds=35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8167ddf7-081d-4b28-a3e2-709f9aac03e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Custom logger is already specified. Specify more than one logger at same time is not thread safe."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 13.0293626\ttotal: 116ms\tremaining: 1m 55s\n",
      "999:\tlearn: 5.9164225\ttotal: 1m 57s\tremaining: 0us\n"
     ]
    }
   ],
   "source": [
    "if allow_optimize:\n",
    "    model = CatBoostRegressor(**best_params, n_estimators=model_tmp.get_best_iteration(), verbose=1000).fit(X, y)\n",
    "else:\n",
    "    model = CatBoostRegressor(\n",
    "        verbose=1000,\n",
    "        early_stopping_rounds=10,\n",
    "        #iterations=5000,\n",
    "        random_state = 2022, learning_rate = 0.0824038781081412, bagging_temperature = 0.03568558360430449, max_depth = 16, \n",
    "        random_strength = 47, l2_leaf_reg = 7.459775961819184e-06, min_child_samples = 49, max_bin = 320, od_type = 'Iter', \n",
    "        task_type = 'GPU', \n",
    "        loss_function = 'MAE', eval_metric = 'MAE'\n",
    "    ).fit(X, y)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f08d5d3d-01c5-488b-bcc6-8db7d2e9846f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plot_feature_importance' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-49-d368faeeb3c6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mplot_feature_importance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_feature_importance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'CatBoost'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'plot_feature_importance' is not defined"
     ]
    }
   ],
   "source": [
    "plot_feature_importance(model.get_feature_importance(),X.columns,'CatBoost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b83bc36e-9645-483e-a0e8-34955513ba13",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = df_test.drop(['congestion','row_id'],axis = 1).copy()\n",
    "predictions = model.predict(x_test)\n",
    "submit_cat = pd.DataFrame({'row_id':df_test.row_id, 'congestion':predictions})\n",
    "submit_cat = submit_cat.reset_index().drop('time',axis=1).set_index('row_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "0d9ed134-fe47-4b22-ba68-0f6af4cbb60d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'max_depth': 5, 'min_child_weight': 1, 'n_estimators': 75}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "allow_optimize = 1\n",
    "if allow_optimize:\n",
    "    param_grid={\n",
    "#         'max_depth': [4,5,6,7,8,9],\n",
    "        'max_depth': [5],\n",
    "        #'n_estimators': [100,200,300,400,500,600,700,800,900,1000],\n",
    "        'n_estimators': [75],\n",
    "#         'min_child_weight' : [1,2,3,4,5,6],\n",
    "        'min_child_weight' : [1],\n",
    "        \n",
    "#       'gpu_id' : [0]\n",
    "        }\n",
    "\n",
    "    regressor = XGBRegressor(\n",
    "#         tree_method = 'gpu_hist', \n",
    "#         predictor = 'gpu_predictor'\n",
    "    )\n",
    "    CV_regressor = GridSearchCV(regressor, param_grid, cv=3, scoring=\"neg_mean_absolute_error\", n_jobs= -1, return_train_score = True, verbose = 1)\n",
    "    CV_regressor.fit(X, y)\n",
    "    \n",
    "    print(\"The best hyperparameters are : \",\"\\n\")\n",
    "    print(CV_regressor.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "139d41f0-a170-4497-ae08-fcb110548a3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "             colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
       "             gamma=0, gpu_id=-1, importance_type=None,\n",
       "             interaction_constraints='', learning_rate=0.300000012,\n",
       "             max_delta_step=0, max_depth=5, min_child_weight=1, missing=nan,\n",
       "             monotone_constraints='()', n_estimators=75, n_jobs=12,\n",
       "             num_parallel_tree=1, predictor='auto', random_state=0, reg_alpha=0,\n",
       "             reg_lambda=1, scale_pos_weight=1, subsample=1, tree_method='exact',\n",
       "             validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if allow_optimize: \n",
    "    CV_regressor = CV_regressor.best_estimator_\n",
    "else:\n",
    "    CV_regressor = XGBRegressor(\n",
    "#         tree_method = 'gpu_hist',\n",
    "#         predictor = 'gpu_predictor', \n",
    "#         gpu_id = 0, \n",
    "        max_depth = 4, n_estimators = 100)\n",
    "CV_regressor.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cfd2b5a-7205-41f8-8426-c493c510510e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_importance(CV_regressor.feature_importances_,X.columns,'XGBOOST')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "1b46ddb5-eeb7-4335-856a-1a0c10fee837",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = CV_regressor.predict(x_test)\n",
    "submit_xgb = pd.DataFrame({'row_id':df_test.row_id, 'congestion':predictions})\n",
    "submit_xgb = submit_xgb.reset_index().drop('time',axis=1).set_index('row_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "40e0c6f9-d9cb-40d1-9ced-8319509f7d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "submit = pd.DataFrame({'congestion': submit_cat['congestion']+0*submit_xgb['congestion']})\n",
    "# special = pd.read_csv('../input/tps-mar-22-special-values/special v2.csv', index_col=\"row_id\")\n",
    "special = df_sub\n",
    "\n",
    "special = special[['congestion']].rename(columns={'congestion':'special'})\n",
    "submit = submit.merge(special, left_index=True, right_index=True, how='left')\n",
    "submit['special'] = submit['special'].fillna(submit['congestion']).round().astype(int)\n",
    "submit = submit.drop(['congestion'], axis=1).rename(columns={'special':'congestion'})\n",
    "submit['congestion'] = round(submit['congestion'])\n",
    "submit.to_csv('./submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "2932bd28-2ce9-42a2-a9e5-382eebd1795c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hour</th>\n",
       "      <th>roadway</th>\n",
       "      <th>x_cos_hour</th>\n",
       "      <th>y_sen_hour</th>\n",
       "      <th>mean congestion</th>\n",
       "      <th>median congestion</th>\n",
       "      <th>min congestion</th>\n",
       "      <th>max congestion</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1991-04-01</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>39.555557</td>\n",
       "      <td>35.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1991-04-01</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.259260</td>\n",
       "      <td>29.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>69.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1991-04-01</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37.962963</td>\n",
       "      <td>24.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>91.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1991-04-01</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.740741</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1991-04-01</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>61.962963</td>\n",
       "      <td>63.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>72.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            hour  roadway  x_cos_hour  y_sen_hour  mean congestion  median congestion  min congestion  max congestion\n",
       "time                                                                                                                 \n",
       "1991-04-01     0        0         0.0         0.0        39.555557               35.0            30.0            80.0\n",
       "1991-04-01     0        1         0.0         0.0        30.259260               29.0            13.0            69.0\n",
       "1991-04-01     0        2        -0.0         0.0        37.962963               24.0            21.0            91.0\n",
       "1991-04-01     0        3         0.0         0.0        15.740741               17.0             0.0            26.0\n",
       "1991-04-01     0        4         0.0         0.0        61.962963               63.0            52.0            72.0"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6f0aba-8d95-4a5a-8cad-7968aaf3b0fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
