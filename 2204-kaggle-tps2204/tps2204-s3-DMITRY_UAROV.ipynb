{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-2.11.1.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import plotly as py\n",
    "import plotly.graph_objs as go\n",
    "import plotly.express as px\n",
    "from plotly.offline import init_notebook_mode\n",
    "init_notebook_mode(connected = True)\n",
    "import seaborn as sns\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from sklearn.model_selection import KFold, GroupKFold, StratifiedKFold\n",
    "\n",
    "import optuna\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.layers import GlobalMaxPooling1D\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input\n",
    "from tensorflow.keras.layers import Concatenate, LSTM, GRU\n",
    "from tensorflow.keras.layers import Bidirectional, Multiply\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "import lightgbm as lgb\n",
    "\n",
    "np.random.seed(22)\n",
    "tf.random.set_seed(22)\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "#########################################################\n",
    "\n",
    "DIR_DATASET = \"C:/Users/0stxx/Datasets/\"\n",
    "NAME_PROJECT = '2204-kaggle-tps2204'\n",
    "\n",
    "df_train = pd.read_csv(DIR_DATASET + NAME_PROJECT + '/train.csv')\n",
    "df_train_labels = pd.read_csv(DIR_DATASET + NAME_PROJECT + '/train_labels.csv')\n",
    "df_test = pd.read_csv(DIR_DATASET + NAME_PROJECT + '/test.csv')\n",
    "df_sub = pd.read_csv(DIR_DATASET + NAME_PROJECT + '/sample_submission.csv')\n",
    "\n",
    "train = df_train\n",
    "t_lbls = df_train_labels\n",
    "test = df_test\n",
    "ss = df_sub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first and most obvious features that came to mind in my first [notebook](https://www.kaggle.com/code/dmitryuarov/tps-sensors-2xlstm-xgb-auc-0-976) at the beginning of the competition were lags and differences between steps. After experimenting, first, I found new features that allowed me to get better mean val AUC on 12 folds: *rolling* + *mean/std/sum*. I also tried another functions in rolling, like *var*, *min*, *max* and *expanding* window function, but they only made the result worse. Then I added 3 experemental features because of ~0.5+ correlation between sensors.\n",
    "\n",
    "And the main thing, that turned out to be very important - correlation between state and count of sequences that the subject had. As you can see on my plot, subjects, who had more than ~95 sequences, were more likely to get target \"1\" and subjects, who had less than ~25 sequences, were more likely to get target \"0\". Therefore, dividing the sequences into three groups turned out to be a great idea. Although this improved the data, but in real life this would not have worked, because most likely we would have predicted the result of one sequence for one subject, therefore the feature about the number of sequences would have been useless.\n",
    "\n",
    "In more details, how the new features affected, I wrote below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = train.columns.tolist()[3:]\n",
    "\n",
    "def sub_imp(x):\n",
    "    if x < 25:\n",
    "        return 0\n",
    "    elif x > 95:\n",
    "        return 2\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "def prep(df):\n",
    "    for feature in features:\n",
    "        df[feature+'_lag1'] = df.groupby('sequence')[feature].shift(1)\n",
    "        df[feature+'_back_lag1'] = df.groupby('sequence')[feature].shift(-1)\n",
    "        \n",
    "        df.fillna(0, inplace=True)\n",
    "        df[feature+'_diff1'] = df[feature] - df[feature+'_lag1']\n",
    "        \n",
    "        # New features\n",
    "        for window in [3,6,12]:\n",
    "            df[feature+'_roll_'+str(window)+'_mean'] = df.groupby('sequence')[feature]\\\n",
    "            .rolling(window=window, min_periods=1).mean().reset_index(level=0,drop=True)\n",
    "            \n",
    "            df[feature+'_roll_'+str(window)+'_std'] = df.groupby('sequence')[feature]\\\n",
    "            .rolling(window=window, min_periods=1).std().reset_index(level=0,drop=True)\n",
    "            \n",
    "            df[feature+'_roll_'+str(window)+'_sum'] = df.groupby('sequence')[feature]\\\n",
    "            .rolling(window=window, min_periods=1).sum().reset_index(level=0,drop=True)\n",
    "        \n",
    "    # Experemental features\n",
    "    df['sens_00_06'] = df['sensor_00'] * df['sensor_06']\n",
    "    df['sens_03_07'] = df['sensor_03'] * df['sensor_07']\n",
    "    df['sens_03_11'] = df['sensor_03'] * df['sensor_11']\n",
    "\n",
    "    for feature in ['sens_00_06', 'sens_03_07', 'sens_03_11']:\n",
    "        df[feature + '_lag1'] = df.groupby('sequence')[feature].shift(1)\n",
    "    df.fillna(0, inplace=True)\n",
    "    \n",
    "    # Subject feature\n",
    "    sub_stat = df[['sequence', 'subject']].drop_duplicates().groupby('subject').agg({'sequence': 'count'})\\\n",
    "    .rename(columns={'sequence': 'count'}).reset_index()\n",
    "    df = df.merge(sub_stat, on='subject', how='left')\n",
    "    df['sub_imp'] = df['count'].apply(lambda x: sub_imp(x))\n",
    "    df.drop('count', axis=1, inplace=True)\n",
    "     \n",
    "prep(train)\n",
    "prep(test)\n",
    "\n",
    "features = train.columns.tolist()[3:]\n",
    "sc = StandardScaler()\n",
    "train[features] = sc.fit_transform(train[features])\n",
    "test[features] = sc.transform(test[features])\n",
    "\n",
    "groups = train[\"sequence\"]\n",
    "labels = t_lbls[\"state\"]\n",
    "\n",
    "train = train.drop([\"sequence\", \"subject\", \"step\"], axis=1).values\n",
    "train = train.reshape(-1, 60, train.shape[-1])\n",
    "\n",
    "test = test.drop([\"sequence\", \"subject\", \"step\"], axis=1).values\n",
    "test = test.reshape(-1, 60, test.shape[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on 1 replicas\n",
      "Batch Size: 256\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "    tf.config.experimental_connect_to_cluster(tpu)\n",
    "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "    tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "    BATCH_SIZE = tpu_strategy.num_replicas_in_sync * 64\n",
    "    print(\"Running on TPU:\", tpu.master())\n",
    "    print(f\"Batch Size: {BATCH_SIZE}\")\n",
    "    \n",
    "except ValueError:\n",
    "    strategy = tf.distribute.get_strategy()\n",
    "    BATCH_SIZE = 256\n",
    "    print(f\"Running on {strategy.num_replicas_in_sync} replicas\")\n",
    "    print(f\"Batch Size: {BATCH_SIZE}\")\n",
    "    \n",
    "def hist_plot(history):\n",
    "    fig = plt.figure(figsize = (5, 3))\n",
    "    plt.plot(history.history['auc'])\n",
    "    plt.plot(history.history['val_auc'])\n",
    "    plt.grid(color = 'gray', linestyle = '-', axis = 'both', linewidth=0.5, visible=0.5)\n",
    "    plt.plot(history.history['val_auc'].index(max(history.history['val_auc'])), \n",
    "         max(history.history['val_auc']), 'ko', markersize = 5,\n",
    "             fillstyle = 'full', color = 'r')\n",
    "    plt.title('model training')\n",
    "    plt.ylabel('AUC')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'val'], loc='lower right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) ', 'for plot_model/model_to_dot to work.')\n"
     ]
    }
   ],
   "source": [
    "def dnn_model():\n",
    "\n",
    "    x_input = Input(shape=(train.shape[-2:]))\n",
    "    x1 = Bidirectional(LSTM(768, return_sequences=True))(x_input)\n",
    "        \n",
    "    x21 = Bidirectional(LSTM(512, return_sequences=True))(x1)\n",
    "    x22 = Bidirectional(LSTM(512, return_sequences=True))(x_input)\n",
    "    l2 = Concatenate(axis=2)([x21, x22])\n",
    "        \n",
    "    x31 = Bidirectional(LSTM(384, return_sequences=True))(l2)\n",
    "    x32 = Bidirectional(LSTM(384, return_sequences=True))(x21)\n",
    "    l3 = Concatenate(axis=2)([x31, x32])\n",
    "        \n",
    "    x41 = Bidirectional(LSTM(256, return_sequences=True))(l3)\n",
    "    x42 = Bidirectional(LSTM(128, return_sequences=True))(x32)\n",
    "    l4 = Concatenate(axis=2)([x41, x42])\n",
    "        \n",
    "    l5 = Concatenate(axis=2)([x1, l2, l3, l4])\n",
    "    g = GlobalMaxPooling1D()(l5)\n",
    "    x7 = Dense(128, activation='selu')(g)\n",
    "    d = Dropout(0.05)(x7)\n",
    "    x_output = Dense(units=1, activation=\"sigmoid\")(d)\n",
    "    \n",
    "    model = Model(inputs=x_input, outputs=x_output, name='lstm_model')\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = dnn_model()\n",
    "\n",
    "plot_model(\n",
    "    model, \n",
    "    show_shapes=True,\n",
    "    show_layer_names=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tpu_strategy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mtpu_strategy\u001b[49m\u001b[38;5;241m.\u001b[39mscope():\n\u001b[0;32m      2\u001b[0m     VERBOSE \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m      3\u001b[0m     predictions, scores \u001b[38;5;241m=\u001b[39m [], []\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tpu_strategy' is not defined"
     ]
    }
   ],
   "source": [
    "with tpu_strategy.scope():\n",
    "    VERBOSE = False\n",
    "    predictions, scores = [], []\n",
    "    k = GroupKFold(n_splits = 12)\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(k.split(train, labels, groups.unique())):\n",
    "        print('-'*17, '>', f'Fold {fold+1}', '<', '-'*17)\n",
    "    \n",
    "        X_train, X_val = train[train_idx], train[val_idx]\n",
    "        y_train, y_val = labels.iloc[train_idx].values, labels.iloc[val_idx].values\n",
    "        \n",
    "        model = dnn_model()\n",
    "        model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics='AUC')\n",
    "\n",
    "        lr = ReduceLROnPlateau(monitor=\"val_auc\", factor=0.5, \n",
    "                               patience=1, verbose=VERBOSE, mode=\"max\")\n",
    "\n",
    "        es = EarlyStopping(monitor=\"val_auc\", patience=3, \n",
    "                           verbose=VERBOSE, mode=\"max\", \n",
    "                           restore_best_weights=True)\n",
    "        \n",
    "        save_locally = tf.saved_model.SaveOptions(experimental_io_device='/job:localhost')\n",
    "        chk_point = ModelCheckpoint(f'./TPS_model_2022_{fold+1}C.h5', options=save_locally, \n",
    "                                    monitor='val_auc', verbose=VERBOSE, \n",
    "                                    save_best_only=True, mode='max')\n",
    "        \n",
    "        training = model.fit(X_train, y_train, \n",
    "                  validation_data=(X_val, y_val), \n",
    "                  epochs=20,\n",
    "                  verbose=VERBOSE,\n",
    "                  batch_size=BATCH_SIZE, \n",
    "                  callbacks=[lr, chk_point, es])\n",
    "        \n",
    "        hist_plot(training)\n",
    "        \n",
    "        load_locally = tf.saved_model.LoadOptions(experimental_io_device='/job:localhost')\n",
    "        model = load_model(f'./TPS_model_2022_{fold+1}C.h5', options=load_locally)\n",
    "        \n",
    "        y_pred = model.predict(X_val, batch_size=BATCH_SIZE).squeeze()\n",
    "        score = roc_auc_score(y_val, y_pred)\n",
    "        scores.append(score)\n",
    "        predictions.append(model.predict(test, batch_size=BATCH_SIZE).squeeze())\n",
    "        print(f\"Fold-{fold+1} | OOF Score: {score}\")\n",
    "    \n",
    "    print('-'*40)\n",
    "    print(f'Mean AUC on {k.n_splits} folds - {np.mean(scores)}')\n",
    "\n",
    "del X_train, X_val, y_train, y_val\n",
    "ss[\"state\"] = sum(predictions)/k.n_splits "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, how new features improved the result of the model:\n",
    "\n",
    "1. Rolling features: *0.968 - 0.970*\n",
    "2. Experemental features: *0.970 - 0.971*\n",
    "3. Subject feature: *0.971 -* **0.977**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Blending and postprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking ahead, I will do what many people like to do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "s2 = pd.read_csv('../input/tps-apr/en_blend_0977.csv') # In this submission blending results from my first work (XGB + LSTM with old features)\n",
    "ss['state'] = ss['state']*0.5 + s2['state']*0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea of postprocessing is based on the number of sequences that the subjects had. There is a very dangerous point here, so in order not to take too much risk, I have set conditions suitable only for those sequences in which we can be very confident. This is a minor improvement that can improve the result on LB by about 0.0005-0.001."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('../input/tabular-playground-series-apr-2022/test.csv')\n",
    "post = ss.merge(test[['sequence', 'subject']], on='sequence', how='left').drop_duplicates()\n",
    "post = post.merge(post.groupby('subject').agg({'state':'count'})\\\n",
    "                  .reset_index().rename(columns={'state': 'count'}), on='subject', how='left')\n",
    "\n",
    "plt.title('Basic predictions')\n",
    "sns.histplot(post['state'])\n",
    "plt.show()\n",
    "\n",
    "def repredict(row):\n",
    "    if row['count'] < 20 and row['state'] < 0.3:\n",
    "        return 0.0\n",
    "    elif row['count'] > 100 and row['state'] > 0.7:\n",
    "        return 1.0\n",
    "    else:\n",
    "        return row['state']\n",
    "    \n",
    "post['repredict'] = post[['state', 'subject', 'count']].apply(lambda row: repredict(row), axis=1)\n",
    "\n",
    "print('-'*35)\n",
    "print(f\"{len(ss) - sum(post['repredict'] == post['state'])} predictions have been rounded\")\n",
    "print('-'*35)\n",
    "print()\n",
    "\n",
    "plt.title('Postprocessing predictions')\n",
    "sns.histplot(post['repredict'])\n",
    "plt.show()\n",
    "\n",
    "ss = post.drop(['state', 'subject', 'count'], axis=1).rename(columns={'repredict': 'state'})\n",
    "ss.to_csv('blend_sub31_exp.csv', index=False)\n",
    "ss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**I think I have added new ideas for new experiments and hope you enjoyed my work, so I will be glad for the upvote :)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
