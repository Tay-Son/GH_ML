{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1a9fe14-9a8c-429d-8526-cc161f8b8279",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 주요 라이브러리 임포트\n",
    "\n",
    "# 파이썬 warning 무시\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# pandas import 및 출력 설정\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "# numpy\n",
    "import numpy as np\n",
    "\n",
    "# matplotlib 및 출력 설정\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"figure.figsize\"] = (35,20)\n",
    "\n",
    "# seaborn\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a859fe07-acf6-4eb5-a127-97ef47151b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원본 데이터셋 로드\n",
    "dir_dataset = \"C:/Users/0stix/Datasets/\"\n",
    "name_project = 'hand_gesture'\n",
    "df_test = pd.read_csv(dir_dataset+name_project+'/test.csv')\n",
    "df_train = pd.read_csv(dir_dataset+name_project+'/train.csv')\n",
    "df_sub = pd.read_csv(dir_dataset+name_project+'/sample_submission.csv')\n",
    "# df_all.info()\n",
    "# df_all.head()\n",
    "\n",
    "len_train = len(df_train)\n",
    "df_all = pd.concat([df_train, df_test], axis=0).drop('id', axis=1)\n",
    "\n",
    "target = 'target'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b6ffd315-90e2-4b3d-8d95-9b13758acc02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sensor_1</th>\n",
       "      <th>sensor_2</th>\n",
       "      <th>sensor_3</th>\n",
       "      <th>sensor_4</th>\n",
       "      <th>sensor_5</th>\n",
       "      <th>sensor_6</th>\n",
       "      <th>sensor_7</th>\n",
       "      <th>sensor_8</th>\n",
       "      <th>sensor_9</th>\n",
       "      <th>sensor_10</th>\n",
       "      <th>sensor_11</th>\n",
       "      <th>sensor_12</th>\n",
       "      <th>sensor_13</th>\n",
       "      <th>sensor_14</th>\n",
       "      <th>sensor_15</th>\n",
       "      <th>sensor_16</th>\n",
       "      <th>sensor_17</th>\n",
       "      <th>sensor_18</th>\n",
       "      <th>sensor_19</th>\n",
       "      <th>sensor_20</th>\n",
       "      <th>sensor_21</th>\n",
       "      <th>sensor_22</th>\n",
       "      <th>sensor_23</th>\n",
       "      <th>sensor_24</th>\n",
       "      <th>sensor_25</th>\n",
       "      <th>sensor_26</th>\n",
       "      <th>sensor_27</th>\n",
       "      <th>sensor_28</th>\n",
       "      <th>sensor_29</th>\n",
       "      <th>sensor_30</th>\n",
       "      <th>sensor_31</th>\n",
       "      <th>sensor_32</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-6.149463</td>\n",
       "      <td>-0.929714</td>\n",
       "      <td>9.058368</td>\n",
       "      <td>-7.017854</td>\n",
       "      <td>-2.958471</td>\n",
       "      <td>0.179233</td>\n",
       "      <td>-0.956591</td>\n",
       "      <td>-0.972401</td>\n",
       "      <td>5.956213</td>\n",
       "      <td>4.145636</td>\n",
       "      <td>25.017645</td>\n",
       "      <td>-4.061254</td>\n",
       "      <td>0.996632</td>\n",
       "      <td>-3.837345</td>\n",
       "      <td>-13.956994</td>\n",
       "      <td>-2.042957</td>\n",
       "      <td>2.130210</td>\n",
       "      <td>-1.957662</td>\n",
       "      <td>-1.149930</td>\n",
       "      <td>6.082028</td>\n",
       "      <td>0.878612</td>\n",
       "      <td>5.093102</td>\n",
       "      <td>-6.066648</td>\n",
       "      <td>-7.026436</td>\n",
       "      <td>-6.006282</td>\n",
       "      <td>-6.005836</td>\n",
       "      <td>7.043084</td>\n",
       "      <td>21.884650</td>\n",
       "      <td>-3.064152</td>\n",
       "      <td>-5.247552</td>\n",
       "      <td>-6.026107</td>\n",
       "      <td>-11.990822</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-2.238836</td>\n",
       "      <td>-1.003511</td>\n",
       "      <td>5.098079</td>\n",
       "      <td>-10.880357</td>\n",
       "      <td>-0.804562</td>\n",
       "      <td>-2.992123</td>\n",
       "      <td>26.972724</td>\n",
       "      <td>-8.900861</td>\n",
       "      <td>-5.968298</td>\n",
       "      <td>-4.060134</td>\n",
       "      <td>2.952843</td>\n",
       "      <td>-5.046353</td>\n",
       "      <td>1.083819</td>\n",
       "      <td>3.978378</td>\n",
       "      <td>-25.072542</td>\n",
       "      <td>-2.041602</td>\n",
       "      <td>2.912269</td>\n",
       "      <td>-3.998035</td>\n",
       "      <td>6.069698</td>\n",
       "      <td>4.966187</td>\n",
       "      <td>1.994051</td>\n",
       "      <td>-1.132059</td>\n",
       "      <td>14.906205</td>\n",
       "      <td>-1.996714</td>\n",
       "      <td>-7.933806</td>\n",
       "      <td>-3.136773</td>\n",
       "      <td>8.774211</td>\n",
       "      <td>10.944759</td>\n",
       "      <td>9.858186</td>\n",
       "      <td>-0.969241</td>\n",
       "      <td>-3.935553</td>\n",
       "      <td>-15.892421</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.087934</td>\n",
       "      <td>-2.092514</td>\n",
       "      <td>0.946750</td>\n",
       "      <td>-21.831788</td>\n",
       "      <td>9.119235</td>\n",
       "      <td>17.853587</td>\n",
       "      <td>-21.069954</td>\n",
       "      <td>-15.933212</td>\n",
       "      <td>-9.016039</td>\n",
       "      <td>-5.975194</td>\n",
       "      <td>-23.218408</td>\n",
       "      <td>-9.000630</td>\n",
       "      <td>9.115957</td>\n",
       "      <td>12.097318</td>\n",
       "      <td>-10.954367</td>\n",
       "      <td>-3.930714</td>\n",
       "      <td>-19.069594</td>\n",
       "      <td>-6.118940</td>\n",
       "      <td>-5.001346</td>\n",
       "      <td>-9.105371</td>\n",
       "      <td>-9.894885</td>\n",
       "      <td>10.107614</td>\n",
       "      <td>4.948570</td>\n",
       "      <td>-6.889685</td>\n",
       "      <td>54.052330</td>\n",
       "      <td>-6.109238</td>\n",
       "      <td>12.154595</td>\n",
       "      <td>6.095989</td>\n",
       "      <td>-40.195088</td>\n",
       "      <td>-3.958124</td>\n",
       "      <td>-8.079537</td>\n",
       "      <td>-5.160090</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-2.211629</td>\n",
       "      <td>-1.930904</td>\n",
       "      <td>21.888406</td>\n",
       "      <td>-3.067560</td>\n",
       "      <td>-0.240634</td>\n",
       "      <td>2.985056</td>\n",
       "      <td>-29.073369</td>\n",
       "      <td>0.200774</td>\n",
       "      <td>-1.043742</td>\n",
       "      <td>2.099845</td>\n",
       "      <td>-15.123774</td>\n",
       "      <td>-0.069867</td>\n",
       "      <td>-0.114247</td>\n",
       "      <td>-1.896109</td>\n",
       "      <td>5.127194</td>\n",
       "      <td>-2.877423</td>\n",
       "      <td>2.970044</td>\n",
       "      <td>-1.099702</td>\n",
       "      <td>3.116767</td>\n",
       "      <td>8.124209</td>\n",
       "      <td>-0.917418</td>\n",
       "      <td>-1.027199</td>\n",
       "      <td>14.048298</td>\n",
       "      <td>-2.126170</td>\n",
       "      <td>-1.035526</td>\n",
       "      <td>2.178769</td>\n",
       "      <td>10.032723</td>\n",
       "      <td>-1.010897</td>\n",
       "      <td>-3.912848</td>\n",
       "      <td>-2.980338</td>\n",
       "      <td>-12.983597</td>\n",
       "      <td>-3.001077</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.953852</td>\n",
       "      <td>2.964892</td>\n",
       "      <td>-36.044802</td>\n",
       "      <td>0.899838</td>\n",
       "      <td>26.930210</td>\n",
       "      <td>11.004409</td>\n",
       "      <td>-21.962423</td>\n",
       "      <td>-11.950189</td>\n",
       "      <td>-20.933785</td>\n",
       "      <td>-4.000506</td>\n",
       "      <td>16.010442</td>\n",
       "      <td>5.961219</td>\n",
       "      <td>9.907115</td>\n",
       "      <td>-0.067754</td>\n",
       "      <td>-9.970728</td>\n",
       "      <td>0.868499</td>\n",
       "      <td>1.892233</td>\n",
       "      <td>-3.161698</td>\n",
       "      <td>-9.225990</td>\n",
       "      <td>3.953956</td>\n",
       "      <td>-17.959652</td>\n",
       "      <td>-3.115491</td>\n",
       "      <td>-6.051674</td>\n",
       "      <td>-2.051761</td>\n",
       "      <td>10.917567</td>\n",
       "      <td>1.905335</td>\n",
       "      <td>-13.004707</td>\n",
       "      <td>17.169552</td>\n",
       "      <td>2.105194</td>\n",
       "      <td>3.967986</td>\n",
       "      <td>11.861657</td>\n",
       "      <td>-27.088846</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    sensor_1  sensor_2   sensor_3   sensor_4   sensor_5   sensor_6   sensor_7   sensor_8   sensor_9  sensor_10  sensor_11  sensor_12  sensor_13  sensor_14  sensor_15  sensor_16  sensor_17  sensor_18  sensor_19  sensor_20  sensor_21  sensor_22  sensor_23  sensor_24  sensor_25  sensor_26  sensor_27  sensor_28  sensor_29  sensor_30  sensor_31  sensor_32  target\n",
       "0  -6.149463 -0.929714   9.058368  -7.017854  -2.958471   0.179233  -0.956591  -0.972401   5.956213   4.145636  25.017645  -4.061254   0.996632  -3.837345 -13.956994  -2.042957   2.130210  -1.957662  -1.149930   6.082028   0.878612   5.093102  -6.066648  -7.026436  -6.006282  -6.005836   7.043084  21.884650  -3.064152  -5.247552  -6.026107 -11.990822     1.0\n",
       "1  -2.238836 -1.003511   5.098079 -10.880357  -0.804562  -2.992123  26.972724  -8.900861  -5.968298  -4.060134   2.952843  -5.046353   1.083819   3.978378 -25.072542  -2.041602   2.912269  -3.998035   6.069698   4.966187   1.994051  -1.132059  14.906205  -1.996714  -7.933806  -3.136773   8.774211  10.944759   9.858186  -0.969241  -3.935553 -15.892421     1.0\n",
       "2  19.087934 -2.092514   0.946750 -21.831788   9.119235  17.853587 -21.069954 -15.933212  -9.016039  -5.975194 -23.218408  -9.000630   9.115957  12.097318 -10.954367  -3.930714 -19.069594  -6.118940  -5.001346  -9.105371  -9.894885  10.107614   4.948570  -6.889685  54.052330  -6.109238  12.154595   6.095989 -40.195088  -3.958124  -8.079537  -5.160090     0.0\n",
       "3  -2.211629 -1.930904  21.888406  -3.067560  -0.240634   2.985056 -29.073369   0.200774  -1.043742   2.099845 -15.123774  -0.069867  -0.114247  -1.896109   5.127194  -2.877423   2.970044  -1.099702   3.116767   8.124209  -0.917418  -1.027199  14.048298  -2.126170  -1.035526   2.178769  10.032723  -1.010897  -3.912848  -2.980338 -12.983597  -3.001077     1.0\n",
       "4   3.953852  2.964892 -36.044802   0.899838  26.930210  11.004409 -21.962423 -11.950189 -20.933785  -4.000506  16.010442   5.961219   9.907115  -0.067754  -9.970728   0.868499   1.892233  -3.161698  -9.225990   3.953956 -17.959652  -3.115491  -6.051674  -2.051761  10.917567   1.905335 -13.004707  17.169552   2.105194   3.967986  11.861657 -27.088846     2.0"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all.head()\n",
    "# df_all.info()\n",
    "# df_all.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "5208358e-8fec-4b75-a2e0-a7b54abab507",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x19c80043640>"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "mat = df_all.iloc[:,:32].corr().values\n",
    "for idx_ in range(len(mat)):\n",
    "    mat[idx_,idx_] = 0.0\n",
    "max_ = np.max(mat)\n",
    "img = Image.fromarray(mat/max_*255)\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c785291c-67f3-439f-bb2d-c9698b083d71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0    599\n",
       "2.0    593\n",
       "1.0    574\n",
       "0.0    569\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all[target].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "64a578c0-7aed-49fa-b0ca-f835292f08b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations as cb\n",
    "df_v0 = df_all.copy()\n",
    "\n",
    "for each_ in df_v0.drop(target, axis=1):\n",
    "    min_ = np.min(df_v0[each_])\n",
    "    max_ = np.max(df_v0[each_])\n",
    "    \n",
    "    df_v0[each_] = (df_v0[each_] - min_) / (max_ - min_)\n",
    "    \n",
    "#     df_v0[each_] = df_v0[each_] - np.mean(df_v0[each_])\n",
    "#     df_v0[each_] = df_v0[each_]/np.var(df_v0[each_]) + 1\n",
    "\n",
    "# for a_, b_ in cb(range(1, 33), 2):\n",
    "#     str_ = 'd '+ str(a_).zfill(2) + '_'+ str(b_).zfill(2)\n",
    "#     df_v0[str_] = df_v0['sensor_'+str(a_)] - df_v0['sensor_'+str(b_)]\n",
    "# cnt_ = 0\n",
    "\n",
    "# for a_, b_ in cb(df_v0.drop(target, axis=1).columns, 2):\n",
    "#     str_ = str(cnt_)\n",
    "#     df_v0[str_] = df_v0[b_] - df_v0[a_]\n",
    "#     cnt_ += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "b0ccfb88-d83c-420e-87a1-6eda7bf80879",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2335, 32), (9343, 32), (2335,))"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_ = df_v0.drop(target, axis=1).values\n",
    "X_train, X_test = X_[:len_train], X_[len_train:]\n",
    "y_train = df_v0[target][:len_train].values\n",
    "\n",
    "X_train.shape, X_test.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "0109cee6-2ddb-4ab9-93a9-09f3c570cd40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:15:55] WARNING: ..\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[22:15:55] WARNING: ..\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[22:15:56] WARNING: ..\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[22:15:56] WARNING: ..\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[22:15:57] WARNING: ..\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.812847965738758,\n",
       " array([0.82441113, 0.81156317, 0.81156317, 0.81798715, 0.7987152 ]))"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from xgboost import XGBClassifier\n",
    "xgb_clf = XGBClassifier()\n",
    "scores = cross_val_score(xgb_clf, X_train, y_train, scoring='accuracy', cv=5)\n",
    "np.mean(scores), scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "10323d0d-578b-4b12-bc2d-a8eceed6e400",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x19cd5e78bd0>"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "88b56607-2b24-440e-b49d-19f0b3c43afe",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH   0, loss:0.1394, accuracy:0.9569 \n",
      "EPOCH   1, loss:0.1266, accuracy:0.9556 \n",
      "EPOCH   2, loss:0.0700, accuracy:0.9802 \n",
      "EPOCH   3, loss:0.0640, accuracy:0.9866 \n",
      "EPOCH   4, loss:0.1519, accuracy:0.9565 \n",
      "EPOCH   5, loss:0.0190, accuracy:1.0017 \n",
      "EPOCH   6, loss:0.0359, accuracy:0.9931 \n",
      "EPOCH   7, loss:0.3321, accuracy:0.8983 \n",
      "EPOCH   8, loss:0.1572, accuracy:0.9457 \n",
      "EPOCH   9, loss:0.0432, accuracy:0.9922 \n",
      "EPOCH  10, loss:0.0438, accuracy:0.9927 \n",
      "EPOCH  11, loss:0.1038, accuracy:0.9728 \n",
      "EPOCH  12, loss:0.4313, accuracy:0.8711 \n",
      "EPOCH  13, loss:0.1140, accuracy:0.9659 \n",
      "EPOCH  14, loss:0.0249, accuracy:1.0004 \n",
      "EPOCH  15, loss:0.0089, accuracy:1.0060 \n",
      "EPOCH  16, loss:0.0083, accuracy:1.0056 \n",
      "EPOCH  17, loss:0.0116, accuracy:1.0043 \n",
      "EPOCH  18, loss:0.0578, accuracy:0.9853 \n",
      "EPOCH  19, loss:0.1459, accuracy:0.9509 \n",
      "EPOCH  20, loss:0.2558, accuracy:0.9194 \n",
      "EPOCH  21, loss:0.1504, accuracy:0.9539 \n",
      "EPOCH  22, loss:0.0228, accuracy:1.0013 \n",
      "EPOCH  23, loss:0.0334, accuracy:0.9974 \n",
      "EPOCH  24, loss:0.1573, accuracy:0.9565 \n",
      "EPOCH  25, loss:0.1001, accuracy:0.9711 \n",
      "EPOCH  26, loss:0.0424, accuracy:0.9901 \n",
      "EPOCH  27, loss:0.0897, accuracy:0.9746 \n",
      "EPOCH  28, loss:0.2833, accuracy:0.9164 \n",
      "EPOCH  29, loss:0.1468, accuracy:0.9560 \n",
      "EPOCH  30, loss:0.0538, accuracy:0.9879 \n",
      "EPOCH  31, loss:0.0220, accuracy:0.9996 \n",
      "EPOCH  32, loss:0.0268, accuracy:0.9991 \n",
      "EPOCH  33, loss:0.0653, accuracy:0.9819 \n",
      "EPOCH  34, loss:0.3231, accuracy:0.9017 \n",
      "EPOCH  35, loss:0.1324, accuracy:0.9591 \n",
      "EPOCH  36, loss:0.0429, accuracy:0.9918 \n",
      "EPOCH  37, loss:0.0114, accuracy:1.0043 \n",
      "EPOCH  38, loss:0.0160, accuracy:1.0043 \n",
      "EPOCH  39, loss:0.0050, accuracy:1.0065 \n",
      "EPOCH  40, loss:0.0053, accuracy:1.0065 \n",
      "EPOCH  41, loss:0.0037, accuracy:1.0065 \n",
      "EPOCH  42, loss:0.0034, accuracy:1.0065 \n",
      "EPOCH  43, loss:0.0037, accuracy:1.0060 \n",
      "EPOCH  44, loss:0.5328, accuracy:0.8703 \n",
      "EPOCH  45, loss:0.4299, accuracy:0.8612 \n",
      "EPOCH  46, loss:0.2311, accuracy:0.9216 \n",
      "EPOCH  47, loss:0.0652, accuracy:0.9849 \n",
      "EPOCH  48, loss:0.0373, accuracy:0.9974 \n",
      "EPOCH  49, loss:0.0708, accuracy:0.9819 \n",
      "EPOCH  50, loss:0.1318, accuracy:0.9522 \n",
      "EPOCH  51, loss:0.0641, accuracy:0.9866 \n",
      "EPOCH  52, loss:0.1391, accuracy:0.9522 \n",
      "EPOCH  53, loss:0.1024, accuracy:0.9694 \n",
      "EPOCH  54, loss:0.1153, accuracy:0.9681 \n",
      "EPOCH  55, loss:0.3131, accuracy:0.9026 \n",
      "EPOCH  56, loss:0.1384, accuracy:0.9547 \n",
      "EPOCH  57, loss:0.0350, accuracy:0.9987 \n",
      "EPOCH  58, loss:0.0098, accuracy:1.0056 \n",
      "EPOCH  59, loss:0.0060, accuracy:1.0060 \n",
      "EPOCH  60, loss:0.0039, accuracy:1.0065 \n",
      "EPOCH  61, loss:0.0260, accuracy:0.9953 \n",
      "EPOCH  62, loss:0.3316, accuracy:0.9052 \n",
      "EPOCH  63, loss:0.1486, accuracy:0.9500 \n",
      "EPOCH  64, loss:0.0780, accuracy:0.9780 \n",
      "EPOCH  65, loss:0.0135, accuracy:1.0022 \n",
      "EPOCH  66, loss:0.0056, accuracy:1.0065 \n",
      "EPOCH  67, loss:0.0035, accuracy:1.0065 \n",
      "EPOCH  68, loss:0.0049, accuracy:1.0060 \n",
      "EPOCH  69, loss:0.5346, accuracy:0.8582 \n",
      "EPOCH  70, loss:0.2347, accuracy:0.9241 \n",
      "EPOCH  71, loss:0.0678, accuracy:0.9853 \n",
      "EPOCH  72, loss:0.0623, accuracy:0.9828 \n",
      "EPOCH  73, loss:0.0631, accuracy:0.9832 \n",
      "EPOCH  74, loss:0.2200, accuracy:0.9237 \n",
      "EPOCH  75, loss:0.3054, accuracy:0.8953 \n",
      "EPOCH  76, loss:0.0707, accuracy:0.9841 \n",
      "EPOCH  77, loss:0.0186, accuracy:1.0034 \n",
      "EPOCH  78, loss:0.0675, accuracy:0.9836 \n",
      "EPOCH  79, loss:0.1671, accuracy:0.9466 \n",
      "EPOCH  80, loss:0.0932, accuracy:0.9746 \n",
      "EPOCH  81, loss:0.0583, accuracy:0.9866 \n",
      "EPOCH  82, loss:0.0523, accuracy:0.9884 \n",
      "EPOCH  83, loss:0.0253, accuracy:0.9991 \n",
      "EPOCH  84, loss:0.0103, accuracy:1.0052 \n",
      "EPOCH  85, loss:0.1135, accuracy:0.9776 \n",
      "EPOCH  86, loss:0.4353, accuracy:0.8711 \n",
      "EPOCH  87, loss:0.0319, accuracy:0.9991 \n",
      "EPOCH  88, loss:0.0269, accuracy:0.9996 \n",
      "EPOCH  89, loss:0.0083, accuracy:1.0060 \n",
      "EPOCH  90, loss:0.0052, accuracy:1.0065 \n",
      "EPOCH  91, loss:0.0040, accuracy:1.0065 \n",
      "EPOCH  92, loss:0.0042, accuracy:1.0065 \n",
      "EPOCH  93, loss:0.0038, accuracy:1.0065 \n",
      "EPOCH  94, loss:0.0024, accuracy:1.0065 \n",
      "EPOCH  95, loss:0.0030, accuracy:1.0065 \n",
      "EPOCH  96, loss:0.0027, accuracy:1.0065 \n",
      "EPOCH  97, loss:0.0019, accuracy:1.0065 \n",
      "EPOCH  98, loss:0.3342, accuracy:0.9194 \n",
      "EPOCH  99, loss:1.1957, accuracy:0.5668 \n",
      "EPOCH 100, loss:0.5902, accuracy:0.7789 \n",
      "EPOCH 101, loss:0.3317, accuracy:0.8832 \n",
      "EPOCH 102, loss:0.2425, accuracy:0.9228 \n",
      "EPOCH 103, loss:0.1936, accuracy:0.9414 \n",
      "EPOCH 104, loss:0.1569, accuracy:0.9466 \n",
      "EPOCH 105, loss:0.1802, accuracy:0.9392 \n",
      "EPOCH 106, loss:0.1362, accuracy:0.9573 \n",
      "EPOCH 107, loss:0.1525, accuracy:0.9530 \n",
      "EPOCH 108, loss:0.1304, accuracy:0.9569 \n",
      "EPOCH 109, loss:0.0446, accuracy:0.9978 \n",
      "EPOCH 110, loss:0.0864, accuracy:0.9763 \n",
      "EPOCH 111, loss:0.2484, accuracy:0.9194 \n",
      "EPOCH 112, loss:0.1289, accuracy:0.9608 \n",
      "EPOCH 113, loss:0.0733, accuracy:0.9789 \n",
      "EPOCH 114, loss:0.0425, accuracy:0.9953 \n",
      "EPOCH 115, loss:0.0461, accuracy:0.9953 \n",
      "EPOCH 116, loss:0.3204, accuracy:0.9022 \n",
      "EPOCH 117, loss:0.1499, accuracy:0.9582 \n",
      "EPOCH 118, loss:0.0589, accuracy:0.9836 \n",
      "EPOCH 119, loss:0.0698, accuracy:0.9810 \n",
      "EPOCH 120, loss:0.0759, accuracy:0.9789 \n",
      "EPOCH 121, loss:0.2007, accuracy:0.9371 \n",
      "EPOCH 122, loss:0.1770, accuracy:0.9414 \n",
      "EPOCH 123, loss:0.0859, accuracy:0.9754 \n",
      "EPOCH 124, loss:0.0499, accuracy:0.9905 \n",
      "EPOCH 125, loss:0.3020, accuracy:0.9056 \n",
      "EPOCH 126, loss:0.0959, accuracy:0.9759 \n",
      "EPOCH 127, loss:0.0222, accuracy:1.0013 \n",
      "EPOCH 128, loss:0.0245, accuracy:0.9991 \n",
      "EPOCH 129, loss:0.0326, accuracy:0.9957 \n",
      "EPOCH 130, loss:0.0453, accuracy:0.9914 \n",
      "EPOCH 131, loss:0.1339, accuracy:0.9582 \n",
      "EPOCH 132, loss:0.2603, accuracy:0.9147 \n",
      "EPOCH 133, loss:0.1121, accuracy:0.9625 \n",
      "EPOCH 134, loss:0.1110, accuracy:0.9655 \n",
      "EPOCH 135, loss:0.0817, accuracy:0.9784 \n",
      "EPOCH 136, loss:0.0232, accuracy:0.9991 \n",
      "EPOCH 137, loss:0.0135, accuracy:1.0052 \n",
      "EPOCH 138, loss:0.0132, accuracy:1.0030 \n",
      "EPOCH 139, loss:0.0098, accuracy:1.0056 \n",
      "EPOCH 140, loss:0.0096, accuracy:1.0056 \n",
      "EPOCH 141, loss:0.2362, accuracy:0.9366 \n",
      "EPOCH 142, loss:0.5891, accuracy:0.8250 \n",
      "EPOCH 143, loss:0.1396, accuracy:0.9556 \n",
      "EPOCH 144, loss:0.0734, accuracy:0.9815 \n",
      "EPOCH 145, loss:0.0516, accuracy:0.9909 \n",
      "EPOCH 146, loss:0.0550, accuracy:0.9884 \n",
      "EPOCH 147, loss:0.1587, accuracy:0.9444 \n",
      "EPOCH 148, loss:0.1370, accuracy:0.9586 \n",
      "EPOCH 149, loss:0.0861, accuracy:0.9759 \n",
      "EPOCH 150, loss:0.0269, accuracy:0.9987 \n",
      "EPOCH 151, loss:0.0173, accuracy:1.0009 \n",
      "EPOCH 152, loss:0.0079, accuracy:1.0060 \n",
      "EPOCH 153, loss:0.0043, accuracy:1.0060 \n",
      "EPOCH 154, loss:0.0036, accuracy:1.0065 \n",
      "EPOCH 155, loss:0.0037, accuracy:1.0060 \n",
      "EPOCH 156, loss:0.0465, accuracy:0.9897 \n",
      "EPOCH 157, loss:0.6795, accuracy:0.7948 \n",
      "EPOCH 158, loss:0.2556, accuracy:0.9203 \n",
      "EPOCH 159, loss:0.0707, accuracy:0.9849 \n",
      "EPOCH 160, loss:0.0321, accuracy:0.9961 \n",
      "EPOCH 161, loss:0.0198, accuracy:1.0013 \n",
      "EPOCH 162, loss:0.0885, accuracy:0.9746 \n",
      "EPOCH 163, loss:0.2140, accuracy:0.9315 \n",
      "EPOCH 164, loss:0.0738, accuracy:0.9841 \n",
      "EPOCH 165, loss:0.0223, accuracy:1.0022 \n",
      "EPOCH 166, loss:0.0534, accuracy:0.9871 \n",
      "EPOCH 167, loss:0.1303, accuracy:0.9599 \n",
      "EPOCH 168, loss:0.0886, accuracy:0.9741 \n",
      "EPOCH 169, loss:0.1046, accuracy:0.9694 \n",
      "EPOCH 170, loss:0.0488, accuracy:0.9901 \n",
      "EPOCH 171, loss:0.0086, accuracy:1.0056 \n",
      "EPOCH 172, loss:0.0073, accuracy:1.0052 \n",
      "EPOCH 173, loss:0.0883, accuracy:0.9819 \n",
      "EPOCH 174, loss:0.2816, accuracy:0.9112 \n",
      "EPOCH 175, loss:0.1589, accuracy:0.9534 \n",
      "EPOCH 176, loss:0.1682, accuracy:0.9466 \n",
      "EPOCH 177, loss:0.0191, accuracy:1.0034 \n",
      "EPOCH 178, loss:0.0163, accuracy:1.0030 \n",
      "EPOCH 179, loss:0.0062, accuracy:1.0060 \n",
      "EPOCH 180, loss:0.0033, accuracy:1.0060 \n",
      "EPOCH 181, loss:0.0042, accuracy:1.0065 \n",
      "EPOCH 182, loss:0.0039, accuracy:1.0065 \n",
      "EPOCH 183, loss:0.0136, accuracy:1.0013 \n",
      "EPOCH 184, loss:0.5182, accuracy:0.8461 \n",
      "EPOCH 185, loss:0.2236, accuracy:0.9289 \n",
      "EPOCH 186, loss:0.0967, accuracy:0.9733 \n",
      "EPOCH 187, loss:0.1888, accuracy:0.9418 \n",
      "EPOCH 188, loss:0.0185, accuracy:1.0026 \n",
      "EPOCH 189, loss:0.0066, accuracy:1.0060 \n",
      "EPOCH 190, loss:0.0056, accuracy:1.0060 \n",
      "EPOCH 191, loss:0.0049, accuracy:1.0060 \n",
      "EPOCH 192, loss:0.0039, accuracy:1.0065 \n",
      "EPOCH 193, loss:0.0040, accuracy:1.0065 \n",
      "EPOCH 194, loss:0.0025, accuracy:1.0065 \n",
      "EPOCH 195, loss:0.0026, accuracy:1.0065 \n",
      "EPOCH 196, loss:0.0020, accuracy:1.0065 \n",
      "EPOCH 197, loss:0.0022, accuracy:1.0065 \n",
      "EPOCH 198, loss:0.0015, accuracy:1.0065 \n",
      "EPOCH 199, loss:0.0014, accuracy:1.0065 \n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
    "\n",
    "for epoch in range(200):\n",
    "    running_loss = 0.0\n",
    "    accuracy = 0\n",
    "    for i, data in enumerate(train_dataloader, 0):\n",
    "        inputs, labels = data\n",
    "\n",
    "        optimizer.zero_grad() # 매개변수를 0으로 만듭니다. 매 학습시 초기화해줘야합니다.\n",
    "        outputs = model(inputs) # 입력값을 넣어 순전파를 진행시킵니다.\n",
    "\n",
    "        loss = criterion(outputs, labels) # 모델 출력값와 실제값을 손실함수에 대입합니다.\n",
    "        loss.backward() # 손실함수에서 역전파 수행합니다.\n",
    "        optimizer.step() # 옵티마이저를 사용해 매개변수를 최적화합니다.\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        _, predictions = torch.max(outputs, 1)\n",
    "\n",
    "        for label, prediction in zip(labels, predictions):\n",
    "            if label == prediction:\n",
    "                accuracy = accuracy + 1\n",
    "\n",
    "        \n",
    "                \n",
    "    print('EPOCH {:3}, loss:{:5.4f}, accuracy:{:5.4f} '.format(epoch, running_loss / i, accuracy / (i * 16)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "07b9fdcf-f3b8-4443-9821-523b13f64669",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 1,  ..., 2, 0, 3])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval() # 모델을 평가모드로 바꿉니다. dropout이 일어나지 않습니다.\n",
    "\n",
    "with torch.no_grad(): # 이 안의 코드는 가중치 업데이트가 일어나지 않습니다.\n",
    "    outputs = model(test_x)\n",
    "    _, pred = torch.max(outputs, 1)\n",
    "\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "cc70540c-c15d-450c-956c-62ea50391c45",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    3243\n",
       "2    2237\n",
       "3    1984\n",
       "0    1879\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sub['target'] = pred.numpy()\n",
    "df_sub['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "0b90d957-598e-4ced-9193-359fe3a78eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "36c02682-0ec9-4478-8457-e5b035f404a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your Machine is: NVIDIA GeForce RTX 3070\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "print('Your Machine is: ' + torch.cuda.get_device_name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "76f05e86-0219-455a-ae90-1e4473ab6902",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_workers = 0\n",
    "batch_size = 16\n",
    "is_cuda = True if torch.cuda.is_available() else False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "d8ef0750-634b-4435-b3c9-a1ebda1e96ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = torch.from_numpy(X_train).float()\n",
    "train_y = torch.tensor(y_train, dtype = torch.int64)\n",
    "test_x = torch.from_numpy(X_test).float()\n",
    "\n",
    "train_dataset = TensorDataset(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "2b67110c-0ccc-4949-b9bd-68143c266571",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 32])\n",
      "tensor([1, 3, 1, 1, 1, 2, 1, 0, 3, 0, 3, 0, 3, 2, 2, 0])\n"
     ]
    }
   ],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size = 16, shuffle = True)\n",
    "\n",
    "for batch_idx, samples in enumerate(train_dataloader):\n",
    "    if batch_idx > 0:\n",
    "        break\n",
    "    print(samples[0].shape)\n",
    "    print(samples[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "51e629d0-e3b4-48ce-815a-725d9995b35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "a1d94b03-c713-4bf2-b323-e4a9cb3109ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1868, 32]), torch.Size([1868]))"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transform = transforms.ToTensor()\n",
    "\n",
    "X_ = df_v0.drop(target, axis=1).values\n",
    "X_train, X_te = X_[:len_train], X_[len_train:]\n",
    "y_train = df_v0[target][:len_train].values\n",
    "\n",
    "X_tr, X_va, y_tr, y_va = train_test_split(X_train, y_train, test_size=0.2, shuffle=True)\n",
    "\n",
    "X_tr = torch.from_numpy(X_tr).float()\n",
    "X_va = torch.from_numpy(X_va).float()\n",
    "X_te = torch.from_numpy(X_te).float()\n",
    "\n",
    "y_tr = torch.tensor(y_tr, dtype=torch.int64)\n",
    "y_va = torch.tensor(y_va, dtype=torch.int64)\n",
    "\n",
    "X_tr.size(), y_tr.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "a4068697-c614-44dd-89d2-9d97fdccd12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(X_tr, y_tr)\n",
    "val_dataset = TensorDataset(X_va, y_va)\n",
    "test_dataset = TensorDataset(X_te)\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "# train_data = datasets.MNIST(root='data', train=True, download=True, transform=transform)\n",
    "# test_data = datasets.MNIST(root='data', train=False, download=True, transform=transform)\n",
    "\n",
    "# train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, num_workers=num_workers, shuffle=True)\n",
    "# test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, num_workers=num_workers, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "bf74e615-1e29-4ee3-aea9-bcf75e5fb1ff",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models(\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=32, out_features=128, bias=True)\n",
      "    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.2, inplace=False)\n",
      "    (4): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (5): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): ReLU()\n",
      "    (7): Dropout(p=0.2, inplace=False)\n",
      "    (8): Linear(in_features=128, out_features=4, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Models(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(32, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, 4),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear_relu_stack(x)\n",
    "        return x\n",
    "\n",
    "model = Models()\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "04122dee-3583-45be-bc19-754519d456a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=32, out_features=1024, bias=True)\n",
      "    (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.5, inplace=False)\n",
      "    (4): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (5): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): ReLU()\n",
      "    (7): Dropout(p=0.5, inplace=False)\n",
      "    (8): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (9): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): ReLU()\n",
      "    (11): Dropout(p=0.5, inplace=False)\n",
      "    (12): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (13): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (14): ReLU()\n",
      "    (15): Dropout(p=0.5, inplace=False)\n",
      "    (16): Linear(in_features=1024, out_features=4, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch.nn.init as I\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(32, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            \n",
    "            nn.Linear(1024, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            \n",
    "            nn.Linear(1024, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            \n",
    "            nn.Linear(1024, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "#             nn.Linear(512, 512),\n",
    "#             nn.BatchNorm1d(512),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(0.2),\n",
    "            \n",
    "            nn.Linear(1024, 4),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear_relu_stack(x)\n",
    "        return x\n",
    "        \n",
    "        #         super(Net, self).__init__()\n",
    "#         self.conv1 = nn.Conv2d(1, 16, kernel_size=5)\n",
    "#         self.fc1 = nn.Linear(12 * 12 * 16, 100)\n",
    "#         self.fc2 = nn.Linear(100, 10)\n",
    "        \n",
    "# #         I.kaiming_normal_(self.conv1.weight)\n",
    "# #         I.kaiming_normal_(self.fc1.weight)\n",
    "# #         I.kaiming_normal_(self.fc2.weight)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = F.max_pool2d(F.relu(self.conv1(x)), 2)\n",
    "#         x = F.relu(self.fc1(x.view(-1, 12 * 12 * 16)))\n",
    "#         x = F.relu(self.fc2(x))\n",
    "#         return x\n",
    "\n",
    "model = Net()\n",
    "if is_cuda:\n",
    "    model.cuda()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "4332951e-fdc6-4101-894c-679851c47b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "if is_cuda:\n",
    "    criterion.cuda()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "b6ede7d7-5fdf-4f00-97ee-b8973136e253",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_loss = 10\n",
    "best_state = None\n",
    "cnt_epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "cf43a693-8de5-4bf3-a8ea-88a806089e21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is running.\n",
      "Epoch   1: \tTraining Loss:  1.4195,\tValidation Loss:  1.5198\n",
      "Epoch   2: \tTraining Loss:  1.3374,\tValidation Loss:  1.5767\n",
      "Epoch   3: \tTraining Loss:  1.3161,\tValidation Loss:  1.5440\n",
      "Epoch   4: \tTraining Loss:  1.2659,\tValidation Loss:  1.5304\n",
      "Epoch   5: \tTraining Loss:  1.2399,\tValidation Loss:  1.4166\n",
      "Epoch   6: \tTraining Loss:  1.1560,\tValidation Loss:  1.5007\n",
      "Epoch   7: \tTraining Loss:  1.1393,\tValidation Loss:  1.4185\n",
      "Epoch   8: \tTraining Loss:  1.0735,\tValidation Loss:  1.4178\n",
      "Epoch   9: \tTraining Loss:  1.0473,\tValidation Loss:  1.4126\n",
      "Epoch  10: \tTraining Loss:  1.0210,\tValidation Loss:  1.4603\n",
      "Epoch  11: \tTraining Loss:  0.9831,\tValidation Loss:  1.2872\n",
      "Epoch  12: \tTraining Loss:  0.9562,\tValidation Loss:  1.3131\n",
      "Epoch  13: \tTraining Loss:  0.9410,\tValidation Loss:  1.2280\n",
      "Epoch  14: \tTraining Loss:  0.9132,\tValidation Loss:  1.2518\n",
      "Epoch  15: \tTraining Loss:  0.8700,\tValidation Loss:  1.2417\n",
      "Epoch  16: \tTraining Loss:  0.9020,\tValidation Loss:  1.2475\n",
      "Epoch  17: \tTraining Loss:  0.8703,\tValidation Loss:  1.1580\n",
      "Epoch  18: \tTraining Loss:  0.8505,\tValidation Loss:  1.0949\n",
      "Epoch  19: \tTraining Loss:  0.8315,\tValidation Loss:  1.1295\n",
      "Epoch  20: \tTraining Loss:  0.8065,\tValidation Loss:  1.1211\n",
      "Epoch  21: \tTraining Loss:  0.8072,\tValidation Loss:  1.0642\n",
      "Epoch  22: \tTraining Loss:  0.7886,\tValidation Loss:  1.1674\n",
      "Epoch  23: \tTraining Loss:  0.7912,\tValidation Loss:  1.1100\n",
      "Epoch  24: \tTraining Loss:  0.7500,\tValidation Loss:  1.0391\n",
      "Epoch  25: \tTraining Loss:  0.7342,\tValidation Loss:  1.1160\n",
      "Epoch  26: \tTraining Loss:  0.7505,\tValidation Loss:  1.0473\n",
      "Epoch  27: \tTraining Loss:  0.6996,\tValidation Loss:  0.9907\n",
      "Epoch  28: \tTraining Loss:  0.7316,\tValidation Loss:  1.0689\n",
      "Epoch  29: \tTraining Loss:  0.7616,\tValidation Loss:  0.9596\n",
      "Epoch  30: \tTraining Loss:  0.6761,\tValidation Loss:  0.9695\n",
      "Epoch  31: \tTraining Loss:  0.7171,\tValidation Loss:  0.9365\n",
      "Epoch  32: \tTraining Loss:  0.6715,\tValidation Loss:  0.9603\n",
      "Epoch  33: \tTraining Loss:  0.6761,\tValidation Loss:  0.9191\n",
      "Epoch  34: \tTraining Loss:  0.6501,\tValidation Loss:  1.0137\n",
      "Epoch  35: \tTraining Loss:  0.6489,\tValidation Loss:  0.8862\n",
      "Epoch  36: \tTraining Loss:  0.6523,\tValidation Loss:  0.9495\n",
      "Epoch  37: \tTraining Loss:  0.6306,\tValidation Loss:  0.9112\n",
      "Epoch  38: \tTraining Loss:  0.6381,\tValidation Loss:  0.8612\n",
      "Epoch  39: \tTraining Loss:  0.6272,\tValidation Loss:  0.9088\n",
      "Epoch  40: \tTraining Loss:  0.6189,\tValidation Loss:  0.8561\n",
      "Epoch  41: \tTraining Loss:  0.6228,\tValidation Loss:  0.8171\n",
      "Epoch  42: \tTraining Loss:  0.6504,\tValidation Loss:  0.7895\n",
      "Epoch  43: \tTraining Loss:  0.6319,\tValidation Loss:  0.8926\n",
      "Epoch  44: \tTraining Loss:  0.5742,\tValidation Loss:  0.8654\n",
      "Epoch  45: \tTraining Loss:  0.6023,\tValidation Loss:  0.7925\n",
      "Epoch  46: \tTraining Loss:  0.5788,\tValidation Loss:  0.7998\n",
      "Epoch  47: \tTraining Loss:  0.6043,\tValidation Loss:  0.8059\n",
      "Epoch  48: \tTraining Loss:  0.5585,\tValidation Loss:  0.8165\n",
      "Epoch  49: \tTraining Loss:  0.5661,\tValidation Loss:  0.8306\n",
      "Epoch  50: \tTraining Loss:  0.5902,\tValidation Loss:  0.8664\n",
      "Epoch  51: \tTraining Loss:  0.5827,\tValidation Loss:  0.6827\n",
      "Epoch  52: \tTraining Loss:  0.5372,\tValidation Loss:  0.7775\n",
      "Epoch  53: \tTraining Loss:  0.5615,\tValidation Loss:  0.7456\n",
      "Epoch  54: \tTraining Loss:  0.5465,\tValidation Loss:  0.7133\n",
      "Epoch  55: \tTraining Loss:  0.5670,\tValidation Loss:  0.6709\n",
      "Epoch  56: \tTraining Loss:  0.5628,\tValidation Loss:  0.7282\n",
      "Epoch  57: \tTraining Loss:  0.5412,\tValidation Loss:  0.8234\n",
      "Epoch  58: \tTraining Loss:  0.5100,\tValidation Loss:  0.8423\n",
      "Epoch  59: \tTraining Loss:  0.5043,\tValidation Loss:  0.7677\n",
      "Epoch  60: \tTraining Loss:  0.5395,\tValidation Loss:  0.7732\n",
      "Epoch  61: \tTraining Loss:  0.4923,\tValidation Loss:  0.7673\n",
      "Epoch  62: \tTraining Loss:  0.5180,\tValidation Loss:  0.7401\n",
      "Epoch  63: \tTraining Loss:  0.5234,\tValidation Loss:  0.7495\n",
      "Epoch  64: \tTraining Loss:  0.5177,\tValidation Loss:  0.7086\n",
      "Epoch  65: \tTraining Loss:  0.4942,\tValidation Loss:  0.7466\n",
      "Epoch  66: \tTraining Loss:  0.5203,\tValidation Loss:  0.6563\n",
      "Epoch  67: \tTraining Loss:  0.4868,\tValidation Loss:  0.7317\n",
      "Epoch  68: \tTraining Loss:  0.4935,\tValidation Loss:  0.7042\n",
      "Epoch  69: \tTraining Loss:  0.4885,\tValidation Loss:  0.6930\n",
      "Epoch  70: \tTraining Loss:  0.5177,\tValidation Loss:  0.7653\n",
      "Epoch  71: \tTraining Loss:  0.4527,\tValidation Loss:  0.7241\n",
      "Epoch  72: \tTraining Loss:  0.4698,\tValidation Loss:  0.7055\n",
      "Epoch  73: \tTraining Loss:  0.4804,\tValidation Loss:  0.6957\n",
      "Epoch  74: \tTraining Loss:  0.4651,\tValidation Loss:  0.6880\n",
      "Epoch  75: \tTraining Loss:  0.5074,\tValidation Loss:  0.6280\n",
      "Epoch  76: \tTraining Loss:  0.4306,\tValidation Loss:  0.6748\n",
      "Epoch  77: \tTraining Loss:  0.4805,\tValidation Loss:  0.6506\n",
      "Epoch  78: \tTraining Loss:  0.4407,\tValidation Loss:  0.7375\n",
      "Epoch  79: \tTraining Loss:  0.4820,\tValidation Loss:  0.6853\n",
      "Epoch  80: \tTraining Loss:  0.4386,\tValidation Loss:  0.7000\n",
      "Epoch  81: \tTraining Loss:  0.4265,\tValidation Loss:  0.7038\n",
      "Epoch  82: \tTraining Loss:  0.4664,\tValidation Loss:  0.8185\n",
      "Epoch  83: \tTraining Loss:  0.4505,\tValidation Loss:  0.6511\n",
      "Epoch  84: \tTraining Loss:  0.4420,\tValidation Loss:  0.6532\n",
      "Epoch  85: \tTraining Loss:  0.4518,\tValidation Loss:  0.6491\n",
      "Epoch  86: \tTraining Loss:  0.4372,\tValidation Loss:  0.7338\n",
      "Epoch  87: \tTraining Loss:  0.4371,\tValidation Loss:  0.6028\n",
      "Epoch  88: \tTraining Loss:  0.4242,\tValidation Loss:  0.7540\n",
      "Epoch  89: \tTraining Loss:  0.4207,\tValidation Loss:  0.7218\n",
      "Epoch  90: \tTraining Loss:  0.4095,\tValidation Loss:  0.6386\n",
      "Epoch  91: \tTraining Loss:  0.4166,\tValidation Loss:  0.7166\n",
      "Epoch  92: \tTraining Loss:  0.4215,\tValidation Loss:  0.7207\n",
      "Epoch  93: \tTraining Loss:  0.4271,\tValidation Loss:  0.6673\n",
      "Epoch  94: \tTraining Loss:  0.4236,\tValidation Loss:  0.7037\n",
      "Epoch  95: \tTraining Loss:  0.4575,\tValidation Loss:  0.6598\n",
      "Epoch  96: \tTraining Loss:  0.4110,\tValidation Loss:  0.6442\n",
      "Epoch  97: \tTraining Loss:  0.4240,\tValidation Loss:  0.6280\n",
      "Epoch  98: \tTraining Loss:  0.4236,\tValidation Loss:  0.6193\n",
      "Epoch  99: \tTraining Loss:  0.4265,\tValidation Loss:  0.6421\n",
      "Epoch 100: \tTraining Loss:  0.3898,\tValidation Loss:  0.6726\n",
      "Epoch 101: \tTraining Loss:  0.3757,\tValidation Loss:  0.6562\n",
      "Epoch 102: \tTraining Loss:  0.4143,\tValidation Loss:  0.6541\n",
      "Epoch 103: \tTraining Loss:  0.3778,\tValidation Loss:  0.7347\n",
      "Epoch 104: \tTraining Loss:  0.3945,\tValidation Loss:  0.7010\n",
      "Epoch 105: \tTraining Loss:  0.3840,\tValidation Loss:  0.6224\n",
      "Epoch 106: \tTraining Loss:  0.4133,\tValidation Loss:  0.6435\n",
      "Epoch 107: \tTraining Loss:  0.3809,\tValidation Loss:  0.6285\n",
      "Epoch 108: \tTraining Loss:  0.3990,\tValidation Loss:  0.6417\n",
      "Epoch 109: \tTraining Loss:  0.4027,\tValidation Loss:  0.6202\n",
      "Epoch 110: \tTraining Loss:  0.3646,\tValidation Loss:  0.6238\n",
      "Epoch 111: \tTraining Loss:  0.3823,\tValidation Loss:  0.6311\n",
      "Epoch 112: \tTraining Loss:  0.3761,\tValidation Loss:  0.6061\n",
      "Epoch 113: \tTraining Loss:  0.3942,\tValidation Loss:  0.6618\n",
      "Epoch 114: \tTraining Loss:  0.3936,\tValidation Loss:  0.6034\n",
      "Epoch 115: \tTraining Loss:  0.3789,\tValidation Loss:  0.6443\n",
      "Epoch 116: \tTraining Loss:  0.3546,\tValidation Loss:  0.6170\n",
      "Epoch 117: \tTraining Loss:  0.3917,\tValidation Loss:  0.6277\n",
      "Epoch 118: \tTraining Loss:  0.3555,\tValidation Loss:  0.6598\n",
      "Epoch 119: \tTraining Loss:  0.3458,\tValidation Loss:  0.6421\n",
      "Epoch 120: \tTraining Loss:  0.3873,\tValidation Loss:  0.6313\n",
      "Epoch 121: \tTraining Loss:  0.3717,\tValidation Loss:  0.6472\n",
      "Epoch 122: \tTraining Loss:  0.3798,\tValidation Loss:  0.6492\n",
      "Epoch 123: \tTraining Loss:  0.3418,\tValidation Loss:  0.6916\n",
      "Epoch 124: \tTraining Loss:  0.3347,\tValidation Loss:  0.7059\n",
      "Epoch 125: \tTraining Loss:  0.3825,\tValidation Loss:  0.6319\n",
      "Epoch 126: \tTraining Loss:  0.3760,\tValidation Loss:  0.6399\n",
      "Epoch 127: \tTraining Loss:  0.3476,\tValidation Loss:  0.5821\n",
      "Epoch 128: \tTraining Loss:  0.3749,\tValidation Loss:  0.6566\n",
      "Epoch 129: \tTraining Loss:  0.3346,\tValidation Loss:  0.6236\n",
      "Epoch 130: \tTraining Loss:  0.3645,\tValidation Loss:  0.6290\n",
      "Epoch 131: \tTraining Loss:  0.3333,\tValidation Loss:  0.6523\n",
      "Epoch 132: \tTraining Loss:  0.3516,\tValidation Loss:  0.7174\n",
      "Epoch 133: \tTraining Loss:  0.3231,\tValidation Loss:  0.6626\n",
      "Epoch 134: \tTraining Loss:  0.3501,\tValidation Loss:  0.6263\n",
      "Epoch 135: \tTraining Loss:  0.3211,\tValidation Loss:  0.6019\n",
      "Epoch 136: \tTraining Loss:  0.3456,\tValidation Loss:  0.6100\n",
      "Epoch 137: \tTraining Loss:  0.3705,\tValidation Loss:  0.6004\n",
      "Epoch 138: \tTraining Loss:  0.3344,\tValidation Loss:  0.6293\n",
      "Epoch 139: \tTraining Loss:  0.3219,\tValidation Loss:  0.6472\n",
      "Epoch 140: \tTraining Loss:  0.3110,\tValidation Loss:  0.5595\n",
      "Epoch 141: \tTraining Loss:  0.3606,\tValidation Loss:  0.6105\n",
      "Epoch 142: \tTraining Loss:  0.3251,\tValidation Loss:  0.6374\n",
      "Epoch 143: \tTraining Loss:  0.3378,\tValidation Loss:  0.6125\n",
      "Epoch 144: \tTraining Loss:  0.3349,\tValidation Loss:  0.6679\n",
      "Epoch 145: \tTraining Loss:  0.3660,\tValidation Loss:  0.6598\n",
      "Epoch 146: \tTraining Loss:  0.3284,\tValidation Loss:  0.6565\n",
      "Epoch 147: \tTraining Loss:  0.3203,\tValidation Loss:  0.6453\n",
      "Epoch 148: \tTraining Loss:  0.3368,\tValidation Loss:  0.6407\n",
      "Epoch 149: \tTraining Loss:  0.3446,\tValidation Loss:  0.5989\n",
      "Epoch 150: \tTraining Loss:  0.3401,\tValidation Loss:  0.6393\n",
      "Epoch 151: \tTraining Loss:  0.3139,\tValidation Loss:  0.6462\n",
      "Epoch 152: \tTraining Loss:  0.3252,\tValidation Loss:  0.6671\n",
      "Epoch 153: \tTraining Loss:  0.3235,\tValidation Loss:  0.6460\n",
      "Epoch 154: \tTraining Loss:  0.3150,\tValidation Loss:  0.6297\n",
      "Epoch 155: \tTraining Loss:  0.3337,\tValidation Loss:  0.6389\n",
      "Epoch 156: \tTraining Loss:  0.3539,\tValidation Loss:  0.6075\n",
      "Epoch 157: \tTraining Loss:  0.3018,\tValidation Loss:  0.6131\n",
      "Epoch 158: \tTraining Loss:  0.2898,\tValidation Loss:  0.7022\n",
      "Epoch 159: \tTraining Loss:  0.3034,\tValidation Loss:  0.6437\n",
      "Epoch 160: \tTraining Loss:  0.2990,\tValidation Loss:  0.5963\n",
      "Epoch 161: \tTraining Loss:  0.3122,\tValidation Loss:  0.6411\n",
      "Epoch 162: \tTraining Loss:  0.3189,\tValidation Loss:  0.6155\n",
      "Epoch 163: \tTraining Loss:  0.3137,\tValidation Loss:  0.6871\n",
      "Epoch 164: \tTraining Loss:  0.3212,\tValidation Loss:  0.7114\n",
      "Epoch 165: \tTraining Loss:  0.3321,\tValidation Loss:  0.6457\n",
      "Epoch 166: \tTraining Loss:  0.3024,\tValidation Loss:  0.7030\n",
      "Epoch 167: \tTraining Loss:  0.3096,\tValidation Loss:  0.6177\n",
      "Epoch 168: \tTraining Loss:  0.3103,\tValidation Loss:  0.6868\n",
      "Epoch 169: \tTraining Loss:  0.3260,\tValidation Loss:  0.6699\n",
      "Epoch 170: \tTraining Loss:  0.3021,\tValidation Loss:  0.6496\n",
      "Epoch 171: \tTraining Loss:  0.3013,\tValidation Loss:  0.6051\n",
      "Epoch 172: \tTraining Loss:  0.3052,\tValidation Loss:  0.6480\n",
      "Epoch 173: \tTraining Loss:  0.3167,\tValidation Loss:  0.6336\n",
      "Epoch 174: \tTraining Loss:  0.2849,\tValidation Loss:  0.6399\n",
      "Epoch 175: \tTraining Loss:  0.2859,\tValidation Loss:  0.6880\n",
      "Epoch 176: \tTraining Loss:  0.3052,\tValidation Loss:  0.6411\n",
      "Epoch 177: \tTraining Loss:  0.2852,\tValidation Loss:  0.6961\n",
      "Epoch 178: \tTraining Loss:  0.3060,\tValidation Loss:  0.6691\n",
      "Epoch 179: \tTraining Loss:  0.2891,\tValidation Loss:  0.6332\n",
      "Epoch 180: \tTraining Loss:  0.2905,\tValidation Loss:  0.6932\n",
      "Epoch 181: \tTraining Loss:  0.2968,\tValidation Loss:  0.6993\n",
      "Epoch 182: \tTraining Loss:  0.2796,\tValidation Loss:  0.6613\n",
      "Epoch 183: \tTraining Loss:  0.2750,\tValidation Loss:  0.6775\n",
      "Epoch 184: \tTraining Loss:  0.2900,\tValidation Loss:  0.6253\n",
      "Epoch 185: \tTraining Loss:  0.2944,\tValidation Loss:  0.6561\n",
      "Epoch 186: \tTraining Loss:  0.2739,\tValidation Loss:  0.6682\n",
      "Epoch 187: \tTraining Loss:  0.2621,\tValidation Loss:  0.6265\n",
      "Epoch 188: \tTraining Loss:  0.2990,\tValidation Loss:  0.7112\n",
      "Epoch 189: \tTraining Loss:  0.2646,\tValidation Loss:  0.6354\n",
      "Epoch 190: \tTraining Loss:  0.3024,\tValidation Loss:  0.6849\n",
      "Epoch 191: \tTraining Loss:  0.2854,\tValidation Loss:  0.6118\n",
      "Epoch 192: \tTraining Loss:  0.2763,\tValidation Loss:  0.6827\n",
      "Epoch 193: \tTraining Loss:  0.3190,\tValidation Loss:  0.6374\n",
      "Epoch 194: \tTraining Loss:  0.2738,\tValidation Loss:  0.6605\n",
      "Epoch 195: \tTraining Loss:  0.2836,\tValidation Loss:  0.6715\n",
      "Epoch 196: \tTraining Loss:  0.2677,\tValidation Loss:  0.6963\n",
      "Epoch 197: \tTraining Loss:  0.2618,\tValidation Loss:  0.6845\n",
      "Epoch 198: \tTraining Loss:  0.2995,\tValidation Loss:  0.6866\n",
      "Epoch 199: \tTraining Loss:  0.2540,\tValidation Loss:  0.6459\n",
      "Epoch 200: \tTraining Loss:  0.2636,\tValidation Loss:  0.6497\n",
      "Epoch 201: \tTraining Loss:  0.2643,\tValidation Loss:  0.6916\n",
      "Epoch 202: \tTraining Loss:  0.2831,\tValidation Loss:  0.6463\n",
      "Epoch 203: \tTraining Loss:  0.3056,\tValidation Loss:  0.6622\n",
      "Epoch 204: \tTraining Loss:  0.2670,\tValidation Loss:  0.6238\n",
      "Epoch 205: \tTraining Loss:  0.2463,\tValidation Loss:  0.7152\n",
      "Epoch 206: \tTraining Loss:  0.2875,\tValidation Loss:  0.6542\n",
      "Epoch 207: \tTraining Loss:  0.2751,\tValidation Loss:  0.6464\n",
      "Epoch 208: \tTraining Loss:  0.2863,\tValidation Loss:  0.6514\n",
      "Epoch 209: \tTraining Loss:  0.2599,\tValidation Loss:  0.6450\n",
      "Epoch 210: \tTraining Loss:  0.2651,\tValidation Loss:  0.6648\n",
      "Epoch 211: \tTraining Loss:  0.2485,\tValidation Loss:  0.6360\n",
      "Epoch 212: \tTraining Loss:  0.2456,\tValidation Loss:  0.6479\n",
      "Epoch 213: \tTraining Loss:  0.2378,\tValidation Loss:  0.5985\n",
      "Epoch 214: \tTraining Loss:  0.2746,\tValidation Loss:  0.6536\n",
      "Epoch 215: \tTraining Loss:  0.2434,\tValidation Loss:  0.6515\n",
      "Epoch 216: \tTraining Loss:  0.2789,\tValidation Loss:  0.6428\n",
      "Epoch 217: \tTraining Loss:  0.2486,\tValidation Loss:  0.6432\n",
      "Epoch 218: \tTraining Loss:  0.2757,\tValidation Loss:  0.6520\n",
      "Epoch 219: \tTraining Loss:  0.2927,\tValidation Loss:  0.6680\n",
      "Epoch 220: \tTraining Loss:  0.2521,\tValidation Loss:  0.6211\n",
      "Epoch 221: \tTraining Loss:  0.2380,\tValidation Loss:  0.6007\n",
      "Epoch 222: \tTraining Loss:  0.2784,\tValidation Loss:  0.6170\n",
      "Epoch 223: \tTraining Loss:  0.2724,\tValidation Loss:  0.7148\n",
      "Epoch 224: \tTraining Loss:  0.2462,\tValidation Loss:  0.6814\n",
      "Epoch 225: \tTraining Loss:  0.2466,\tValidation Loss:  0.6374\n",
      "Epoch 226: \tTraining Loss:  0.2616,\tValidation Loss:  0.6492\n",
      "Epoch 227: \tTraining Loss:  0.2542,\tValidation Loss:  0.6220\n",
      "Epoch 228: \tTraining Loss:  0.2648,\tValidation Loss:  0.6366\n",
      "Epoch 229: \tTraining Loss:  0.2597,\tValidation Loss:  0.6475\n",
      "Epoch 230: \tTraining Loss:  0.2511,\tValidation Loss:  0.6468\n",
      "Epoch 231: \tTraining Loss:  0.2504,\tValidation Loss:  0.6290\n",
      "Epoch 232: \tTraining Loss:  0.2589,\tValidation Loss:  0.6315\n",
      "Epoch 233: \tTraining Loss:  0.2530,\tValidation Loss:  0.6444\n",
      "Epoch 234: \tTraining Loss:  0.2627,\tValidation Loss:  0.6510\n",
      "Epoch 235: \tTraining Loss:  0.2436,\tValidation Loss:  0.6307\n",
      "Epoch 236: \tTraining Loss:  0.2674,\tValidation Loss:  0.6608\n",
      "Epoch 237: \tTraining Loss:  0.2557,\tValidation Loss:  0.6783\n",
      "Epoch 238: \tTraining Loss:  0.2211,\tValidation Loss:  0.6909\n",
      "Epoch 239: \tTraining Loss:  0.2422,\tValidation Loss:  0.6899\n",
      "Epoch 240: \tTraining Loss:  0.2416,\tValidation Loss:  0.6421\n",
      "Epoch 241: \tTraining Loss:  0.2386,\tValidation Loss:  0.6805\n",
      "Epoch 242: \tTraining Loss:  0.2576,\tValidation Loss:  0.6585\n",
      "Epoch 243: \tTraining Loss:  0.2577,\tValidation Loss:  0.6624\n",
      "Epoch 244: \tTraining Loss:  0.2674,\tValidation Loss:  0.6649\n",
      "Epoch 245: \tTraining Loss:  0.2610,\tValidation Loss:  0.6466\n",
      "Epoch 246: \tTraining Loss:  0.2202,\tValidation Loss:  0.6327\n",
      "Epoch 247: \tTraining Loss:  0.2456,\tValidation Loss:  0.6467\n",
      "Epoch 248: \tTraining Loss:  0.2308,\tValidation Loss:  0.6617\n",
      "Epoch 249: \tTraining Loss:  0.2441,\tValidation Loss:  0.6876\n",
      "Epoch 250: \tTraining Loss:  0.2178,\tValidation Loss:  0.6501\n",
      "Epoch 251: \tTraining Loss:  0.2457,\tValidation Loss:  0.6507\n",
      "Epoch 252: \tTraining Loss:  0.2120,\tValidation Loss:  0.6477\n",
      "Epoch 253: \tTraining Loss:  0.2414,\tValidation Loss:  0.6324\n",
      "Epoch 254: \tTraining Loss:  0.2085,\tValidation Loss:  0.6656\n",
      "Epoch 255: \tTraining Loss:  0.2743,\tValidation Loss:  0.6789\n",
      "Epoch 256: \tTraining Loss:  0.2429,\tValidation Loss:  0.6960\n",
      "Epoch 257: \tTraining Loss:  0.2651,\tValidation Loss:  0.6700\n",
      "Epoch 258: \tTraining Loss:  0.2111,\tValidation Loss:  0.6799\n",
      "Epoch 259: \tTraining Loss:  0.2274,\tValidation Loss:  0.6718\n",
      "Epoch 260: \tTraining Loss:  0.2197,\tValidation Loss:  0.6818\n",
      "Epoch 261: \tTraining Loss:  0.2349,\tValidation Loss:  0.6980\n",
      "Epoch 262: \tTraining Loss:  0.2141,\tValidation Loss:  0.6781\n",
      "Epoch 263: \tTraining Loss:  0.2447,\tValidation Loss:  0.7035\n",
      "Epoch 264: \tTraining Loss:  0.2106,\tValidation Loss:  0.7337\n",
      "Epoch 265: \tTraining Loss:  0.2394,\tValidation Loss:  0.6533\n",
      "Epoch 266: \tTraining Loss:  0.2277,\tValidation Loss:  0.6706\n",
      "Epoch 267: \tTraining Loss:  0.2240,\tValidation Loss:  0.6708\n",
      "Epoch 268: \tTraining Loss:  0.2230,\tValidation Loss:  0.6648\n",
      "Epoch 269: \tTraining Loss:  0.2478,\tValidation Loss:  0.6332\n",
      "Epoch 270: \tTraining Loss:  0.2067,\tValidation Loss:  0.6513\n",
      "Epoch 271: \tTraining Loss:  0.2239,\tValidation Loss:  0.6551\n",
      "Epoch 272: \tTraining Loss:  0.1952,\tValidation Loss:  0.6688\n",
      "Epoch 273: \tTraining Loss:  0.2135,\tValidation Loss:  0.6439\n",
      "Epoch 274: \tTraining Loss:  0.2392,\tValidation Loss:  0.6790\n",
      "Epoch 275: \tTraining Loss:  0.2187,\tValidation Loss:  0.6989\n",
      "Epoch 276: \tTraining Loss:  0.2467,\tValidation Loss:  0.6239\n",
      "Epoch 277: \tTraining Loss:  0.2278,\tValidation Loss:  0.6302\n",
      "Epoch 278: \tTraining Loss:  0.2330,\tValidation Loss:  0.6184\n",
      "Epoch 279: \tTraining Loss:  0.2253,\tValidation Loss:  0.6685\n",
      "Epoch 280: \tTraining Loss:  0.2280,\tValidation Loss:  0.6168\n",
      "Epoch 281: \tTraining Loss:  0.2064,\tValidation Loss:  0.6734\n",
      "Epoch 282: \tTraining Loss:  0.2592,\tValidation Loss:  0.6401\n",
      "Epoch 283: \tTraining Loss:  0.2271,\tValidation Loss:  0.6338\n",
      "Epoch 284: \tTraining Loss:  0.2232,\tValidation Loss:  0.6784\n",
      "Epoch 285: \tTraining Loss:  0.2071,\tValidation Loss:  0.6536\n",
      "Epoch 286: \tTraining Loss:  0.2148,\tValidation Loss:  0.7196\n",
      "Epoch 287: \tTraining Loss:  0.2331,\tValidation Loss:  0.6747\n",
      "Epoch 288: \tTraining Loss:  0.2237,\tValidation Loss:  0.6542\n",
      "Epoch 289: \tTraining Loss:  0.2121,\tValidation Loss:  0.7024\n",
      "Epoch 290: \tTraining Loss:  0.2062,\tValidation Loss:  0.6906\n",
      "Epoch 291: \tTraining Loss:  0.2148,\tValidation Loss:  0.6861\n",
      "Epoch 292: \tTraining Loss:  0.2373,\tValidation Loss:  0.6077\n",
      "Epoch 293: \tTraining Loss:  0.2362,\tValidation Loss:  0.6615\n",
      "Epoch 294: \tTraining Loss:  0.2294,\tValidation Loss:  0.6575\n",
      "Epoch 295: \tTraining Loss:  0.1946,\tValidation Loss:  0.6561\n",
      "Epoch 296: \tTraining Loss:  0.2202,\tValidation Loss:  0.6694\n",
      "Epoch 297: \tTraining Loss:  0.1865,\tValidation Loss:  0.6723\n",
      "Epoch 298: \tTraining Loss:  0.2153,\tValidation Loss:  0.6661\n",
      "Epoch 299: \tTraining Loss:  0.2173,\tValidation Loss:  0.6577\n",
      "Epoch 300: \tTraining Loss:  0.2204,\tValidation Loss:  0.7109\n",
      "Epoch 301: \tTraining Loss:  0.2096,\tValidation Loss:  0.6485\n",
      "Epoch 302: \tTraining Loss:  0.2192,\tValidation Loss:  0.6611\n",
      "Epoch 303: \tTraining Loss:  0.2026,\tValidation Loss:  0.7334\n",
      "Epoch 304: \tTraining Loss:  0.2205,\tValidation Loss:  0.6266\n",
      "Epoch 305: \tTraining Loss:  0.2142,\tValidation Loss:  0.6747\n",
      "Epoch 306: \tTraining Loss:  0.2159,\tValidation Loss:  0.6696\n",
      "Epoch 307: \tTraining Loss:  0.2006,\tValidation Loss:  0.6902\n",
      "Epoch 308: \tTraining Loss:  0.2148,\tValidation Loss:  0.6694\n",
      "Epoch 309: \tTraining Loss:  0.2054,\tValidation Loss:  0.6513\n",
      "Epoch 310: \tTraining Loss:  0.2278,\tValidation Loss:  0.6769\n",
      "Epoch 311: \tTraining Loss:  0.1762,\tValidation Loss:  0.7020\n",
      "Epoch 312: \tTraining Loss:  0.1948,\tValidation Loss:  0.6378\n",
      "Epoch 313: \tTraining Loss:  0.2054,\tValidation Loss:  0.6533\n",
      "Epoch 314: \tTraining Loss:  0.2046,\tValidation Loss:  0.6598\n",
      "Epoch 315: \tTraining Loss:  0.2043,\tValidation Loss:  0.6470\n",
      "Epoch 316: \tTraining Loss:  0.2049,\tValidation Loss:  0.6792\n",
      "Epoch 317: \tTraining Loss:  0.2315,\tValidation Loss:  0.6890\n",
      "Epoch 318: \tTraining Loss:  0.2085,\tValidation Loss:  0.6839\n",
      "Epoch 319: \tTraining Loss:  0.2034,\tValidation Loss:  0.6784\n",
      "Epoch 320: \tTraining Loss:  0.2159,\tValidation Loss:  0.6383\n",
      "Epoch 321: \tTraining Loss:  0.2040,\tValidation Loss:  0.6805\n",
      "Epoch 322: \tTraining Loss:  0.2008,\tValidation Loss:  0.6363\n",
      "Epoch 323: \tTraining Loss:  0.1811,\tValidation Loss:  0.6988\n",
      "Epoch 324: \tTraining Loss:  0.1909,\tValidation Loss:  0.7558\n",
      "Epoch 325: \tTraining Loss:  0.2019,\tValidation Loss:  0.7079\n",
      "Epoch 326: \tTraining Loss:  0.2229,\tValidation Loss:  0.6721\n",
      "Epoch 327: \tTraining Loss:  0.2251,\tValidation Loss:  0.6763\n",
      "Epoch 328: \tTraining Loss:  0.1815,\tValidation Loss:  0.6377\n",
      "Epoch 329: \tTraining Loss:  0.1845,\tValidation Loss:  0.6758\n",
      "Epoch 330: \tTraining Loss:  0.1975,\tValidation Loss:  0.6765\n",
      "Epoch 331: \tTraining Loss:  0.1911,\tValidation Loss:  0.7058\n",
      "Epoch 332: \tTraining Loss:  0.2037,\tValidation Loss:  0.6930\n",
      "Epoch 333: \tTraining Loss:  0.2155,\tValidation Loss:  0.6876\n",
      "Epoch 334: \tTraining Loss:  0.1810,\tValidation Loss:  0.6846\n",
      "Epoch 335: \tTraining Loss:  0.1986,\tValidation Loss:  0.6639\n",
      "Epoch 336: \tTraining Loss:  0.1808,\tValidation Loss:  0.7080\n",
      "Epoch 337: \tTraining Loss:  0.1983,\tValidation Loss:  0.6777\n",
      "Epoch 338: \tTraining Loss:  0.2075,\tValidation Loss:  0.6555\n",
      "Epoch 339: \tTraining Loss:  0.2118,\tValidation Loss:  0.6376\n",
      "Epoch 340: \tTraining Loss:  0.2057,\tValidation Loss:  0.6664\n",
      "Epoch 341: \tTraining Loss:  0.1917,\tValidation Loss:  0.7117\n",
      "Epoch 342: \tTraining Loss:  0.1746,\tValidation Loss:  0.6540\n",
      "Epoch 343: \tTraining Loss:  0.1899,\tValidation Loss:  0.6077\n",
      "Epoch 344: \tTraining Loss:  0.2079,\tValidation Loss:  0.6324\n",
      "Epoch 345: \tTraining Loss:  0.1800,\tValidation Loss:  0.6452\n",
      "Epoch 346: \tTraining Loss:  0.1881,\tValidation Loss:  0.6496\n",
      "Epoch 347: \tTraining Loss:  0.1962,\tValidation Loss:  0.6313\n",
      "Epoch 348: \tTraining Loss:  0.2366,\tValidation Loss:  0.6364\n",
      "Epoch 349: \tTraining Loss:  0.2077,\tValidation Loss:  0.5976\n",
      "Epoch 350: \tTraining Loss:  0.1935,\tValidation Loss:  0.6912\n",
      "Epoch 351: \tTraining Loss:  0.2015,\tValidation Loss:  0.6305\n",
      "Epoch 352: \tTraining Loss:  0.1762,\tValidation Loss:  0.6588\n",
      "Epoch 353: \tTraining Loss:  0.1851,\tValidation Loss:  0.6993\n",
      "Epoch 354: \tTraining Loss:  0.2069,\tValidation Loss:  0.6429\n",
      "Epoch 355: \tTraining Loss:  0.1863,\tValidation Loss:  0.6978\n",
      "Epoch 356: \tTraining Loss:  0.2215,\tValidation Loss:  0.6714\n",
      "Epoch 357: \tTraining Loss:  0.1914,\tValidation Loss:  0.6681\n",
      "Epoch 358: \tTraining Loss:  0.1648,\tValidation Loss:  0.6555\n",
      "Epoch 359: \tTraining Loss:  0.2033,\tValidation Loss:  0.6360\n",
      "Epoch 360: \tTraining Loss:  0.1922,\tValidation Loss:  0.6965\n",
      "Epoch 361: \tTraining Loss:  0.1937,\tValidation Loss:  0.6806\n",
      "Epoch 362: \tTraining Loss:  0.2116,\tValidation Loss:  0.6477\n",
      "Epoch 363: \tTraining Loss:  0.1970,\tValidation Loss:  0.6423\n",
      "Epoch 364: \tTraining Loss:  0.1969,\tValidation Loss:  0.5927\n",
      "Epoch 365: \tTraining Loss:  0.1915,\tValidation Loss:  0.6389\n",
      "Epoch 366: \tTraining Loss:  0.1933,\tValidation Loss:  0.6658\n",
      "Epoch 367: \tTraining Loss:  0.1776,\tValidation Loss:  0.6800\n",
      "Epoch 368: \tTraining Loss:  0.1939,\tValidation Loss:  0.6721\n",
      "Epoch 369: \tTraining Loss:  0.2169,\tValidation Loss:  0.6492\n",
      "Epoch 370: \tTraining Loss:  0.2120,\tValidation Loss:  0.6633\n",
      "Epoch 371: \tTraining Loss:  0.2159,\tValidation Loss:  0.6362\n",
      "Epoch 372: \tTraining Loss:  0.1692,\tValidation Loss:  0.7041\n",
      "Epoch 373: \tTraining Loss:  0.1833,\tValidation Loss:  0.7194\n",
      "Epoch 374: \tTraining Loss:  0.1931,\tValidation Loss:  0.6741\n",
      "Epoch 375: \tTraining Loss:  0.1624,\tValidation Loss:  0.7060\n",
      "Epoch 376: \tTraining Loss:  0.1798,\tValidation Loss:  0.7280\n",
      "Epoch 377: \tTraining Loss:  0.1955,\tValidation Loss:  0.7081\n",
      "Epoch 378: \tTraining Loss:  0.1749,\tValidation Loss:  0.6807\n",
      "Epoch 379: \tTraining Loss:  0.1930,\tValidation Loss:  0.7210\n",
      "Epoch 380: \tTraining Loss:  0.1957,\tValidation Loss:  0.6984\n",
      "Epoch 381: \tTraining Loss:  0.1847,\tValidation Loss:  0.7138\n",
      "Epoch 382: \tTraining Loss:  0.1731,\tValidation Loss:  0.7048\n",
      "Epoch 383: \tTraining Loss:  0.1833,\tValidation Loss:  0.6971\n",
      "Epoch 384: \tTraining Loss:  0.1756,\tValidation Loss:  0.7397\n",
      "Epoch 385: \tTraining Loss:  0.1491,\tValidation Loss:  0.7289\n",
      "Epoch 386: \tTraining Loss:  0.1888,\tValidation Loss:  0.6513\n",
      "Epoch 387: \tTraining Loss:  0.1919,\tValidation Loss:  0.6877\n",
      "Epoch 388: \tTraining Loss:  0.1614,\tValidation Loss:  0.6495\n",
      "Epoch 389: \tTraining Loss:  0.1723,\tValidation Loss:  0.6975\n",
      "Epoch 390: \tTraining Loss:  0.1969,\tValidation Loss:  0.7516\n",
      "Epoch 391: \tTraining Loss:  0.1835,\tValidation Loss:  0.7008\n",
      "Epoch 392: \tTraining Loss:  0.1842,\tValidation Loss:  0.7015\n",
      "Epoch 393: \tTraining Loss:  0.1674,\tValidation Loss:  0.6875\n",
      "Epoch 394: \tTraining Loss:  0.1856,\tValidation Loss:  0.6808\n",
      "Epoch 395: \tTraining Loss:  0.1893,\tValidation Loss:  0.6717\n",
      "Epoch 396: \tTraining Loss:  0.1700,\tValidation Loss:  0.6986\n",
      "Epoch 397: \tTraining Loss:  0.1636,\tValidation Loss:  0.6758\n",
      "Epoch 398: \tTraining Loss:  0.1721,\tValidation Loss:  0.7685\n",
      "Epoch 399: \tTraining Loss:  0.1610,\tValidation Loss:  0.7559\n",
      "Epoch 400: \tTraining Loss:  0.2225,\tValidation Loss:  0.7139\n",
      "Epoch 401: \tTraining Loss:  0.1928,\tValidation Loss:  0.7259\n",
      "Epoch 402: \tTraining Loss:  0.1778,\tValidation Loss:  0.6606\n",
      "Epoch 403: \tTraining Loss:  0.1687,\tValidation Loss:  0.6686\n",
      "Epoch 404: \tTraining Loss:  0.1768,\tValidation Loss:  0.7049\n",
      "Epoch 405: \tTraining Loss:  0.1871,\tValidation Loss:  0.6195\n",
      "Epoch 406: \tTraining Loss:  0.1691,\tValidation Loss:  0.6859\n",
      "Epoch 407: \tTraining Loss:  0.1434,\tValidation Loss:  0.7068\n",
      "Epoch 408: \tTraining Loss:  0.1690,\tValidation Loss:  0.6877\n",
      "Epoch 409: \tTraining Loss:  0.1828,\tValidation Loss:  0.6976\n",
      "Epoch 410: \tTraining Loss:  0.1891,\tValidation Loss:  0.7593\n",
      "Epoch 411: \tTraining Loss:  0.1713,\tValidation Loss:  0.7075\n",
      "Epoch 412: \tTraining Loss:  0.1943,\tValidation Loss:  0.7068\n",
      "Epoch 413: \tTraining Loss:  0.1854,\tValidation Loss:  0.6645\n",
      "Epoch 414: \tTraining Loss:  0.1736,\tValidation Loss:  0.7284\n",
      "Epoch 415: \tTraining Loss:  0.1826,\tValidation Loss:  0.6734\n",
      "Epoch 416: \tTraining Loss:  0.1653,\tValidation Loss:  0.6572\n",
      "Epoch 417: \tTraining Loss:  0.1716,\tValidation Loss:  0.6911\n",
      "Epoch 418: \tTraining Loss:  0.1596,\tValidation Loss:  0.6524\n",
      "Epoch 419: \tTraining Loss:  0.1836,\tValidation Loss:  0.6964\n",
      "Epoch 420: \tTraining Loss:  0.1820,\tValidation Loss:  0.6390\n",
      "Epoch 421: \tTraining Loss:  0.1497,\tValidation Loss:  0.7078\n",
      "Epoch 422: \tTraining Loss:  0.1557,\tValidation Loss:  0.6951\n",
      "Epoch 423: \tTraining Loss:  0.1743,\tValidation Loss:  0.6578\n",
      "Epoch 424: \tTraining Loss:  0.1857,\tValidation Loss:  0.6990\n",
      "Epoch 425: \tTraining Loss:  0.1775,\tValidation Loss:  0.6389\n",
      "Epoch 426: \tTraining Loss:  0.1771,\tValidation Loss:  0.6419\n",
      "Epoch 427: \tTraining Loss:  0.1999,\tValidation Loss:  0.7185\n",
      "Epoch 428: \tTraining Loss:  0.1728,\tValidation Loss:  0.7010\n",
      "Epoch 429: \tTraining Loss:  0.1752,\tValidation Loss:  0.6786\n",
      "Epoch 430: \tTraining Loss:  0.1572,\tValidation Loss:  0.6898\n",
      "Epoch 431: \tTraining Loss:  0.1655,\tValidation Loss:  0.7046\n",
      "Epoch 432: \tTraining Loss:  0.1819,\tValidation Loss:  0.7270\n",
      "Epoch 433: \tTraining Loss:  0.1577,\tValidation Loss:  0.7177\n",
      "Epoch 434: \tTraining Loss:  0.1729,\tValidation Loss:  0.7007\n",
      "Epoch 435: \tTraining Loss:  0.1394,\tValidation Loss:  0.6792\n",
      "Epoch 436: \tTraining Loss:  0.1811,\tValidation Loss:  0.7481\n",
      "Epoch 437: \tTraining Loss:  0.1806,\tValidation Loss:  0.6705\n",
      "Epoch 438: \tTraining Loss:  0.1465,\tValidation Loss:  0.7048\n",
      "Epoch 439: \tTraining Loss:  0.1828,\tValidation Loss:  0.6897\n",
      "Epoch 440: \tTraining Loss:  0.1898,\tValidation Loss:  0.6447\n",
      "Epoch 441: \tTraining Loss:  0.1555,\tValidation Loss:  0.6968\n",
      "Epoch 442: \tTraining Loss:  0.1942,\tValidation Loss:  0.6790\n",
      "Epoch 443: \tTraining Loss:  0.1474,\tValidation Loss:  0.6868\n",
      "Epoch 444: \tTraining Loss:  0.1468,\tValidation Loss:  0.6975\n",
      "Epoch 445: \tTraining Loss:  0.1788,\tValidation Loss:  0.7189\n",
      "Epoch 446: \tTraining Loss:  0.1785,\tValidation Loss:  0.6834\n",
      "Epoch 447: \tTraining Loss:  0.1866,\tValidation Loss:  0.7017\n",
      "Epoch 448: \tTraining Loss:  0.1744,\tValidation Loss:  0.7045\n",
      "Epoch 449: \tTraining Loss:  0.1603,\tValidation Loss:  0.7077\n",
      "Epoch 450: \tTraining Loss:  0.1645,\tValidation Loss:  0.7339\n",
      "Epoch 451: \tTraining Loss:  0.1732,\tValidation Loss:  0.6639\n",
      "Epoch 452: \tTraining Loss:  0.1657,\tValidation Loss:  0.7273\n",
      "Epoch 453: \tTraining Loss:  0.1578,\tValidation Loss:  0.6659\n",
      "Epoch 454: \tTraining Loss:  0.1938,\tValidation Loss:  0.7165\n",
      "Epoch 455: \tTraining Loss:  0.1550,\tValidation Loss:  0.6568\n",
      "Epoch 456: \tTraining Loss:  0.1779,\tValidation Loss:  0.6739\n",
      "Epoch 457: \tTraining Loss:  0.1544,\tValidation Loss:  0.6696\n",
      "Epoch 458: \tTraining Loss:  0.1524,\tValidation Loss:  0.7041\n",
      "Epoch 459: \tTraining Loss:  0.1722,\tValidation Loss:  0.7250\n",
      "Epoch 460: \tTraining Loss:  0.1575,\tValidation Loss:  0.6800\n",
      "Epoch 461: \tTraining Loss:  0.1616,\tValidation Loss:  0.6828\n",
      "Epoch 462: \tTraining Loss:  0.1608,\tValidation Loss:  0.6886\n",
      "Epoch 463: \tTraining Loss:  0.1601,\tValidation Loss:  0.6682\n",
      "Epoch 464: \tTraining Loss:  0.1562,\tValidation Loss:  0.7041\n",
      "Epoch 465: \tTraining Loss:  0.1633,\tValidation Loss:  0.7194\n",
      "Epoch 466: \tTraining Loss:  0.1847,\tValidation Loss:  0.7483\n",
      "Epoch 467: \tTraining Loss:  0.1516,\tValidation Loss:  0.7437\n",
      "Epoch 468: \tTraining Loss:  0.1508,\tValidation Loss:  0.7527\n",
      "Epoch 469: \tTraining Loss:  0.1698,\tValidation Loss:  0.7437\n",
      "Epoch 470: \tTraining Loss:  0.1363,\tValidation Loss:  0.7006\n",
      "Epoch 471: \tTraining Loss:  0.1860,\tValidation Loss:  0.7072\n",
      "Epoch 472: \tTraining Loss:  0.1587,\tValidation Loss:  0.6989\n",
      "Epoch 473: \tTraining Loss:  0.1564,\tValidation Loss:  0.7191\n",
      "Epoch 474: \tTraining Loss:  0.1401,\tValidation Loss:  0.6907\n",
      "Epoch 475: \tTraining Loss:  0.1646,\tValidation Loss:  0.7050\n",
      "Epoch 476: \tTraining Loss:  0.1697,\tValidation Loss:  0.7379\n",
      "Epoch 477: \tTraining Loss:  0.1551,\tValidation Loss:  0.6770\n",
      "Epoch 478: \tTraining Loss:  0.1763,\tValidation Loss:  0.7193\n",
      "Epoch 479: \tTraining Loss:  0.1646,\tValidation Loss:  0.7176\n",
      "Epoch 480: \tTraining Loss:  0.1817,\tValidation Loss:  0.7031\n",
      "Epoch 481: \tTraining Loss:  0.1671,\tValidation Loss:  0.7113\n",
      "Epoch 482: \tTraining Loss:  0.1753,\tValidation Loss:  0.7139\n",
      "Epoch 483: \tTraining Loss:  0.1394,\tValidation Loss:  0.8020\n",
      "Epoch 484: \tTraining Loss:  0.1412,\tValidation Loss:  0.6691\n",
      "Epoch 485: \tTraining Loss:  0.1743,\tValidation Loss:  0.7043\n",
      "Epoch 486: \tTraining Loss:  0.1519,\tValidation Loss:  0.6729\n",
      "Epoch 487: \tTraining Loss:  0.1665,\tValidation Loss:  0.7003\n",
      "Epoch 488: \tTraining Loss:  0.1398,\tValidation Loss:  0.6847\n",
      "Epoch 489: \tTraining Loss:  0.1646,\tValidation Loss:  0.7144\n",
      "Epoch 490: \tTraining Loss:  0.1435,\tValidation Loss:  0.7008\n",
      "Epoch 491: \tTraining Loss:  0.1660,\tValidation Loss:  0.7472\n",
      "Epoch 492: \tTraining Loss:  0.1533,\tValidation Loss:  0.6954\n",
      "Epoch 493: \tTraining Loss:  0.1766,\tValidation Loss:  0.6641\n",
      "Epoch 494: \tTraining Loss:  0.1586,\tValidation Loss:  0.7242\n",
      "Epoch 495: \tTraining Loss:  0.1448,\tValidation Loss:  0.6813\n",
      "Epoch 496: \tTraining Loss:  0.1661,\tValidation Loss:  0.6970\n",
      "Epoch 497: \tTraining Loss:  0.1662,\tValidation Loss:  0.6768\n",
      "Epoch 498: \tTraining Loss:  0.1603,\tValidation Loss:  0.6875\n",
      "Epoch 499: \tTraining Loss:  0.1401,\tValidation Loss:  0.7317\n",
      "Epoch 500: \tTraining Loss:  0.1496,\tValidation Loss:  0.6764\n",
      "Elapsed Time:  241.840Sec\n",
      "0.5595413516624581\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 500\n",
    "time_start = time.time()\n",
    "print('CUDA is' + (' ' if is_cuda else ' not ') + 'running.')\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    cnt_epoch += 1\n",
    "    train_loss = 0.0\n",
    "    model.train()\n",
    "    for data, target in train_loader:\n",
    "        if is_cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()*data.size(0)\n",
    "        \n",
    "    val_loss = 0.0\n",
    "    model.eval()\n",
    "    for data, target in val_loader:\n",
    "        if is_cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        val_loss += loss.item() * data.size(0)\n",
    "    \n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    val_loss /= len(val_loader.dataset)\n",
    "    \n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        best_state = copy.deepcopy(model.state_dict())\n",
    "    \n",
    "    print('Epoch {:3d}: \\tTraining Loss: {:7.4f},\\tValidation Loss: {:7.4f}'.format(cnt_epoch, train_loss, val_loss))\n",
    "    \n",
    "print('Elapsed Time: {:8.3f}Sec'.format(time.time()-time_start))\n",
    "print(best_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "0fb0693f-dc59-448a-8608-28a6b9b18464",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 0, 1,  ..., 2, 0, 3], device='cuda:0')"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval() # 모델을 평가모드로 바꿉니다. dropout이 일어나지 않습니다.\n",
    "model.load_state_dict(best_state)\n",
    "\n",
    "if is_cuda:\n",
    "    X_te = X_te.cuda()\n",
    "\n",
    "with torch.no_grad(): # 이 안의 코드는 가중치 업데이트가 일어나지 않습니다.\n",
    "    outputs = model(X_te)\n",
    "    _, pred = torch.max(outputs, 1)\n",
    "\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "cc7eb11c-95cf-4841-9ca3-06908f399a51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    3395\n",
       "0    2143\n",
       "2    2052\n",
       "3    1753\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if is_cuda:\n",
    "    pred = pred.cpu()\n",
    "\n",
    "df_sub['target'] = pred.numpy()\n",
    "df_sub['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180cf444-9eda-4790-8cc0-b47b60aca04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss = 0.0\n",
    "class_correct = list(0 for i in range(10))\n",
    "class_total = list(0 for i in range(10))\n",
    "confusion_matrix = [[0 for _ in range(10)] for _ in range(10)]\n",
    "\n",
    "targets = []\n",
    "preds = []\n",
    "\n",
    "model.load_state_dict(best_state)\n",
    "\n",
    "for data, target in test_loader:\n",
    "    if is_cuda:\n",
    "        data, target = data.cuda(), target.cuda()\n",
    "    output = model(data)\n",
    "    loss = criterion(output, target)\n",
    "    test_loss += loss.item()*data.size(0)\n",
    "    _, pred = torch.max(output, 1)\n",
    "    correct = np.squeeze(pred.eq(target.data.view_as(pred)))\n",
    "    for i in range(len(target.data)):\n",
    "        label = target.data[i]\n",
    "        predict = pred[i]\n",
    "        class_correct[label] += correct[i].item()\n",
    "        class_total[label] += 1\n",
    "        confusion_matrix[label][predict] += 1\n",
    "\n",
    "test_loss /= len(test_loader.dataset)\n",
    "print('Test Loss: {:7.4f}'.format(test_loss))\n",
    "print('Test Accuracy: {:6.2f}% ({:d}/{:d})'.format(100. * np.sum(class_correct) / np.sum(class_total), np.sum(class_correct), np.sum(class_total)))\n",
    "\n",
    "print(' A\\P| ' + ' '.join(map(lambda x:'{:4d}'.format(x), range(10))))\n",
    "print('-'*4 + '+' + '-'*50)\n",
    "for num_, each_ in enumerate(confusion_matrix):\n",
    "    print('{:3d} | '.format(num_) + ' '.join(map(lambda x:'{:4d}'.format(x), each_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "6b9ec699-2ed0-48c4-9f35-6738ad46371a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def submission(df_sub, dir_dataset, name_project):\n",
    "    import datetime\n",
    "    now = datetime.datetime.now()\n",
    "    str_datetime = now.strftime(\"%y%m%d_%H%M%S\")\n",
    "    df_sub.to_csv(dir_dataset+'submission-'+name_project+'-'+str_datetime+'.csv', index = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "a0b8f2b7-0e01-4c39-b6fa-09b99e5b878d",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission(df_sub, dir_dataset, name_project)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
