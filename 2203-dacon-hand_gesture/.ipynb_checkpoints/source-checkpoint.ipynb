{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1a9fe14-9a8c-429d-8526-cc161f8b8279",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 주요 라이브러리 임포트\n",
    "\n",
    "# 파이썬 warning 무시\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# pandas import 및 출력 설정\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "# numpy\n",
    "import numpy as np\n",
    "\n",
    "# matplotlib 및 출력 설정\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"figure.figsize\"] = (35,20)\n",
    "\n",
    "# seaborn\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a859fe07-acf6-4eb5-a127-97ef47151b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원본 데이터셋 로드\n",
    "dir_dataset = \"C:/Users/0stix/Datasets/\"\n",
    "name_project = 'hand_gesture'\n",
    "df_test = pd.read_csv(dir_dataset+name_project+'/test.csv')\n",
    "df_train = pd.read_csv(dir_dataset+name_project+'/train.csv')\n",
    "df_sub = pd.read_csv(dir_dataset+name_project+'/sample_submission.csv')\n",
    "# df_all.info()\n",
    "# df_all.head()\n",
    "\n",
    "len_train = len(df_train)\n",
    "df_all = pd.concat([df_train, df_test], axis=0).drop('id', axis=1)\n",
    "\n",
    "target = 'target'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b6ffd315-90e2-4b3d-8d95-9b13758acc02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sensor_1</th>\n",
       "      <th>sensor_2</th>\n",
       "      <th>sensor_3</th>\n",
       "      <th>sensor_4</th>\n",
       "      <th>sensor_5</th>\n",
       "      <th>sensor_6</th>\n",
       "      <th>sensor_7</th>\n",
       "      <th>sensor_8</th>\n",
       "      <th>sensor_9</th>\n",
       "      <th>sensor_10</th>\n",
       "      <th>sensor_11</th>\n",
       "      <th>sensor_12</th>\n",
       "      <th>sensor_13</th>\n",
       "      <th>sensor_14</th>\n",
       "      <th>sensor_15</th>\n",
       "      <th>sensor_16</th>\n",
       "      <th>sensor_17</th>\n",
       "      <th>sensor_18</th>\n",
       "      <th>sensor_19</th>\n",
       "      <th>sensor_20</th>\n",
       "      <th>sensor_21</th>\n",
       "      <th>sensor_22</th>\n",
       "      <th>sensor_23</th>\n",
       "      <th>sensor_24</th>\n",
       "      <th>sensor_25</th>\n",
       "      <th>sensor_26</th>\n",
       "      <th>sensor_27</th>\n",
       "      <th>sensor_28</th>\n",
       "      <th>sensor_29</th>\n",
       "      <th>sensor_30</th>\n",
       "      <th>sensor_31</th>\n",
       "      <th>sensor_32</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-6.149463</td>\n",
       "      <td>-0.929714</td>\n",
       "      <td>9.058368</td>\n",
       "      <td>-7.017854</td>\n",
       "      <td>-2.958471</td>\n",
       "      <td>0.179233</td>\n",
       "      <td>-0.956591</td>\n",
       "      <td>-0.972401</td>\n",
       "      <td>5.956213</td>\n",
       "      <td>4.145636</td>\n",
       "      <td>25.017645</td>\n",
       "      <td>-4.061254</td>\n",
       "      <td>0.996632</td>\n",
       "      <td>-3.837345</td>\n",
       "      <td>-13.956994</td>\n",
       "      <td>-2.042957</td>\n",
       "      <td>2.130210</td>\n",
       "      <td>-1.957662</td>\n",
       "      <td>-1.149930</td>\n",
       "      <td>6.082028</td>\n",
       "      <td>0.878612</td>\n",
       "      <td>5.093102</td>\n",
       "      <td>-6.066648</td>\n",
       "      <td>-7.026436</td>\n",
       "      <td>-6.006282</td>\n",
       "      <td>-6.005836</td>\n",
       "      <td>7.043084</td>\n",
       "      <td>21.884650</td>\n",
       "      <td>-3.064152</td>\n",
       "      <td>-5.247552</td>\n",
       "      <td>-6.026107</td>\n",
       "      <td>-11.990822</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-2.238836</td>\n",
       "      <td>-1.003511</td>\n",
       "      <td>5.098079</td>\n",
       "      <td>-10.880357</td>\n",
       "      <td>-0.804562</td>\n",
       "      <td>-2.992123</td>\n",
       "      <td>26.972724</td>\n",
       "      <td>-8.900861</td>\n",
       "      <td>-5.968298</td>\n",
       "      <td>-4.060134</td>\n",
       "      <td>2.952843</td>\n",
       "      <td>-5.046353</td>\n",
       "      <td>1.083819</td>\n",
       "      <td>3.978378</td>\n",
       "      <td>-25.072542</td>\n",
       "      <td>-2.041602</td>\n",
       "      <td>2.912269</td>\n",
       "      <td>-3.998035</td>\n",
       "      <td>6.069698</td>\n",
       "      <td>4.966187</td>\n",
       "      <td>1.994051</td>\n",
       "      <td>-1.132059</td>\n",
       "      <td>14.906205</td>\n",
       "      <td>-1.996714</td>\n",
       "      <td>-7.933806</td>\n",
       "      <td>-3.136773</td>\n",
       "      <td>8.774211</td>\n",
       "      <td>10.944759</td>\n",
       "      <td>9.858186</td>\n",
       "      <td>-0.969241</td>\n",
       "      <td>-3.935553</td>\n",
       "      <td>-15.892421</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.087934</td>\n",
       "      <td>-2.092514</td>\n",
       "      <td>0.946750</td>\n",
       "      <td>-21.831788</td>\n",
       "      <td>9.119235</td>\n",
       "      <td>17.853587</td>\n",
       "      <td>-21.069954</td>\n",
       "      <td>-15.933212</td>\n",
       "      <td>-9.016039</td>\n",
       "      <td>-5.975194</td>\n",
       "      <td>-23.218408</td>\n",
       "      <td>-9.000630</td>\n",
       "      <td>9.115957</td>\n",
       "      <td>12.097318</td>\n",
       "      <td>-10.954367</td>\n",
       "      <td>-3.930714</td>\n",
       "      <td>-19.069594</td>\n",
       "      <td>-6.118940</td>\n",
       "      <td>-5.001346</td>\n",
       "      <td>-9.105371</td>\n",
       "      <td>-9.894885</td>\n",
       "      <td>10.107614</td>\n",
       "      <td>4.948570</td>\n",
       "      <td>-6.889685</td>\n",
       "      <td>54.052330</td>\n",
       "      <td>-6.109238</td>\n",
       "      <td>12.154595</td>\n",
       "      <td>6.095989</td>\n",
       "      <td>-40.195088</td>\n",
       "      <td>-3.958124</td>\n",
       "      <td>-8.079537</td>\n",
       "      <td>-5.160090</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-2.211629</td>\n",
       "      <td>-1.930904</td>\n",
       "      <td>21.888406</td>\n",
       "      <td>-3.067560</td>\n",
       "      <td>-0.240634</td>\n",
       "      <td>2.985056</td>\n",
       "      <td>-29.073369</td>\n",
       "      <td>0.200774</td>\n",
       "      <td>-1.043742</td>\n",
       "      <td>2.099845</td>\n",
       "      <td>-15.123774</td>\n",
       "      <td>-0.069867</td>\n",
       "      <td>-0.114247</td>\n",
       "      <td>-1.896109</td>\n",
       "      <td>5.127194</td>\n",
       "      <td>-2.877423</td>\n",
       "      <td>2.970044</td>\n",
       "      <td>-1.099702</td>\n",
       "      <td>3.116767</td>\n",
       "      <td>8.124209</td>\n",
       "      <td>-0.917418</td>\n",
       "      <td>-1.027199</td>\n",
       "      <td>14.048298</td>\n",
       "      <td>-2.126170</td>\n",
       "      <td>-1.035526</td>\n",
       "      <td>2.178769</td>\n",
       "      <td>10.032723</td>\n",
       "      <td>-1.010897</td>\n",
       "      <td>-3.912848</td>\n",
       "      <td>-2.980338</td>\n",
       "      <td>-12.983597</td>\n",
       "      <td>-3.001077</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.953852</td>\n",
       "      <td>2.964892</td>\n",
       "      <td>-36.044802</td>\n",
       "      <td>0.899838</td>\n",
       "      <td>26.930210</td>\n",
       "      <td>11.004409</td>\n",
       "      <td>-21.962423</td>\n",
       "      <td>-11.950189</td>\n",
       "      <td>-20.933785</td>\n",
       "      <td>-4.000506</td>\n",
       "      <td>16.010442</td>\n",
       "      <td>5.961219</td>\n",
       "      <td>9.907115</td>\n",
       "      <td>-0.067754</td>\n",
       "      <td>-9.970728</td>\n",
       "      <td>0.868499</td>\n",
       "      <td>1.892233</td>\n",
       "      <td>-3.161698</td>\n",
       "      <td>-9.225990</td>\n",
       "      <td>3.953956</td>\n",
       "      <td>-17.959652</td>\n",
       "      <td>-3.115491</td>\n",
       "      <td>-6.051674</td>\n",
       "      <td>-2.051761</td>\n",
       "      <td>10.917567</td>\n",
       "      <td>1.905335</td>\n",
       "      <td>-13.004707</td>\n",
       "      <td>17.169552</td>\n",
       "      <td>2.105194</td>\n",
       "      <td>3.967986</td>\n",
       "      <td>11.861657</td>\n",
       "      <td>-27.088846</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    sensor_1  sensor_2   sensor_3   sensor_4   sensor_5   sensor_6   sensor_7   sensor_8   sensor_9  sensor_10  sensor_11  sensor_12  sensor_13  sensor_14  sensor_15  sensor_16  sensor_17  sensor_18  sensor_19  sensor_20  sensor_21  sensor_22  sensor_23  sensor_24  sensor_25  sensor_26  sensor_27  sensor_28  sensor_29  sensor_30  sensor_31  sensor_32  target\n",
       "0  -6.149463 -0.929714   9.058368  -7.017854  -2.958471   0.179233  -0.956591  -0.972401   5.956213   4.145636  25.017645  -4.061254   0.996632  -3.837345 -13.956994  -2.042957   2.130210  -1.957662  -1.149930   6.082028   0.878612   5.093102  -6.066648  -7.026436  -6.006282  -6.005836   7.043084  21.884650  -3.064152  -5.247552  -6.026107 -11.990822     1.0\n",
       "1  -2.238836 -1.003511   5.098079 -10.880357  -0.804562  -2.992123  26.972724  -8.900861  -5.968298  -4.060134   2.952843  -5.046353   1.083819   3.978378 -25.072542  -2.041602   2.912269  -3.998035   6.069698   4.966187   1.994051  -1.132059  14.906205  -1.996714  -7.933806  -3.136773   8.774211  10.944759   9.858186  -0.969241  -3.935553 -15.892421     1.0\n",
       "2  19.087934 -2.092514   0.946750 -21.831788   9.119235  17.853587 -21.069954 -15.933212  -9.016039  -5.975194 -23.218408  -9.000630   9.115957  12.097318 -10.954367  -3.930714 -19.069594  -6.118940  -5.001346  -9.105371  -9.894885  10.107614   4.948570  -6.889685  54.052330  -6.109238  12.154595   6.095989 -40.195088  -3.958124  -8.079537  -5.160090     0.0\n",
       "3  -2.211629 -1.930904  21.888406  -3.067560  -0.240634   2.985056 -29.073369   0.200774  -1.043742   2.099845 -15.123774  -0.069867  -0.114247  -1.896109   5.127194  -2.877423   2.970044  -1.099702   3.116767   8.124209  -0.917418  -1.027199  14.048298  -2.126170  -1.035526   2.178769  10.032723  -1.010897  -3.912848  -2.980338 -12.983597  -3.001077     1.0\n",
       "4   3.953852  2.964892 -36.044802   0.899838  26.930210  11.004409 -21.962423 -11.950189 -20.933785  -4.000506  16.010442   5.961219   9.907115  -0.067754  -9.970728   0.868499   1.892233  -3.161698  -9.225990   3.953956 -17.959652  -3.115491  -6.051674  -2.051761  10.917567   1.905335 -13.004707  17.169552   2.105194   3.967986  11.861657 -27.088846     2.0"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all.head()\n",
    "# df_all.info()\n",
    "# df_all.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "5208358e-8fec-4b75-a2e0-a7b54abab507",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x19c80043640>"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "mat = df_all.iloc[:,:32].corr().values\n",
    "for idx_ in range(len(mat)):\n",
    "    mat[idx_,idx_] = 0.0\n",
    "max_ = np.max(mat)\n",
    "img = Image.fromarray(mat/max_*255)\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c785291c-67f3-439f-bb2d-c9698b083d71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0    599\n",
       "2.0    593\n",
       "1.0    574\n",
       "0.0    569\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all[target].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "64a578c0-7aed-49fa-b0ca-f835292f08b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations as cb\n",
    "df_v0 = df_all.copy()\n",
    "\n",
    "for each_ in df_v0.drop(target, axis=1):\n",
    "    min_ = np.min(df_v0[each_])\n",
    "    max_ = np.max(df_v0[each_])\n",
    "    \n",
    "    df_v0[each_] = (df_v0[each_] - min_) / (max_ - min_)\n",
    "    \n",
    "#     df_v0[each_] = df_v0[each_] - np.mean(df_v0[each_])\n",
    "#     df_v0[each_] = df_v0[each_]/np.var(df_v0[each_]) + 1\n",
    "\n",
    "# for a_, b_ in cb(range(1, 33), 2):\n",
    "#     str_ = 'd '+ str(a_).zfill(2) + '_'+ str(b_).zfill(2)\n",
    "#     df_v0[str_] = df_v0['sensor_'+str(a_)] - df_v0['sensor_'+str(b_)]\n",
    "# cnt_ = 0\n",
    "\n",
    "# for a_, b_ in cb(df_v0.drop(target, axis=1).columns, 2):\n",
    "#     str_ = str(cnt_)\n",
    "#     df_v0[str_] = df_v0[b_] - df_v0[a_]\n",
    "#     cnt_ += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "b0ccfb88-d83c-420e-87a1-6eda7bf80879",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2335, 32), (9343, 32), (2335,))"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_ = df_v0.drop(target, axis=1).values\n",
    "X_train, X_test = X_[:len_train], X_[len_train:]\n",
    "y_train = df_v0[target][:len_train].values\n",
    "\n",
    "X_train.shape, X_test.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "0109cee6-2ddb-4ab9-93a9-09f3c570cd40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:15:55] WARNING: ..\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[22:15:55] WARNING: ..\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[22:15:56] WARNING: ..\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[22:15:56] WARNING: ..\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[22:15:57] WARNING: ..\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.812847965738758,\n",
       " array([0.82441113, 0.81156317, 0.81156317, 0.81798715, 0.7987152 ]))"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from xgboost import XGBClassifier\n",
    "xgb_clf = XGBClassifier()\n",
    "scores = cross_val_score(xgb_clf, X_train, y_train, scoring='accuracy', cv=5)\n",
    "np.mean(scores), scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "10323d0d-578b-4b12-bc2d-a8eceed6e400",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x19cd5e78bd0>"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "88b56607-2b24-440e-b49d-19f0b3c43afe",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH   0, loss:0.1394, accuracy:0.9569 \n",
      "EPOCH   1, loss:0.1266, accuracy:0.9556 \n",
      "EPOCH   2, loss:0.0700, accuracy:0.9802 \n",
      "EPOCH   3, loss:0.0640, accuracy:0.9866 \n",
      "EPOCH   4, loss:0.1519, accuracy:0.9565 \n",
      "EPOCH   5, loss:0.0190, accuracy:1.0017 \n",
      "EPOCH   6, loss:0.0359, accuracy:0.9931 \n",
      "EPOCH   7, loss:0.3321, accuracy:0.8983 \n",
      "EPOCH   8, loss:0.1572, accuracy:0.9457 \n",
      "EPOCH   9, loss:0.0432, accuracy:0.9922 \n",
      "EPOCH  10, loss:0.0438, accuracy:0.9927 \n",
      "EPOCH  11, loss:0.1038, accuracy:0.9728 \n",
      "EPOCH  12, loss:0.4313, accuracy:0.8711 \n",
      "EPOCH  13, loss:0.1140, accuracy:0.9659 \n",
      "EPOCH  14, loss:0.0249, accuracy:1.0004 \n",
      "EPOCH  15, loss:0.0089, accuracy:1.0060 \n",
      "EPOCH  16, loss:0.0083, accuracy:1.0056 \n",
      "EPOCH  17, loss:0.0116, accuracy:1.0043 \n",
      "EPOCH  18, loss:0.0578, accuracy:0.9853 \n",
      "EPOCH  19, loss:0.1459, accuracy:0.9509 \n",
      "EPOCH  20, loss:0.2558, accuracy:0.9194 \n",
      "EPOCH  21, loss:0.1504, accuracy:0.9539 \n",
      "EPOCH  22, loss:0.0228, accuracy:1.0013 \n",
      "EPOCH  23, loss:0.0334, accuracy:0.9974 \n",
      "EPOCH  24, loss:0.1573, accuracy:0.9565 \n",
      "EPOCH  25, loss:0.1001, accuracy:0.9711 \n",
      "EPOCH  26, loss:0.0424, accuracy:0.9901 \n",
      "EPOCH  27, loss:0.0897, accuracy:0.9746 \n",
      "EPOCH  28, loss:0.2833, accuracy:0.9164 \n",
      "EPOCH  29, loss:0.1468, accuracy:0.9560 \n",
      "EPOCH  30, loss:0.0538, accuracy:0.9879 \n",
      "EPOCH  31, loss:0.0220, accuracy:0.9996 \n",
      "EPOCH  32, loss:0.0268, accuracy:0.9991 \n",
      "EPOCH  33, loss:0.0653, accuracy:0.9819 \n",
      "EPOCH  34, loss:0.3231, accuracy:0.9017 \n",
      "EPOCH  35, loss:0.1324, accuracy:0.9591 \n",
      "EPOCH  36, loss:0.0429, accuracy:0.9918 \n",
      "EPOCH  37, loss:0.0114, accuracy:1.0043 \n",
      "EPOCH  38, loss:0.0160, accuracy:1.0043 \n",
      "EPOCH  39, loss:0.0050, accuracy:1.0065 \n",
      "EPOCH  40, loss:0.0053, accuracy:1.0065 \n",
      "EPOCH  41, loss:0.0037, accuracy:1.0065 \n",
      "EPOCH  42, loss:0.0034, accuracy:1.0065 \n",
      "EPOCH  43, loss:0.0037, accuracy:1.0060 \n",
      "EPOCH  44, loss:0.5328, accuracy:0.8703 \n",
      "EPOCH  45, loss:0.4299, accuracy:0.8612 \n",
      "EPOCH  46, loss:0.2311, accuracy:0.9216 \n",
      "EPOCH  47, loss:0.0652, accuracy:0.9849 \n",
      "EPOCH  48, loss:0.0373, accuracy:0.9974 \n",
      "EPOCH  49, loss:0.0708, accuracy:0.9819 \n",
      "EPOCH  50, loss:0.1318, accuracy:0.9522 \n",
      "EPOCH  51, loss:0.0641, accuracy:0.9866 \n",
      "EPOCH  52, loss:0.1391, accuracy:0.9522 \n",
      "EPOCH  53, loss:0.1024, accuracy:0.9694 \n",
      "EPOCH  54, loss:0.1153, accuracy:0.9681 \n",
      "EPOCH  55, loss:0.3131, accuracy:0.9026 \n",
      "EPOCH  56, loss:0.1384, accuracy:0.9547 \n",
      "EPOCH  57, loss:0.0350, accuracy:0.9987 \n",
      "EPOCH  58, loss:0.0098, accuracy:1.0056 \n",
      "EPOCH  59, loss:0.0060, accuracy:1.0060 \n",
      "EPOCH  60, loss:0.0039, accuracy:1.0065 \n",
      "EPOCH  61, loss:0.0260, accuracy:0.9953 \n",
      "EPOCH  62, loss:0.3316, accuracy:0.9052 \n",
      "EPOCH  63, loss:0.1486, accuracy:0.9500 \n",
      "EPOCH  64, loss:0.0780, accuracy:0.9780 \n",
      "EPOCH  65, loss:0.0135, accuracy:1.0022 \n",
      "EPOCH  66, loss:0.0056, accuracy:1.0065 \n",
      "EPOCH  67, loss:0.0035, accuracy:1.0065 \n",
      "EPOCH  68, loss:0.0049, accuracy:1.0060 \n",
      "EPOCH  69, loss:0.5346, accuracy:0.8582 \n",
      "EPOCH  70, loss:0.2347, accuracy:0.9241 \n",
      "EPOCH  71, loss:0.0678, accuracy:0.9853 \n",
      "EPOCH  72, loss:0.0623, accuracy:0.9828 \n",
      "EPOCH  73, loss:0.0631, accuracy:0.9832 \n",
      "EPOCH  74, loss:0.2200, accuracy:0.9237 \n",
      "EPOCH  75, loss:0.3054, accuracy:0.8953 \n",
      "EPOCH  76, loss:0.0707, accuracy:0.9841 \n",
      "EPOCH  77, loss:0.0186, accuracy:1.0034 \n",
      "EPOCH  78, loss:0.0675, accuracy:0.9836 \n",
      "EPOCH  79, loss:0.1671, accuracy:0.9466 \n",
      "EPOCH  80, loss:0.0932, accuracy:0.9746 \n",
      "EPOCH  81, loss:0.0583, accuracy:0.9866 \n",
      "EPOCH  82, loss:0.0523, accuracy:0.9884 \n",
      "EPOCH  83, loss:0.0253, accuracy:0.9991 \n",
      "EPOCH  84, loss:0.0103, accuracy:1.0052 \n",
      "EPOCH  85, loss:0.1135, accuracy:0.9776 \n",
      "EPOCH  86, loss:0.4353, accuracy:0.8711 \n",
      "EPOCH  87, loss:0.0319, accuracy:0.9991 \n",
      "EPOCH  88, loss:0.0269, accuracy:0.9996 \n",
      "EPOCH  89, loss:0.0083, accuracy:1.0060 \n",
      "EPOCH  90, loss:0.0052, accuracy:1.0065 \n",
      "EPOCH  91, loss:0.0040, accuracy:1.0065 \n",
      "EPOCH  92, loss:0.0042, accuracy:1.0065 \n",
      "EPOCH  93, loss:0.0038, accuracy:1.0065 \n",
      "EPOCH  94, loss:0.0024, accuracy:1.0065 \n",
      "EPOCH  95, loss:0.0030, accuracy:1.0065 \n",
      "EPOCH  96, loss:0.0027, accuracy:1.0065 \n",
      "EPOCH  97, loss:0.0019, accuracy:1.0065 \n",
      "EPOCH  98, loss:0.3342, accuracy:0.9194 \n",
      "EPOCH  99, loss:1.1957, accuracy:0.5668 \n",
      "EPOCH 100, loss:0.5902, accuracy:0.7789 \n",
      "EPOCH 101, loss:0.3317, accuracy:0.8832 \n",
      "EPOCH 102, loss:0.2425, accuracy:0.9228 \n",
      "EPOCH 103, loss:0.1936, accuracy:0.9414 \n",
      "EPOCH 104, loss:0.1569, accuracy:0.9466 \n",
      "EPOCH 105, loss:0.1802, accuracy:0.9392 \n",
      "EPOCH 106, loss:0.1362, accuracy:0.9573 \n",
      "EPOCH 107, loss:0.1525, accuracy:0.9530 \n",
      "EPOCH 108, loss:0.1304, accuracy:0.9569 \n",
      "EPOCH 109, loss:0.0446, accuracy:0.9978 \n",
      "EPOCH 110, loss:0.0864, accuracy:0.9763 \n",
      "EPOCH 111, loss:0.2484, accuracy:0.9194 \n",
      "EPOCH 112, loss:0.1289, accuracy:0.9608 \n",
      "EPOCH 113, loss:0.0733, accuracy:0.9789 \n",
      "EPOCH 114, loss:0.0425, accuracy:0.9953 \n",
      "EPOCH 115, loss:0.0461, accuracy:0.9953 \n",
      "EPOCH 116, loss:0.3204, accuracy:0.9022 \n",
      "EPOCH 117, loss:0.1499, accuracy:0.9582 \n",
      "EPOCH 118, loss:0.0589, accuracy:0.9836 \n",
      "EPOCH 119, loss:0.0698, accuracy:0.9810 \n",
      "EPOCH 120, loss:0.0759, accuracy:0.9789 \n",
      "EPOCH 121, loss:0.2007, accuracy:0.9371 \n",
      "EPOCH 122, loss:0.1770, accuracy:0.9414 \n",
      "EPOCH 123, loss:0.0859, accuracy:0.9754 \n",
      "EPOCH 124, loss:0.0499, accuracy:0.9905 \n",
      "EPOCH 125, loss:0.3020, accuracy:0.9056 \n",
      "EPOCH 126, loss:0.0959, accuracy:0.9759 \n",
      "EPOCH 127, loss:0.0222, accuracy:1.0013 \n",
      "EPOCH 128, loss:0.0245, accuracy:0.9991 \n",
      "EPOCH 129, loss:0.0326, accuracy:0.9957 \n",
      "EPOCH 130, loss:0.0453, accuracy:0.9914 \n",
      "EPOCH 131, loss:0.1339, accuracy:0.9582 \n",
      "EPOCH 132, loss:0.2603, accuracy:0.9147 \n",
      "EPOCH 133, loss:0.1121, accuracy:0.9625 \n",
      "EPOCH 134, loss:0.1110, accuracy:0.9655 \n",
      "EPOCH 135, loss:0.0817, accuracy:0.9784 \n",
      "EPOCH 136, loss:0.0232, accuracy:0.9991 \n",
      "EPOCH 137, loss:0.0135, accuracy:1.0052 \n",
      "EPOCH 138, loss:0.0132, accuracy:1.0030 \n",
      "EPOCH 139, loss:0.0098, accuracy:1.0056 \n",
      "EPOCH 140, loss:0.0096, accuracy:1.0056 \n",
      "EPOCH 141, loss:0.2362, accuracy:0.9366 \n",
      "EPOCH 142, loss:0.5891, accuracy:0.8250 \n",
      "EPOCH 143, loss:0.1396, accuracy:0.9556 \n",
      "EPOCH 144, loss:0.0734, accuracy:0.9815 \n",
      "EPOCH 145, loss:0.0516, accuracy:0.9909 \n",
      "EPOCH 146, loss:0.0550, accuracy:0.9884 \n",
      "EPOCH 147, loss:0.1587, accuracy:0.9444 \n",
      "EPOCH 148, loss:0.1370, accuracy:0.9586 \n",
      "EPOCH 149, loss:0.0861, accuracy:0.9759 \n",
      "EPOCH 150, loss:0.0269, accuracy:0.9987 \n",
      "EPOCH 151, loss:0.0173, accuracy:1.0009 \n",
      "EPOCH 152, loss:0.0079, accuracy:1.0060 \n",
      "EPOCH 153, loss:0.0043, accuracy:1.0060 \n",
      "EPOCH 154, loss:0.0036, accuracy:1.0065 \n",
      "EPOCH 155, loss:0.0037, accuracy:1.0060 \n",
      "EPOCH 156, loss:0.0465, accuracy:0.9897 \n",
      "EPOCH 157, loss:0.6795, accuracy:0.7948 \n",
      "EPOCH 158, loss:0.2556, accuracy:0.9203 \n",
      "EPOCH 159, loss:0.0707, accuracy:0.9849 \n",
      "EPOCH 160, loss:0.0321, accuracy:0.9961 \n",
      "EPOCH 161, loss:0.0198, accuracy:1.0013 \n",
      "EPOCH 162, loss:0.0885, accuracy:0.9746 \n",
      "EPOCH 163, loss:0.2140, accuracy:0.9315 \n",
      "EPOCH 164, loss:0.0738, accuracy:0.9841 \n",
      "EPOCH 165, loss:0.0223, accuracy:1.0022 \n",
      "EPOCH 166, loss:0.0534, accuracy:0.9871 \n",
      "EPOCH 167, loss:0.1303, accuracy:0.9599 \n",
      "EPOCH 168, loss:0.0886, accuracy:0.9741 \n",
      "EPOCH 169, loss:0.1046, accuracy:0.9694 \n",
      "EPOCH 170, loss:0.0488, accuracy:0.9901 \n",
      "EPOCH 171, loss:0.0086, accuracy:1.0056 \n",
      "EPOCH 172, loss:0.0073, accuracy:1.0052 \n",
      "EPOCH 173, loss:0.0883, accuracy:0.9819 \n",
      "EPOCH 174, loss:0.2816, accuracy:0.9112 \n",
      "EPOCH 175, loss:0.1589, accuracy:0.9534 \n",
      "EPOCH 176, loss:0.1682, accuracy:0.9466 \n",
      "EPOCH 177, loss:0.0191, accuracy:1.0034 \n",
      "EPOCH 178, loss:0.0163, accuracy:1.0030 \n",
      "EPOCH 179, loss:0.0062, accuracy:1.0060 \n",
      "EPOCH 180, loss:0.0033, accuracy:1.0060 \n",
      "EPOCH 181, loss:0.0042, accuracy:1.0065 \n",
      "EPOCH 182, loss:0.0039, accuracy:1.0065 \n",
      "EPOCH 183, loss:0.0136, accuracy:1.0013 \n",
      "EPOCH 184, loss:0.5182, accuracy:0.8461 \n",
      "EPOCH 185, loss:0.2236, accuracy:0.9289 \n",
      "EPOCH 186, loss:0.0967, accuracy:0.9733 \n",
      "EPOCH 187, loss:0.1888, accuracy:0.9418 \n",
      "EPOCH 188, loss:0.0185, accuracy:1.0026 \n",
      "EPOCH 189, loss:0.0066, accuracy:1.0060 \n",
      "EPOCH 190, loss:0.0056, accuracy:1.0060 \n",
      "EPOCH 191, loss:0.0049, accuracy:1.0060 \n",
      "EPOCH 192, loss:0.0039, accuracy:1.0065 \n",
      "EPOCH 193, loss:0.0040, accuracy:1.0065 \n",
      "EPOCH 194, loss:0.0025, accuracy:1.0065 \n",
      "EPOCH 195, loss:0.0026, accuracy:1.0065 \n",
      "EPOCH 196, loss:0.0020, accuracy:1.0065 \n",
      "EPOCH 197, loss:0.0022, accuracy:1.0065 \n",
      "EPOCH 198, loss:0.0015, accuracy:1.0065 \n",
      "EPOCH 199, loss:0.0014, accuracy:1.0065 \n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
    "\n",
    "for epoch in range(200):\n",
    "    running_loss = 0.0\n",
    "    accuracy = 0\n",
    "    for i, data in enumerate(train_dataloader, 0):\n",
    "        inputs, labels = data\n",
    "\n",
    "        optimizer.zero_grad() # 매개변수를 0으로 만듭니다. 매 학습시 초기화해줘야합니다.\n",
    "        outputs = model(inputs) # 입력값을 넣어 순전파를 진행시킵니다.\n",
    "\n",
    "        loss = criterion(outputs, labels) # 모델 출력값와 실제값을 손실함수에 대입합니다.\n",
    "        loss.backward() # 손실함수에서 역전파 수행합니다.\n",
    "        optimizer.step() # 옵티마이저를 사용해 매개변수를 최적화합니다.\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        _, predictions = torch.max(outputs, 1)\n",
    "\n",
    "        for label, prediction in zip(labels, predictions):\n",
    "            if label == prediction:\n",
    "                accuracy = accuracy + 1\n",
    "\n",
    "        \n",
    "                \n",
    "    print('EPOCH {:3}, loss:{:5.4f}, accuracy:{:5.4f} '.format(epoch, running_loss / i, accuracy / (i * 16)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "07b9fdcf-f3b8-4443-9821-523b13f64669",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 1,  ..., 2, 0, 3])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval() # 모델을 평가모드로 바꿉니다. dropout이 일어나지 않습니다.\n",
    "\n",
    "with torch.no_grad(): # 이 안의 코드는 가중치 업데이트가 일어나지 않습니다.\n",
    "    outputs = model(test_x)\n",
    "    _, pred = torch.max(outputs, 1)\n",
    "\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "cc70540c-c15d-450c-956c-62ea50391c45",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    3243\n",
       "2    2237\n",
       "3    1984\n",
       "0    1879\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sub['target'] = pred.numpy()\n",
    "df_sub['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "0b90d957-598e-4ced-9193-359fe3a78eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "36c02682-0ec9-4478-8457-e5b035f404a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your Machine is: NVIDIA GeForce RTX 3070\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "print('Your Machine is: ' + torch.cuda.get_device_name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "76f05e86-0219-455a-ae90-1e4473ab6902",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_workers = 0\n",
    "batch_size = 16\n",
    "is_cuda = True if torch.cuda.is_available() else False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "d8ef0750-634b-4435-b3c9-a1ebda1e96ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = torch.from_numpy(X_train).float()\n",
    "train_y = torch.tensor(y_train, dtype = torch.int64)\n",
    "test_x = torch.from_numpy(X_test).float()\n",
    "\n",
    "train_dataset = TensorDataset(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "2b67110c-0ccc-4949-b9bd-68143c266571",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 32])\n",
      "tensor([1, 3, 1, 1, 1, 2, 1, 0, 3, 0, 3, 0, 3, 2, 2, 0])\n"
     ]
    }
   ],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size = 16, shuffle = True)\n",
    "\n",
    "for batch_idx, samples in enumerate(train_dataloader):\n",
    "    if batch_idx > 0:\n",
    "        break\n",
    "    print(samples[0].shape)\n",
    "    print(samples[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "51e629d0-e3b4-48ce-815a-725d9995b35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "a1d94b03-c713-4bf2-b323-e4a9cb3109ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1868, 32]), torch.Size([1868]))"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transform = transforms.ToTensor()\n",
    "\n",
    "X_ = df_v0.drop(target, axis=1).values\n",
    "X_train, X_te = X_[:len_train], X_[len_train:]\n",
    "y_train = df_v0[target][:len_train].values\n",
    "\n",
    "X_tr, X_va, y_tr, y_va = train_test_split(X_train, y_train, test_size=0.2, shuffle=True)\n",
    "\n",
    "X_tr = torch.from_numpy(X_tr).float()\n",
    "X_va = torch.from_numpy(X_va).float()\n",
    "X_te = torch.from_numpy(X_te).float()\n",
    "\n",
    "y_tr = torch.tensor(y_tr, dtype=torch.int64)\n",
    "y_va = torch.tensor(y_va, dtype=torch.int64)\n",
    "\n",
    "X_tr.size(), y_tr.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "a4068697-c614-44dd-89d2-9d97fdccd12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(X_tr, y_tr)\n",
    "val_dataset = TensorDataset(X_va, y_va)\n",
    "test_dataset = TensorDataset(X_te)\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "# train_data = datasets.MNIST(root='data', train=True, download=True, transform=transform)\n",
    "# test_data = datasets.MNIST(root='data', train=False, download=True, transform=transform)\n",
    "\n",
    "# train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, num_workers=num_workers, shuffle=True)\n",
    "# test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, num_workers=num_workers, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "bf74e615-1e29-4ee3-aea9-bcf75e5fb1ff",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models(\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=32, out_features=128, bias=True)\n",
      "    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.2, inplace=False)\n",
      "    (4): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (5): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): ReLU()\n",
      "    (7): Dropout(p=0.2, inplace=False)\n",
      "    (8): Linear(in_features=128, out_features=4, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Models(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(32, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, 4),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear_relu_stack(x)\n",
    "        return x\n",
    "\n",
    "model = Models()\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "04122dee-3583-45be-bc19-754519d456a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=32, out_features=64, bias=True)\n",
      "    (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.2, inplace=False)\n",
      "    (4): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (5): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): ReLU()\n",
      "    (7): Dropout(p=0.2, inplace=False)\n",
      "    (8): Linear(in_features=128, out_features=256, bias=True)\n",
      "    (9): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): ReLU()\n",
      "    (11): Dropout(p=0.2, inplace=False)\n",
      "    (12): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (13): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (14): ReLU()\n",
      "    (15): Dropout(p=0.2, inplace=False)\n",
      "    (16): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (17): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (18): ReLU()\n",
      "    (19): Dropout(p=0.2, inplace=False)\n",
      "    (20): Linear(in_features=256, out_features=4, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch.nn.init as I\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(32, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            \n",
    "            nn.Linear(64, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            \n",
    "            nn.Linear(128, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            \n",
    "            nn.Linear(256, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            \n",
    "            nn.Linear(512, 4),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear_relu_stack(x)\n",
    "        return x\n",
    "        \n",
    "        #         super(Net, self).__init__()\n",
    "#         self.conv1 = nn.Conv2d(1, 16, kernel_size=5)\n",
    "#         self.fc1 = nn.Linear(12 * 12 * 16, 100)\n",
    "#         self.fc2 = nn.Linear(100, 10)\n",
    "        \n",
    "# #         I.kaiming_normal_(self.conv1.weight)\n",
    "# #         I.kaiming_normal_(self.fc1.weight)\n",
    "# #         I.kaiming_normal_(self.fc2.weight)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = F.max_pool2d(F.relu(self.conv1(x)), 2)\n",
    "#         x = F.relu(self.fc1(x.view(-1, 12 * 12 * 16)))\n",
    "#         x = F.relu(self.fc2(x))\n",
    "#         return x\n",
    "\n",
    "model = Net()\n",
    "if is_cuda:\n",
    "    model.cuda()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "4332951e-fdc6-4101-894c-679851c47b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "if is_cuda:\n",
    "    criterion.cuda()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "b6ede7d7-5fdf-4f00-97ee-b8973136e253",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_loss = 10\n",
    "best_state = None\n",
    "cnt_epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "cf43a693-8de5-4bf3-a8ea-88a806089e21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is running.\n",
      "Epoch   1: \tTraining Loss:  1.4189,\tValidation Loss:  1.3419\n",
      "Epoch   2: \tTraining Loss:  1.3536,\tValidation Loss:  1.3243\n",
      "Epoch   3: \tTraining Loss:  1.3285,\tValidation Loss:  1.3433\n",
      "Epoch   4: \tTraining Loss:  1.3033,\tValidation Loss:  1.3231\n",
      "Epoch   5: \tTraining Loss:  1.2870,\tValidation Loss:  1.3262\n",
      "Epoch   6: \tTraining Loss:  1.2598,\tValidation Loss:  1.3579\n",
      "Epoch   7: \tTraining Loss:  1.2545,\tValidation Loss:  1.2753\n",
      "Epoch   8: \tTraining Loss:  1.2390,\tValidation Loss:  1.3377\n",
      "Epoch   9: \tTraining Loss:  1.2302,\tValidation Loss:  1.3096\n",
      "Epoch  10: \tTraining Loss:  1.2225,\tValidation Loss:  1.2817\n",
      "Epoch  11: \tTraining Loss:  1.2077,\tValidation Loss:  1.3031\n",
      "Epoch  12: \tTraining Loss:  1.2025,\tValidation Loss:  1.2704\n",
      "Epoch  13: \tTraining Loss:  1.1850,\tValidation Loss:  1.2782\n",
      "Epoch  14: \tTraining Loss:  1.1604,\tValidation Loss:  1.2585\n",
      "Epoch  15: \tTraining Loss:  1.1554,\tValidation Loss:  1.2246\n",
      "Epoch  16: \tTraining Loss:  1.1382,\tValidation Loss:  1.2494\n",
      "Epoch  17: \tTraining Loss:  1.1297,\tValidation Loss:  1.2462\n",
      "Epoch  18: \tTraining Loss:  1.1199,\tValidation Loss:  1.2625\n",
      "Epoch  19: \tTraining Loss:  1.1054,\tValidation Loss:  1.1920\n",
      "Epoch  20: \tTraining Loss:  1.1066,\tValidation Loss:  1.2238\n",
      "Epoch  21: \tTraining Loss:  1.0824,\tValidation Loss:  1.2853\n",
      "Epoch  22: \tTraining Loss:  1.1090,\tValidation Loss:  1.2451\n",
      "Epoch  23: \tTraining Loss:  1.0746,\tValidation Loss:  1.2221\n",
      "Epoch  24: \tTraining Loss:  1.0623,\tValidation Loss:  1.1544\n",
      "Epoch  25: \tTraining Loss:  1.0619,\tValidation Loss:  1.1832\n",
      "Epoch  26: \tTraining Loss:  1.0629,\tValidation Loss:  1.2197\n",
      "Epoch  27: \tTraining Loss:  1.0559,\tValidation Loss:  1.1560\n",
      "Epoch  28: \tTraining Loss:  1.0191,\tValidation Loss:  1.1920\n",
      "Epoch  29: \tTraining Loss:  1.0229,\tValidation Loss:  1.1476\n",
      "Epoch  30: \tTraining Loss:  0.9915,\tValidation Loss:  1.1993\n",
      "Epoch  31: \tTraining Loss:  1.0101,\tValidation Loss:  1.0990\n",
      "Epoch  32: \tTraining Loss:  1.0140,\tValidation Loss:  1.1558\n",
      "Epoch  33: \tTraining Loss:  1.0041,\tValidation Loss:  1.1546\n",
      "Epoch  34: \tTraining Loss:  0.9855,\tValidation Loss:  1.1433\n",
      "Epoch  35: \tTraining Loss:  0.9803,\tValidation Loss:  1.0872\n",
      "Epoch  36: \tTraining Loss:  0.9729,\tValidation Loss:  1.0840\n",
      "Epoch  37: \tTraining Loss:  0.9575,\tValidation Loss:  1.1232\n",
      "Epoch  38: \tTraining Loss:  0.9609,\tValidation Loss:  1.0496\n",
      "Epoch  39: \tTraining Loss:  0.9593,\tValidation Loss:  1.0970\n",
      "Epoch  40: \tTraining Loss:  0.9263,\tValidation Loss:  1.0985\n",
      "Epoch  41: \tTraining Loss:  0.9371,\tValidation Loss:  1.1794\n",
      "Epoch  42: \tTraining Loss:  0.9264,\tValidation Loss:  1.1515\n",
      "Epoch  43: \tTraining Loss:  0.9473,\tValidation Loss:  1.0655\n",
      "Epoch  44: \tTraining Loss:  0.9245,\tValidation Loss:  1.0629\n",
      "Epoch  45: \tTraining Loss:  0.9160,\tValidation Loss:  1.0693\n",
      "Epoch  46: \tTraining Loss:  0.9147,\tValidation Loss:  0.9845\n",
      "Epoch  47: \tTraining Loss:  0.9293,\tValidation Loss:  1.0316\n",
      "Epoch  48: \tTraining Loss:  0.8977,\tValidation Loss:  1.0458\n",
      "Epoch  49: \tTraining Loss:  0.8959,\tValidation Loss:  1.0824\n",
      "Epoch  50: \tTraining Loss:  0.8889,\tValidation Loss:  0.9996\n",
      "Epoch  51: \tTraining Loss:  0.8906,\tValidation Loss:  0.9649\n",
      "Epoch  52: \tTraining Loss:  0.8906,\tValidation Loss:  1.0517\n",
      "Epoch  53: \tTraining Loss:  0.8881,\tValidation Loss:  1.0071\n",
      "Epoch  54: \tTraining Loss:  0.9044,\tValidation Loss:  0.9791\n",
      "Epoch  55: \tTraining Loss:  0.8554,\tValidation Loss:  1.0194\n",
      "Epoch  56: \tTraining Loss:  0.8892,\tValidation Loss:  0.9966\n",
      "Epoch  57: \tTraining Loss:  0.8909,\tValidation Loss:  1.0106\n",
      "Epoch  58: \tTraining Loss:  0.8419,\tValidation Loss:  1.0095\n",
      "Epoch  59: \tTraining Loss:  0.8437,\tValidation Loss:  1.0358\n",
      "Epoch  60: \tTraining Loss:  0.8580,\tValidation Loss:  0.9764\n",
      "Epoch  61: \tTraining Loss:  0.8430,\tValidation Loss:  0.9495\n",
      "Epoch  62: \tTraining Loss:  0.8383,\tValidation Loss:  0.9667\n",
      "Epoch  63: \tTraining Loss:  0.8272,\tValidation Loss:  1.0111\n",
      "Epoch  64: \tTraining Loss:  0.8177,\tValidation Loss:  1.0071\n",
      "Epoch  65: \tTraining Loss:  0.8492,\tValidation Loss:  0.9415\n",
      "Epoch  66: \tTraining Loss:  0.8379,\tValidation Loss:  0.9607\n",
      "Epoch  67: \tTraining Loss:  0.8545,\tValidation Loss:  0.9960\n",
      "Epoch  68: \tTraining Loss:  0.8177,\tValidation Loss:  0.9915\n",
      "Epoch  69: \tTraining Loss:  0.8085,\tValidation Loss:  0.9195\n",
      "Epoch  70: \tTraining Loss:  0.8069,\tValidation Loss:  1.0215\n",
      "Epoch  71: \tTraining Loss:  0.7972,\tValidation Loss:  0.9811\n",
      "Epoch  72: \tTraining Loss:  0.8094,\tValidation Loss:  0.9800\n",
      "Epoch  73: \tTraining Loss:  0.7977,\tValidation Loss:  0.9525\n",
      "Epoch  74: \tTraining Loss:  0.7872,\tValidation Loss:  0.9349\n",
      "Epoch  75: \tTraining Loss:  0.8050,\tValidation Loss:  1.0265\n",
      "Epoch  76: \tTraining Loss:  0.8058,\tValidation Loss:  1.0141\n",
      "Epoch  77: \tTraining Loss:  0.8132,\tValidation Loss:  0.9062\n",
      "Epoch  78: \tTraining Loss:  0.7784,\tValidation Loss:  0.9370\n",
      "Epoch  79: \tTraining Loss:  0.7791,\tValidation Loss:  0.9554\n",
      "Epoch  80: \tTraining Loss:  0.7827,\tValidation Loss:  0.9002\n",
      "Epoch  81: \tTraining Loss:  0.7876,\tValidation Loss:  0.8833\n",
      "Epoch  82: \tTraining Loss:  0.8107,\tValidation Loss:  0.9845\n",
      "Epoch  83: \tTraining Loss:  0.7763,\tValidation Loss:  0.9433\n",
      "Epoch  84: \tTraining Loss:  0.7824,\tValidation Loss:  0.9079\n",
      "Epoch  85: \tTraining Loss:  0.7755,\tValidation Loss:  0.8715\n",
      "Epoch  86: \tTraining Loss:  0.7971,\tValidation Loss:  0.9345\n",
      "Epoch  87: \tTraining Loss:  0.7721,\tValidation Loss:  0.9433\n",
      "Epoch  88: \tTraining Loss:  0.7537,\tValidation Loss:  0.9406\n",
      "Epoch  89: \tTraining Loss:  0.7780,\tValidation Loss:  0.9112\n",
      "Epoch  90: \tTraining Loss:  0.7831,\tValidation Loss:  0.8951\n",
      "Epoch  91: \tTraining Loss:  0.7675,\tValidation Loss:  0.8572\n",
      "Epoch  92: \tTraining Loss:  0.7740,\tValidation Loss:  0.8952\n",
      "Epoch  93: \tTraining Loss:  0.7675,\tValidation Loss:  0.8921\n",
      "Epoch  94: \tTraining Loss:  0.7701,\tValidation Loss:  0.9383\n",
      "Epoch  95: \tTraining Loss:  0.7611,\tValidation Loss:  0.9072\n",
      "Epoch  96: \tTraining Loss:  0.7367,\tValidation Loss:  0.9038\n",
      "Epoch  97: \tTraining Loss:  0.7539,\tValidation Loss:  0.8887\n",
      "Epoch  98: \tTraining Loss:  0.7176,\tValidation Loss:  0.9054\n",
      "Epoch  99: \tTraining Loss:  0.7583,\tValidation Loss:  0.8872\n",
      "Epoch 100: \tTraining Loss:  0.7250,\tValidation Loss:  0.9889\n",
      "Epoch 101: \tTraining Loss:  0.7266,\tValidation Loss:  0.9260\n",
      "Epoch 102: \tTraining Loss:  0.7278,\tValidation Loss:  0.8977\n",
      "Epoch 103: \tTraining Loss:  0.7256,\tValidation Loss:  0.8775\n",
      "Epoch 104: \tTraining Loss:  0.7439,\tValidation Loss:  0.9161\n",
      "Epoch 105: \tTraining Loss:  0.7408,\tValidation Loss:  0.8861\n",
      "Epoch 106: \tTraining Loss:  0.7412,\tValidation Loss:  0.9351\n",
      "Epoch 107: \tTraining Loss:  0.7444,\tValidation Loss:  0.8950\n",
      "Epoch 108: \tTraining Loss:  0.7349,\tValidation Loss:  0.8199\n",
      "Epoch 109: \tTraining Loss:  0.7160,\tValidation Loss:  0.9102\n",
      "Epoch 110: \tTraining Loss:  0.7382,\tValidation Loss:  0.8969\n",
      "Epoch 111: \tTraining Loss:  0.7386,\tValidation Loss:  0.9069\n",
      "Epoch 112: \tTraining Loss:  0.7345,\tValidation Loss:  0.8215\n",
      "Epoch 113: \tTraining Loss:  0.7386,\tValidation Loss:  0.8553\n",
      "Epoch 114: \tTraining Loss:  0.7007,\tValidation Loss:  0.8256\n",
      "Epoch 115: \tTraining Loss:  0.7321,\tValidation Loss:  0.8564\n",
      "Epoch 116: \tTraining Loss:  0.7073,\tValidation Loss:  0.8220\n",
      "Epoch 117: \tTraining Loss:  0.6891,\tValidation Loss:  0.8844\n",
      "Epoch 118: \tTraining Loss:  0.7116,\tValidation Loss:  0.8184\n",
      "Epoch 119: \tTraining Loss:  0.7147,\tValidation Loss:  0.8867\n",
      "Epoch 120: \tTraining Loss:  0.7040,\tValidation Loss:  0.8439\n",
      "Epoch 121: \tTraining Loss:  0.7169,\tValidation Loss:  0.9154\n",
      "Epoch 122: \tTraining Loss:  0.6849,\tValidation Loss:  0.8994\n",
      "Epoch 123: \tTraining Loss:  0.7328,\tValidation Loss:  0.8413\n",
      "Epoch 124: \tTraining Loss:  0.7049,\tValidation Loss:  0.8755\n",
      "Epoch 125: \tTraining Loss:  0.7103,\tValidation Loss:  0.8421\n",
      "Epoch 126: \tTraining Loss:  0.6848,\tValidation Loss:  0.7857\n",
      "Epoch 127: \tTraining Loss:  0.6864,\tValidation Loss:  0.8121\n",
      "Epoch 128: \tTraining Loss:  0.6925,\tValidation Loss:  0.8770\n",
      "Epoch 129: \tTraining Loss:  0.6852,\tValidation Loss:  0.8707\n",
      "Epoch 130: \tTraining Loss:  0.7050,\tValidation Loss:  0.8326\n",
      "Epoch 131: \tTraining Loss:  0.6914,\tValidation Loss:  0.8747\n",
      "Epoch 132: \tTraining Loss:  0.6828,\tValidation Loss:  0.9012\n",
      "Epoch 133: \tTraining Loss:  0.6915,\tValidation Loss:  0.9488\n",
      "Epoch 134: \tTraining Loss:  0.6783,\tValidation Loss:  0.7995\n",
      "Epoch 135: \tTraining Loss:  0.7250,\tValidation Loss:  0.8651\n",
      "Epoch 136: \tTraining Loss:  0.6838,\tValidation Loss:  0.8323\n",
      "Epoch 137: \tTraining Loss:  0.6807,\tValidation Loss:  0.8653\n",
      "Epoch 138: \tTraining Loss:  0.6776,\tValidation Loss:  0.8407\n",
      "Epoch 139: \tTraining Loss:  0.6604,\tValidation Loss:  0.8639\n",
      "Epoch 140: \tTraining Loss:  0.6828,\tValidation Loss:  0.8324\n",
      "Epoch 141: \tTraining Loss:  0.6837,\tValidation Loss:  0.8850\n",
      "Epoch 142: \tTraining Loss:  0.6687,\tValidation Loss:  0.8152\n",
      "Epoch 143: \tTraining Loss:  0.6717,\tValidation Loss:  0.8119\n",
      "Epoch 144: \tTraining Loss:  0.6830,\tValidation Loss:  0.8009\n",
      "Epoch 145: \tTraining Loss:  0.6671,\tValidation Loss:  0.7884\n",
      "Epoch 146: \tTraining Loss:  0.6896,\tValidation Loss:  0.8734\n",
      "Epoch 147: \tTraining Loss:  0.6843,\tValidation Loss:  0.9254\n",
      "Epoch 148: \tTraining Loss:  0.6848,\tValidation Loss:  0.7972\n",
      "Epoch 149: \tTraining Loss:  0.6792,\tValidation Loss:  0.8825\n",
      "Epoch 150: \tTraining Loss:  0.6785,\tValidation Loss:  0.8311\n",
      "Epoch 151: \tTraining Loss:  0.6653,\tValidation Loss:  0.8355\n",
      "Epoch 152: \tTraining Loss:  0.6561,\tValidation Loss:  0.8772\n",
      "Epoch 153: \tTraining Loss:  0.6634,\tValidation Loss:  0.8477\n",
      "Epoch 154: \tTraining Loss:  0.6509,\tValidation Loss:  0.7999\n",
      "Epoch 155: \tTraining Loss:  0.6717,\tValidation Loss:  0.8514\n",
      "Epoch 156: \tTraining Loss:  0.6456,\tValidation Loss:  0.8091\n",
      "Epoch 157: \tTraining Loss:  0.6494,\tValidation Loss:  0.8200\n",
      "Epoch 158: \tTraining Loss:  0.6297,\tValidation Loss:  0.8075\n",
      "Epoch 159: \tTraining Loss:  0.6473,\tValidation Loss:  0.9063\n",
      "Epoch 160: \tTraining Loss:  0.6279,\tValidation Loss:  0.8185\n",
      "Epoch 161: \tTraining Loss:  0.6605,\tValidation Loss:  0.8596\n",
      "Epoch 162: \tTraining Loss:  0.6379,\tValidation Loss:  0.7636\n",
      "Epoch 163: \tTraining Loss:  0.6257,\tValidation Loss:  0.8532\n",
      "Epoch 164: \tTraining Loss:  0.6846,\tValidation Loss:  0.7926\n",
      "Epoch 165: \tTraining Loss:  0.6606,\tValidation Loss:  0.8766\n",
      "Epoch 166: \tTraining Loss:  0.6576,\tValidation Loss:  0.7716\n",
      "Epoch 167: \tTraining Loss:  0.6208,\tValidation Loss:  0.7828\n",
      "Epoch 168: \tTraining Loss:  0.6429,\tValidation Loss:  0.8061\n",
      "Epoch 169: \tTraining Loss:  0.6544,\tValidation Loss:  0.8234\n",
      "Epoch 170: \tTraining Loss:  0.6255,\tValidation Loss:  0.7776\n",
      "Epoch 171: \tTraining Loss:  0.6240,\tValidation Loss:  0.8588\n",
      "Epoch 172: \tTraining Loss:  0.6160,\tValidation Loss:  0.8167\n",
      "Epoch 173: \tTraining Loss:  0.6370,\tValidation Loss:  0.8210\n",
      "Epoch 174: \tTraining Loss:  0.6834,\tValidation Loss:  0.8335\n",
      "Epoch 175: \tTraining Loss:  0.6392,\tValidation Loss:  0.8119\n",
      "Epoch 176: \tTraining Loss:  0.6282,\tValidation Loss:  0.7672\n",
      "Epoch 177: \tTraining Loss:  0.6176,\tValidation Loss:  0.8183\n",
      "Epoch 178: \tTraining Loss:  0.6347,\tValidation Loss:  0.7472\n",
      "Epoch 179: \tTraining Loss:  0.6487,\tValidation Loss:  0.8060\n",
      "Epoch 180: \tTraining Loss:  0.6432,\tValidation Loss:  0.7959\n",
      "Epoch 181: \tTraining Loss:  0.6522,\tValidation Loss:  0.7550\n",
      "Epoch 182: \tTraining Loss:  0.6382,\tValidation Loss:  0.7856\n",
      "Epoch 183: \tTraining Loss:  0.6327,\tValidation Loss:  0.8326\n",
      "Epoch 184: \tTraining Loss:  0.6247,\tValidation Loss:  0.7944\n",
      "Epoch 185: \tTraining Loss:  0.6129,\tValidation Loss:  0.7973\n",
      "Epoch 186: \tTraining Loss:  0.6545,\tValidation Loss:  0.7778\n",
      "Epoch 187: \tTraining Loss:  0.6156,\tValidation Loss:  0.8237\n",
      "Epoch 188: \tTraining Loss:  0.6261,\tValidation Loss:  0.8300\n",
      "Epoch 189: \tTraining Loss:  0.6342,\tValidation Loss:  0.7808\n",
      "Epoch 190: \tTraining Loss:  0.6254,\tValidation Loss:  0.8659\n",
      "Epoch 191: \tTraining Loss:  0.6081,\tValidation Loss:  0.7803\n",
      "Epoch 192: \tTraining Loss:  0.6119,\tValidation Loss:  0.7960\n",
      "Epoch 193: \tTraining Loss:  0.6057,\tValidation Loss:  0.7788\n",
      "Epoch 194: \tTraining Loss:  0.6273,\tValidation Loss:  0.8016\n",
      "Epoch 195: \tTraining Loss:  0.5938,\tValidation Loss:  0.8395\n",
      "Epoch 196: \tTraining Loss:  0.5968,\tValidation Loss:  0.8273\n",
      "Epoch 197: \tTraining Loss:  0.6047,\tValidation Loss:  0.7955\n",
      "Epoch 198: \tTraining Loss:  0.6149,\tValidation Loss:  0.7653\n",
      "Epoch 199: \tTraining Loss:  0.5970,\tValidation Loss:  0.7277\n",
      "Epoch 200: \tTraining Loss:  0.6420,\tValidation Loss:  0.7853\n",
      "Epoch 201: \tTraining Loss:  0.6692,\tValidation Loss:  0.8604\n",
      "Epoch 202: \tTraining Loss:  0.6429,\tValidation Loss:  0.8846\n",
      "Epoch 203: \tTraining Loss:  0.5923,\tValidation Loss:  0.8625\n",
      "Epoch 204: \tTraining Loss:  0.6061,\tValidation Loss:  0.7982\n",
      "Epoch 205: \tTraining Loss:  0.6350,\tValidation Loss:  0.8008\n",
      "Epoch 206: \tTraining Loss:  0.6175,\tValidation Loss:  0.7983\n",
      "Epoch 207: \tTraining Loss:  0.6147,\tValidation Loss:  0.7549\n",
      "Epoch 208: \tTraining Loss:  0.6223,\tValidation Loss:  0.7835\n",
      "Epoch 209: \tTraining Loss:  0.5933,\tValidation Loss:  0.8003\n",
      "Epoch 210: \tTraining Loss:  0.6215,\tValidation Loss:  0.7612\n",
      "Epoch 211: \tTraining Loss:  0.6165,\tValidation Loss:  0.7451\n",
      "Epoch 212: \tTraining Loss:  0.6097,\tValidation Loss:  0.7733\n",
      "Epoch 213: \tTraining Loss:  0.6203,\tValidation Loss:  0.8263\n",
      "Epoch 214: \tTraining Loss:  0.6232,\tValidation Loss:  0.7521\n",
      "Epoch 215: \tTraining Loss:  0.5808,\tValidation Loss:  0.7919\n",
      "Epoch 216: \tTraining Loss:  0.6422,\tValidation Loss:  0.7735\n",
      "Epoch 217: \tTraining Loss:  0.6116,\tValidation Loss:  0.8105\n",
      "Epoch 218: \tTraining Loss:  0.5825,\tValidation Loss:  0.7365\n",
      "Epoch 219: \tTraining Loss:  0.5935,\tValidation Loss:  0.8169\n",
      "Epoch 220: \tTraining Loss:  0.6020,\tValidation Loss:  0.7516\n",
      "Epoch 221: \tTraining Loss:  0.5978,\tValidation Loss:  0.7777\n",
      "Epoch 222: \tTraining Loss:  0.6142,\tValidation Loss:  0.7386\n",
      "Epoch 223: \tTraining Loss:  0.6168,\tValidation Loss:  0.7840\n",
      "Epoch 224: \tTraining Loss:  0.5830,\tValidation Loss:  0.7808\n",
      "Epoch 225: \tTraining Loss:  0.5972,\tValidation Loss:  0.7665\n",
      "Epoch 226: \tTraining Loss:  0.6060,\tValidation Loss:  0.7597\n",
      "Epoch 227: \tTraining Loss:  0.5779,\tValidation Loss:  0.7552\n",
      "Epoch 228: \tTraining Loss:  0.5886,\tValidation Loss:  0.7555\n",
      "Epoch 229: \tTraining Loss:  0.6061,\tValidation Loss:  0.8054\n",
      "Epoch 230: \tTraining Loss:  0.5884,\tValidation Loss:  0.8082\n",
      "Epoch 231: \tTraining Loss:  0.6009,\tValidation Loss:  0.8476\n",
      "Epoch 232: \tTraining Loss:  0.5756,\tValidation Loss:  0.7990\n",
      "Epoch 233: \tTraining Loss:  0.5772,\tValidation Loss:  0.7560\n",
      "Epoch 234: \tTraining Loss:  0.5777,\tValidation Loss:  0.7998\n",
      "Epoch 235: \tTraining Loss:  0.6106,\tValidation Loss:  0.7929\n",
      "Epoch 236: \tTraining Loss:  0.6016,\tValidation Loss:  0.7295\n",
      "Epoch 237: \tTraining Loss:  0.6251,\tValidation Loss:  0.7731\n",
      "Epoch 238: \tTraining Loss:  0.5811,\tValidation Loss:  0.7530\n",
      "Epoch 239: \tTraining Loss:  0.6241,\tValidation Loss:  0.7229\n",
      "Epoch 240: \tTraining Loss:  0.5753,\tValidation Loss:  0.7743\n",
      "Epoch 241: \tTraining Loss:  0.5885,\tValidation Loss:  0.7839\n",
      "Epoch 242: \tTraining Loss:  0.5802,\tValidation Loss:  0.8337\n",
      "Epoch 243: \tTraining Loss:  0.5956,\tValidation Loss:  0.8065\n",
      "Epoch 244: \tTraining Loss:  0.5758,\tValidation Loss:  0.7356\n",
      "Epoch 245: \tTraining Loss:  0.5807,\tValidation Loss:  0.6980\n",
      "Epoch 246: \tTraining Loss:  0.6123,\tValidation Loss:  0.7795\n",
      "Epoch 247: \tTraining Loss:  0.5776,\tValidation Loss:  0.7908\n",
      "Epoch 248: \tTraining Loss:  0.5869,\tValidation Loss:  0.7554\n",
      "Epoch 249: \tTraining Loss:  0.5866,\tValidation Loss:  0.7648\n",
      "Epoch 250: \tTraining Loss:  0.5406,\tValidation Loss:  0.7601\n",
      "Epoch 251: \tTraining Loss:  0.6066,\tValidation Loss:  0.7555\n",
      "Epoch 252: \tTraining Loss:  0.5665,\tValidation Loss:  0.8376\n",
      "Epoch 253: \tTraining Loss:  0.5837,\tValidation Loss:  0.7152\n",
      "Epoch 254: \tTraining Loss:  0.5745,\tValidation Loss:  0.7708\n",
      "Epoch 255: \tTraining Loss:  0.5715,\tValidation Loss:  0.7694\n",
      "Epoch 256: \tTraining Loss:  0.5883,\tValidation Loss:  0.7784\n",
      "Epoch 257: \tTraining Loss:  0.5603,\tValidation Loss:  0.7732\n",
      "Epoch 258: \tTraining Loss:  0.5889,\tValidation Loss:  0.6853\n",
      "Epoch 259: \tTraining Loss:  0.6037,\tValidation Loss:  0.7689\n",
      "Epoch 260: \tTraining Loss:  0.5615,\tValidation Loss:  0.7496\n",
      "Epoch 261: \tTraining Loss:  0.5442,\tValidation Loss:  0.7133\n",
      "Epoch 262: \tTraining Loss:  0.5664,\tValidation Loss:  0.7566\n",
      "Epoch 263: \tTraining Loss:  0.5756,\tValidation Loss:  0.7270\n",
      "Epoch 264: \tTraining Loss:  0.5846,\tValidation Loss:  0.8218\n",
      "Epoch 265: \tTraining Loss:  0.5748,\tValidation Loss:  0.7569\n",
      "Epoch 266: \tTraining Loss:  0.5554,\tValidation Loss:  0.7656\n",
      "Epoch 267: \tTraining Loss:  0.5857,\tValidation Loss:  0.7211\n",
      "Epoch 268: \tTraining Loss:  0.5831,\tValidation Loss:  0.7834\n",
      "Epoch 269: \tTraining Loss:  0.5880,\tValidation Loss:  0.7585\n",
      "Epoch 270: \tTraining Loss:  0.5625,\tValidation Loss:  0.7280\n",
      "Epoch 271: \tTraining Loss:  0.5575,\tValidation Loss:  0.7248\n",
      "Epoch 272: \tTraining Loss:  0.5741,\tValidation Loss:  0.7607\n",
      "Epoch 273: \tTraining Loss:  0.5610,\tValidation Loss:  0.7268\n",
      "Epoch 274: \tTraining Loss:  0.5351,\tValidation Loss:  0.7290\n",
      "Epoch 275: \tTraining Loss:  0.5684,\tValidation Loss:  0.7397\n",
      "Epoch 276: \tTraining Loss:  0.5823,\tValidation Loss:  0.7764\n",
      "Epoch 277: \tTraining Loss:  0.5880,\tValidation Loss:  0.8162\n",
      "Epoch 278: \tTraining Loss:  0.5645,\tValidation Loss:  0.7846\n",
      "Epoch 279: \tTraining Loss:  0.5638,\tValidation Loss:  0.7824\n",
      "Epoch 280: \tTraining Loss:  0.5718,\tValidation Loss:  0.6967\n",
      "Epoch 281: \tTraining Loss:  0.5939,\tValidation Loss:  0.7300\n",
      "Epoch 282: \tTraining Loss:  0.5633,\tValidation Loss:  0.7229\n",
      "Epoch 283: \tTraining Loss:  0.5578,\tValidation Loss:  0.7412\n",
      "Epoch 284: \tTraining Loss:  0.5634,\tValidation Loss:  0.7975\n",
      "Epoch 285: \tTraining Loss:  0.5309,\tValidation Loss:  0.7784\n",
      "Epoch 286: \tTraining Loss:  0.5669,\tValidation Loss:  0.7425\n",
      "Epoch 287: \tTraining Loss:  0.5567,\tValidation Loss:  0.7790\n",
      "Epoch 288: \tTraining Loss:  0.5829,\tValidation Loss:  0.7459\n",
      "Epoch 289: \tTraining Loss:  0.5774,\tValidation Loss:  0.7170\n",
      "Epoch 290: \tTraining Loss:  0.5612,\tValidation Loss:  0.7272\n",
      "Epoch 291: \tTraining Loss:  0.5589,\tValidation Loss:  0.6944\n",
      "Epoch 292: \tTraining Loss:  0.5585,\tValidation Loss:  0.7533\n",
      "Epoch 293: \tTraining Loss:  0.5673,\tValidation Loss:  0.7493\n",
      "Epoch 294: \tTraining Loss:  0.5416,\tValidation Loss:  0.7671\n",
      "Epoch 295: \tTraining Loss:  0.5465,\tValidation Loss:  0.7099\n",
      "Epoch 296: \tTraining Loss:  0.5434,\tValidation Loss:  0.7918\n",
      "Epoch 297: \tTraining Loss:  0.5227,\tValidation Loss:  0.7265\n",
      "Epoch 298: \tTraining Loss:  0.5334,\tValidation Loss:  0.7811\n",
      "Epoch 299: \tTraining Loss:  0.5632,\tValidation Loss:  0.7480\n",
      "Epoch 300: \tTraining Loss:  0.5714,\tValidation Loss:  0.7075\n",
      "Epoch 301: \tTraining Loss:  0.5530,\tValidation Loss:  0.7671\n",
      "Epoch 302: \tTraining Loss:  0.5553,\tValidation Loss:  0.6959\n",
      "Epoch 303: \tTraining Loss:  0.5813,\tValidation Loss:  0.7225\n",
      "Epoch 304: \tTraining Loss:  0.5266,\tValidation Loss:  0.7362\n",
      "Epoch 305: \tTraining Loss:  0.5311,\tValidation Loss:  0.7229\n",
      "Epoch 306: \tTraining Loss:  0.5960,\tValidation Loss:  0.7171\n",
      "Epoch 307: \tTraining Loss:  0.5593,\tValidation Loss:  0.7508\n",
      "Epoch 308: \tTraining Loss:  0.5686,\tValidation Loss:  0.7363\n",
      "Epoch 309: \tTraining Loss:  0.5213,\tValidation Loss:  0.7050\n",
      "Epoch 310: \tTraining Loss:  0.5467,\tValidation Loss:  0.7070\n",
      "Epoch 311: \tTraining Loss:  0.5204,\tValidation Loss:  0.7500\n",
      "Epoch 312: \tTraining Loss:  0.5710,\tValidation Loss:  0.6665\n",
      "Epoch 313: \tTraining Loss:  0.5662,\tValidation Loss:  0.7630\n",
      "Epoch 314: \tTraining Loss:  0.5826,\tValidation Loss:  0.7046\n",
      "Epoch 315: \tTraining Loss:  0.5671,\tValidation Loss:  0.6935\n",
      "Epoch 316: \tTraining Loss:  0.5576,\tValidation Loss:  0.7521\n",
      "Epoch 317: \tTraining Loss:  0.5502,\tValidation Loss:  0.6960\n",
      "Epoch 318: \tTraining Loss:  0.5469,\tValidation Loss:  0.7355\n",
      "Epoch 319: \tTraining Loss:  0.5539,\tValidation Loss:  0.7019\n",
      "Epoch 320: \tTraining Loss:  0.5464,\tValidation Loss:  0.7247\n",
      "Epoch 321: \tTraining Loss:  0.5384,\tValidation Loss:  0.7092\n",
      "Epoch 322: \tTraining Loss:  0.5390,\tValidation Loss:  0.7186\n",
      "Epoch 323: \tTraining Loss:  0.5172,\tValidation Loss:  0.6891\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-233-5b16f40a1aeb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m         \u001b[0mtrain_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\optim\\optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m                     \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\autograd\\grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\optim\\adam.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    131\u001b[0m                     \u001b[0mstate_steps\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'step'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 133\u001b[1;33m             F.adam(params_with_grad,\n\u001b[0m\u001b[0;32m    134\u001b[0m                    \u001b[0mgrads\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    135\u001b[0m                    \u001b[0mexp_avgs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\optim\\_functional.py\u001b[0m in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m         \u001b[1;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 86\u001b[1;33m         \u001b[0mexp_avg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     87\u001b[0m         \u001b[0mexp_avg_sq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconj\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mamsgrad\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_epochs = 500\n",
    "time_start = time.time()\n",
    "print('CUDA is' + (' ' if is_cuda else ' not ') + 'running.')\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    cnt_epoch += 1\n",
    "    train_loss = 0.0\n",
    "    model.train()\n",
    "    for data, target in train_loader:\n",
    "        if is_cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()*data.size(0)\n",
    "        \n",
    "    val_loss = 0.0\n",
    "    model.eval()\n",
    "    for data, target in val_loader:\n",
    "        if is_cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        val_loss += loss.item() * data.size(0)\n",
    "    \n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    val_loss /= len(val_loader.dataset)\n",
    "    \n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        best_state = copy.deepcopy(model.state_dict())\n",
    "    \n",
    "    print('Epoch {:3d}: \\tTraining Loss: {:7.4f},\\tValidation Loss: {:7.4f}'.format(cnt_epoch, train_loss, val_loss))\n",
    "    \n",
    "print('Elapsed Time: {:8.3f}Sec'.format(time.time()-time_start))\n",
    "print(best_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "0fb0693f-dc59-448a-8608-28a6b9b18464",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 1,  ..., 0, 0, 3], device='cuda:0')"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval() # 모델을 평가모드로 바꿉니다. dropout이 일어나지 않습니다.\n",
    "model.load_state_dict(best_state)\n",
    "\n",
    "if is_cuda:\n",
    "    X_te = X_te.cuda()\n",
    "\n",
    "with torch.no_grad(): # 이 안의 코드는 가중치 업데이트가 일어나지 않습니다.\n",
    "    outputs = model(X_te)\n",
    "    _, pred = torch.max(outputs, 1)\n",
    "\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "cc7eb11c-95cf-4841-9ca3-06908f399a51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    2817\n",
       "3    2283\n",
       "2    2135\n",
       "0    2108\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if is_cuda:\n",
    "    pred = pred.cpu()\n",
    "\n",
    "df_sub['target'] = pred.numpy()\n",
    "df_sub['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f0a9b1-2434-4494-8d68-94b30fd007bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180cf444-9eda-4790-8cc0-b47b60aca04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss = 0.0\n",
    "class_correct = list(0 for i in range(10))\n",
    "class_total = list(0 for i in range(10))\n",
    "confusion_matrix = [[0 for _ in range(10)] for _ in range(10)]\n",
    "\n",
    "targets = []\n",
    "preds = []\n",
    "\n",
    "model.load_state_dict(best_state)\n",
    "\n",
    "for data, target in test_loader:\n",
    "    if is_cuda:\n",
    "        data, target = data.cuda(), target.cuda()\n",
    "    output = model(data)\n",
    "    loss = criterion(output, target)\n",
    "    test_loss += loss.item()*data.size(0)\n",
    "    _, pred = torch.max(output, 1)\n",
    "    correct = np.squeeze(pred.eq(target.data.view_as(pred)))\n",
    "    for i in range(len(target.data)):\n",
    "        label = target.data[i]\n",
    "        predict = pred[i]\n",
    "        class_correct[label] += correct[i].item()\n",
    "        class_total[label] += 1\n",
    "        confusion_matrix[label][predict] += 1\n",
    "\n",
    "test_loss /= len(test_loader.dataset)\n",
    "print('Test Loss: {:7.4f}'.format(test_loss))\n",
    "print('Test Accuracy: {:6.2f}% ({:d}/{:d})'.format(100. * np.sum(class_correct) / np.sum(class_total), np.sum(class_correct), np.sum(class_total)))\n",
    "\n",
    "print(' A\\P| ' + ' '.join(map(lambda x:'{:4d}'.format(x), range(10))))\n",
    "print('-'*4 + '+' + '-'*50)\n",
    "for num_, each_ in enumerate(confusion_matrix):\n",
    "    print('{:3d} | '.format(num_) + ' '.join(map(lambda x:'{:4d}'.format(x), each_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "8c603925-5797-4fe0-9408-60696d11f03b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "now = datetime.datetime.now()\n",
    "str_datetime = now.strftime(\"%y%m%d_%H%M%S\")\n",
    "df_sub.to_csv(dir_dataset+'submission-'+name_project+'-'+str_datetime+'.csv', index = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9ec699-2ed0-48c4-9f35-6738ad46371a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
